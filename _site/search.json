[
  {
    "objectID": "Statistical-learning/project/Stat-learning-2.html",
    "href": "Statistical-learning/project/Stat-learning-2.html",
    "title": "Predictive Model for HDB Resale Prices",
    "section": "",
    "text": "In this project, we were given data on resale prices of 5 room HDB flats in Punggol. The project task was to create the best predictive regression that can best explain variation in prices. The independent regressors and dependent variable in the regression are as such:\n\n\n\n\n\n\n\nDependent Variable\nIndependent regressors\n\n\n\n\nResale price\nDate of sale\nType of room\nBlock number\nStreet\nStory\nArea\nModel\nLease Begin Date\nAmount of Lease Remaining\n\n\n\n\n\n\n\nWe used 2 approaches – Linear Regression and Decision Trees – to develop and test different models. We selected our predictors using the best subset selection method and extended our model to include polynomials and interaction effects. We use the cross-validation method to estimate the test errors. The model using random forest method gives the smallest estimated test error and is chosen as our final recommended model.\n\n\n\nWe created a pair plot of the dataset to identify non-linear relationships between the response variable and the predictors. However, there does not appear to be any identifiable relationship between them.\n\n\n\nWe use the best subset selection and exhaustive search method to identify the predictors that we should use for our models. The output suggests that all the variables should be included in the prediction model.\n\n\n\nWe extended our analysis to include polynomials and interaction effects and use the 10-fold cross-validation method to estimate test error. We observe that there is an improved model for which the test error is minimized.\n\n\n\nWe further finetuned our linear regression model by using the Ridge Regression and Lasso Regression regularization method. The Lasso Regression improves upon the Ridge Regression and provides a lower test error.\n\n\n\nWe proceeded to determine if a Decision Tree approach would give better predictive capabilities. We opted to focus on the Bagging and Random Forest methods as they offer better predictive power as compared to a single tree. Based on the results of the subset selection, we used all variables for the regression tree. The Random Forest method improves upon both the Bagging and Linear Regression models and provides the smallest estimated test error.\n\n\n\nWe retrain the Random Forest method using the full training data set (Rm5HDB2023P.csv) and test for the mean squared error using the test data set (Rm5HDB2023testP.csv). We obtain a final mean squared error of 2363533420.\n\n\n\n\n\ndata<-read.csv(\"data/Rm5HDB2023P.csv\", stringsAsFactors = TRUE)\n\nlibrary(tree)\nattach(data)\nhead(data)\n\n        Town    Story Area    Model LeaseRem   Price\n1 ANG MO KIO 13 TO 15  113 Improved       95 1025000\n2 ANG MO KIO 01 TO 03  120     DBSS       88  780000\n3 ANG MO KIO  19Above  118 Improved       58  700000\n4 ANG MO KIO  19Above  135  Model A       59  860000\n5 ANG MO KIO 04 TO 06  117 Improved       57  620000\n6 ANG MO KIO 16 TO 18  118 Improved       57  700000\n\ndata <- na.omit(data) #remove NA values\ndim(data) #tells you how many of the datapoints are left after removing na rows\n\n[1] 4995    6\n\nsum(is.na(data)) #Check if there is still any NA values\n\n[1] 0\n\nattach(data)\npairs(data)\n\n\n\n\n\n\n\n\ntrain<-sample(1:nrow(data), nrow(data)/2)\ntest<--train\n\n\n\n\n\nlibrary(leaps)\nlibrary(boot)\n\n\n\n\n\npredict.regsubsets <- function(object, newdata, id) {\n  form <- as.formula(object$call[[2]])\n  mat <- model.matrix(form, newdata)\n  coefi <- coef(object, id = id)\n  xvars <- names(coefi)\n  mat[, xvars] %*% coefi\n} #function to predict the regsubset with the lowest MSE\n\nk <- 10\nset.seed(6789)\nfolds <- sample(1:k, nrow(data), replace = TRUE)\ncv.errors <- matrix(NA, k, 36, dimnames = list(NULL, paste(1:36)))\nfor (j in 1:k){\n  best.fit <- regsubsets(Price ~ ., data = data[folds != j,], nvmax = 36) #Function for performing cross validation @ the j fold\n  for (i in 1:36) {\n    pred <- predict.regsubsets(best.fit, data[folds ==j,], id = i) \n    cv.errors[j, i] <- mean((data$Price[folds ==j] - pred)^2) #Function for making prediction for computing the MSE\n  }\n}\nmean.cv <- apply(cv.errors, 2, mean) #apply function averages over the columns of the matrix \naa <- which.min(mean.cv) \naa #finds which regression model has the lowest MSE\n\n36 \n36 \n\nmean.cv[aa]\n\n        36 \n3456368028 \n\n\nCV error=3446954895\n\n\n\n\nk <- 10\nset.seed(6789)\nfolds <- sample(1:k, nrow(data), replace = TRUE)\ncv.errors <- matrix(NA, k, 38, dimnames = list(NULL, paste(1:38)))\nfor (j in 1:k){\n  best.fit <- regsubsets(Price ~ . + I(Area^2) + I(LeaseRem^2), data = data[folds != j,], nvmax = 38) #Function for performing cross validation @ the j fold\n  for (i in 1:38) {\n    pred <- predict.regsubsets(best.fit, data[folds ==j,], id = i) \n    cv.errors[j, i] <- mean((data$Price[folds ==j] - pred)^2) #Function for making prediction for computing the MSE\n  }\n}\nmean.cv <- apply(cv.errors, 2, mean) #apply function averages over the columns of the matrix \nbb <- which.min(mean.cv) \nbb #finds which regression model has the lowest MSE\n\n38 \n38 \n\nmean.cv[bb]\n\n        38 \n3384739805 \n\n\ncv error: 3376920291 (reduced from previously)\n\n\n\n\nss2 <- glm(Price ~ . + I(Area^2) + I(LeaseRem^2), data = data)\ncv.err2 <- cv.glm(data, ss2, K = 10)\ncv.err2$delta[1]\n\n[1] 3377445110\n\nss3 <- glm(Price ~ . + I(Area^2) + I(LeaseRem^2) + Area:Model, data = data)\ncv.err3 <- cv.glm(data, ss3, K = 10)\ncv.err3$delta[1]\n\n[1] 3369624052\n\nss4 <- glm(Price ~ . + I(Area^2) + I(LeaseRem^2) + Area:Model + Area:Town, data = data)\ncv.err4 <- cv.glm(data, ss4, K = 10)\ncv.err4$delta[1]\n\n[1] 3069297922\n\nss5 <- glm(Price ~ . + I(Area^2) + I(LeaseRem^2) + Area:Model + Area:Town + Area:Story, data = data)\ncv.err5 <- cv.glm(data, ss5, K = 10)\ncv.err5$delta[1]\n\n[1] 3068671934\n\nss6 <- glm(Price ~ . + I(Area^2) + I(LeaseRem^2) + Area:Model + Area:Town + Town:LeaseRem, data = data)\ncv.err6 <- cv.glm(data, ss6, K = 10)\ncv.err6$delta[1]\n\n[1] 2783624091\n\nss7 <- glm(Price ~ . + I(Area^2) + I(LeaseRem^2) + Area:Model + Area:Town + LeaseRem:Town + LeaseRem:Model, data = data)\ncv.err7 <- cv.glm(data, ss7, K = 10)\ncv.err7$delta[1]\n\n[1] 2707993781\n\nss8 <- glm(Price ~ . + I(Area^2) + I(LeaseRem^2) + Area:Model + Area:Town + LeaseRem:Town + LeaseRem:Model + LeaseRem:Story, data = data)\ncv.err8 <- cv.glm(data, ss8, K = 10)\ncv.err8$delta[1]\n\n[1] 2694944674\n\nss9 <- glm(Price ~ . + I(Area^2) + I(LeaseRem^2) + Area:Model + Area:Town + LeaseRem:Town + LeaseRem:Model + LeaseRem:Area, data = data)\ncv.err9 <- cv.glm(data, ss9, K = 10)\ncv.err9$delta[1]\n\n[1] 2735526944\n\n\nRidge Regression\n\nlibrary(glmnet)\n\ntrain<-sample(1:nrow(data), nrow(data)/2)\ntest<--train\n\nx.train<-model.matrix(Price ~ . + I(Area^2) + I(LeaseRem^2) + Area:Model + Area:Town + LeaseRem:Town + LeaseRem:Model + LeaseRem:Area, data=data[train,])[,-6]\ny.train<-data[train,]$Price\n\nx.test<-model.matrix(Price ~ . + I(Area^2) + I(LeaseRem^2) + Area:Model + Area:Town + LeaseRem:Town + LeaseRem:Model + LeaseRem:Area, data=data[test,])[,-6]\ny.test<-data[test,]$Price\n\ncv.rr <- cv.glmnet(x.train, y.train, alpha=0)\nplot(cv.rr)\n\n\n\nbestlam <- cv.rr$lambda.min\n\nridge.mod <- glmnet(x.train, y.train, alpha=0)\n\nridge.pred <- predict(ridge.mod, s=bestlam, newx=x.test) \nmean((ridge.pred-y.test)^2) #calc training error\n\n[1] 3194900912\n\n\ntest mse=3130423699\nLasso regression\n\ncv.lasso <- cv.glmnet(x.train, y.train, alpha=1)\nplot(cv.lasso)\n\n\n\nbestlam <- cv.lasso$lambda.min\n\nlasso.mod <- glmnet(x.train, y.train, alpha=1)\n\nlasso.pred <- predict(lasso.mod, s=bestlam, newx=x.test) \nmean((lasso.pred-y.test)^2) #calc training error\n\n[1] 2764489430\n\n\ntest mse=2641542810\n\n\n\n\nRNGkind(sample.kind=\"Rounding\")\nset.seed(6789)\n\ntree.price <- tree(Price~., data[train,])\nsummary(tree.price)\n\n\nRegression tree:\ntree(formula = Price ~ ., data = data[train, ])\nVariables actually used in tree construction:\n[1] \"Town\"     \"LeaseRem\"\nNumber of terminal nodes:  8 \nResidual mean deviance:  4.753e+09 = 1.183e+13 / 2489 \nDistribution of residuals:\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n-256300  -38680   -4905       0   35460  403500 \n\nplot(tree.price)\ntitle(main=\"Classification tree for price data\")\ntext(tree.price, pretty=0)\n\n\n\nyhat<-predict(tree.price, newdata=data[test,])\nmean((yhat-data[test,]$Price)^2)\n\n[1] 4903542115\n\n\nTest mse: 4552948559\n\n\n\n\n# pruning tree using cross validation\ncv.price <- cv.tree(tree.price)  # use the cv.tree function to prune the tree.boston\ncv.price\n\n$size\n[1] 8 7 6 5 4 3 2 1\n\n$dev\n[1] 1.197899e+13 1.303126e+13 1.458282e+13 1.529848e+13 1.736211e+13\n[6] 2.219128e+13 2.657853e+13 4.655700e+13\n\n$k\n[1]         -Inf 7.927569e+11 1.112794e+12 1.276811e+12 2.117285e+12\n[6] 4.410722e+12 4.994679e+12 2.000411e+13\n\n$method\n[1] \"deviance\"\n\nattr(,\"class\")\n[1] \"prune\"         \"tree.sequence\"\n\n# Plot size against error\nplot(cv.price$size, cv.price$dev, type=\"b\", main=\"Cross validation: Deviance versus Size\",\n     xlab=\"Number of terminal nodes\", ylab=\"deviance\") \n\n\n\n# from plot, we see the relationship between nodes and deviation\n\nnn <- cv.price$size[which.min(cv.price$dev)]  # identify the optimal number of nodes \nnn\n\n[1] 8\n\n# in this case using cv does not reccommend any changes"
  },
  {
    "objectID": "Statistical-learning/project/Stat-learning-1.html",
    "href": "Statistical-learning/project/Stat-learning-1.html",
    "title": "Explanatory Model for HDB Resale Prices",
    "section": "",
    "text": "2 Installation of Packages Required\n\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(lubridate)\nlibrary(fitdistrplus)\nlibrary(olsrr)\n\n\n\n3 Exploratory Data Analysis\n\nPunggol <- read.csv(\"data/Punggol2023P.csv\", stringsAsFactors = TRUE)\nsummary(Punggol)\n\n      Date         Type          Block                   Street   \n 2022-01:197   3 ROOM: 208   308A   :  26   PUNGGOL DR      :311  \n 2022-09:192   4 ROOM:1048   308C   :  23   EDGEDALE PLAINS :308  \n 2022-07:182   5 ROOM: 680   663A   :  20   PUNGGOL FIELD   :240  \n 2022-12:177                 682A   :  19   EDGEFIELD PLAINS:206  \n 2022-04:174                 226A   :  18   PUNGGOL WALK    :182  \n 2022-11:163                 226C   :  18   PUNGGOL CTRL    :155  \n (Other):851                 (Other):1812   (Other)         :534  \n      Story          Area                           Model        LeaseBegin  \n 01 TO 03:219   Min.   : 65.00   Improved              : 301   Min.   :2002  \n 04 TO 06:363   1st Qu.: 92.00   Model A               : 564   1st Qu.:2009  \n 07 TO 09:386   Median : 93.00   Premium Apartment     :1070   Median :2015  \n 10 TO 12:385   Mean   : 96.64   Premium Apartment Loft:   1   Mean   :2012  \n 13 TO 15:348   3rd Qu.:110.00                                 3rd Qu.:2016  \n 16 TO 18:228   Max.   :149.00                                 Max.   :2018  \n 19 TO 21:  7                                                                \n             LeaseRemain    ResalePrice     \n 93 years 06 months:  41   Min.   : 377000  \n 93 years          :  40   1st Qu.: 508000  \n 95 years          :  39   Median : 551000  \n 93 years 03 months:  38   Mean   : 566305  \n 94 years 10 months:  36   3rd Qu.: 611166  \n 93 years 02 months:  35   Max.   :1198000  \n (Other)           :1707                    \n\nhead(Punggol, 5)\n\n     Date   Type Block          Street    Story Area    Model LeaseBegin\n1 2022-01 5 ROOM  120B EDGEDALE PLAINS 07 TO 09  113 Improved       2017\n2 2022-01 5 ROOM  171A EDGEDALE PLAINS 16 TO 18  110 Improved       2004\n3 2022-01 5 ROOM  126C EDGEDALE PLAINS 10 TO 12  110 Improved       2003\n4 2022-01 5 ROOM  120A EDGEDALE PLAINS 13 TO 15  113 Improved       2017\n5 2022-01 5 ROOM  174C EDGEDALE PLAINS 10 TO 12  110 Improved       2004\n         LeaseRemain ResalePrice\n1 94 years 05 months      610000\n2           81 years      540000\n3 80 years 02 months      492500\n4 94 years 04 months      655000\n5           81 years      530000\n\n\n\n\n4 Finding relationships amongst regressors\n\nattach(Punggol)\nplot(Date, ResalePrice, xlab = \"Date\", ylab = \"ResalePrice\", main = \"Relationship between ResalePrice and Date\")\n\n\n\nplot(Type, ResalePrice, xlab = \"Type\", ylab = \"ResalePrice\", main = \"Relationship between ResalePrice and Type\")\n\n\n\nplot(Block, ResalePrice, xlab = \"Block\", ylab = \"ResalePrice\", main = \"Relationship between ResalePrice and Block\")\n\n\n\nplot(Street, ResalePrice, xlab = \"Street\", ylab = \"ResalePrice\", main = \"Relationship between ResalePrice and Street\")\n\n\n\nplot(Story, ResalePrice, xlab = \"Story\", ylab = \"ResalePrice\", main = \"Relationship between ResalePrice and Story\")\n\n\n\nplot(Area, ResalePrice, xlab = \"Area\", ylab = \"ResalePrice\", main = \"Relationship between ResalePrice and Area\")\n\n\n\nplot(Model, ResalePrice, xlab = \"Model\", ylab = \"ResalePrice\", main = \"Relationship between ResalePrice and Model\")\n\n\n\nplot(LeaseBegin, ResalePrice, xlab = \"LeaseBegin\", ylab = \"ResalePrice\", main = \"Relationship between ResalePrice and LeaseBegin\")\n\n\n\nplot(LeaseRemain, ResalePrice, xlab = \"LeaseRemain\", ylab = \"ResalePrice\", main = \"Relationship between ResalePrice and LeaseRemain\")\n\n\n\n\n\n\n5 Data Wrangling\n\nworking_data <- Punggol |> \n  filter(Model != \"Premium Apartment Loft\") |> # Remove single observation for Premium Apartment Loft\n  mutate(Date = as.integer(month(ym(Date))),\n         LeaseRemain = time_length(as.period(as.character(LeaseRemain)), unit = \"month\"), # Convert LeaseRemain to numeric\n         Block = as.character(Block)) |> \n  mutate(Block = as.integer(substr(Block, 1, 3))) |> \n  mutate(Cluster = cut(Block, c(100, 200, 300, 400, 500, 600, 700))) |>  # Clustering of Blocks by 100s\n  droplevels()\n\nattach(working_data)\nsummary(working_data)\n\n      Date            Type          Block                    Street   \n Min.   : 1.000   3 ROOM: 208   Min.   :101.0   PUNGGOL DR      :311  \n 1st Qu.: 3.500   4 ROOM:1048   1st Qu.:209.5   EDGEDALE PLAINS :308  \n Median : 7.000   5 ROOM: 679   Median :302.0   PUNGGOL FIELD   :239  \n Mean   : 6.499                 Mean   :379.6   EDGEFIELD PLAINS:206  \n 3rd Qu.: 9.000                 3rd Qu.:635.0   PUNGGOL WALK    :182  \n Max.   :12.000                 Max.   :684.0   PUNGGOL CTRL    :155  \n                                                (Other)         :534  \n      Story          Area                      Model        LeaseBegin  \n 01 TO 03:219   Min.   : 65.00   Improved         : 301   Min.   :2002  \n 04 TO 06:363   1st Qu.: 92.00   Model A          : 564   1st Qu.:2009  \n 07 TO 09:386   Median : 93.00   Premium Apartment:1070   Median :2015  \n 10 TO 12:385   Mean   : 96.61                            Mean   :2012  \n 13 TO 15:348   3rd Qu.:110.00                            3rd Qu.:2016  \n 16 TO 18:227   Max.   :120.00                            Max.   :2018  \n 19 TO 21:  7                                                           \n  LeaseRemain    ResalePrice          Cluster   \n Min.   : 949   Min.   :377000   (100,200]:422  \n 1st Qu.:1024   1st Qu.:508000   (200,300]:527  \n Median :1100   Median :550000   (300,400]:288  \n Mean   :1074   Mean   :565979   (600,700]:698  \n 3rd Qu.:1122   3rd Qu.:610444                  \n Max.   :1148   Max.   :963800                  \n                                                \n\nhead(working_data, 5)\n\n  Date   Type Block          Street    Story Area    Model LeaseBegin\n1    1 5 ROOM   120 EDGEDALE PLAINS 07 TO 09  113 Improved       2017\n2    1 5 ROOM   171 EDGEDALE PLAINS 16 TO 18  110 Improved       2004\n3    1 5 ROOM   126 EDGEDALE PLAINS 10 TO 12  110 Improved       2003\n4    1 5 ROOM   120 EDGEDALE PLAINS 13 TO 15  113 Improved       2017\n5    1 5 ROOM   174 EDGEDALE PLAINS 10 TO 12  110 Improved       2004\n  LeaseRemain ResalePrice   Cluster\n1        1133      610000 (100,200]\n2         972      540000 (100,200]\n3         962      492500 (100,200]\n4        1132      655000 (100,200]\n5         972      530000 (100,200]\n\nplot(LeaseBegin, ResalePrice, xlab = \"LeaseBegin\", ylab = \"ResalePrice\", main = \"Relationship between ResalePrice and LeaseBegin\")\n\n\n\nplot(LeaseRemain, ResalePrice, xlab = \"LeaseRemain\", ylab = \"ResalePrice\", main = \"Relationship between ResalePrice and LeaseRemain\")\n\n\n\nplot(Cluster, ResalePrice, xlab = \"Cluster\", ylab = \"ResalePrice\", main = \"Relationship between ResalePrice and Cluster\")\n\n\n\npairs(working_data[-ResalePrice])\n\n\n\n\n\n\n6 Eliminating multicollinearity - Type vs Area\n\n# Identifying independent variables which are highly correlated\nplot(Type, Area, xlab = \"Type\", ylab = \"Area\", main = \"Relationship between Area and Type\")\n\n\n\ntypearea_test1 <- lm(Area ~ Type, data = working_data)\nsummary(typearea_test1) # R^2 is almost 1\n\n\nCall:\nlm(formula = Area ~ Type, data = working_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.5592 -0.7791  0.4408  0.5096  8.2209 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  67.4904     0.1223   551.8   <2e-16 ***\nType4 ROOM   25.0688     0.1339   187.2   <2e-16 ***\nType5 ROOM   44.2887     0.1398   316.8   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.764 on 1932 degrees of freedom\nMultiple R-squared:  0.9831,    Adjusted R-squared:  0.9831 \nF-statistic: 5.621e+04 on 2 and 1932 DF,  p-value: < 2.2e-16\n\n# Picking a better regressor\ntypearea_test2 <- lm(ResalePrice ~ Type, data = working_data)\ntypearea_test3 <- lm(ResalePrice ~ Area, data = working_data) # Area has a better R^2\ntypearea_test4 <- lm(ResalePrice ~ Type + Area, data = working_data) # Coefficients of Type become negative, which doesn't make sense, standard errors of Type worsened by a factor of 5\nsummary(typearea_test2)\n\n\nCall:\nlm(formula = ResalePrice ~ Type, data = working_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-194630  -50916    -916   46927  334170 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   434089       5027   86.34   <2e-16 ***\nType4 ROOM    116827       5504   21.23   <2e-16 ***\nType5 ROOM    195541       5746   34.03   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 72510 on 1932 degrees of freedom\nMultiple R-squared:  0.3941,    Adjusted R-squared:  0.3935 \nF-statistic: 628.4 on 2 and 1932 DF,  p-value: < 2.2e-16\n\nsummary(typearea_test3)\n\n\nCall:\nlm(formula = ResalePrice ~ Area, data = working_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-197685  -46603     384   42680  307939 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 118178.5    11232.8   10.52   <2e-16 ***\nArea          4635.2      115.1   40.26   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 68680 on 1933 degrees of freedom\nMultiple R-squared:  0.456, Adjusted R-squared:  0.4558 \nF-statistic:  1621 on 1 and 1933 DF,  p-value: < 2.2e-16\n\nsummary(typearea_test4)\n\n\nCall:\nlm(formula = ResalePrice ~ Type + Area, data = working_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-177662  -40707    -738   38740  253780 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -1035803.7    53711.4  -19.29   <2e-16 ***\nType4 ROOM   -429153.7    20428.4  -21.01   <2e-16 ***\nType5 ROOM   -769036.1    35471.8  -21.68   <2e-16 ***\nArea           21779.3      793.3   27.45   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 61510 on 1931 degrees of freedom\nMultiple R-squared:  0.5642,    Adjusted R-squared:  0.5635 \nF-statistic: 833.3 on 3 and 1931 DF,  p-value: < 2.2e-16\n\nols_vif_tol(typearea_test4) # VIF confirms severe multicollinearity between the two regressors\n\n   Variables   Tolerance       VIF\n1 Type4 ROOM 0.018870789  52.99195\n2 Type5 ROOM 0.006822061 146.58326\n3       Area 0.016893827  59.19322\n\n\n\n\n7 Eliminating multicollinearity - Model vs Area\n\n# Identifying independent variables which are highly correlated\nplot(Model, Area, xlab = \"Model\", ylab = \"Area\", main = \"Relationship between Area and Model\")\n\n\n\nmodelarea_test1 <- lm(Area ~ Model, data = working_data)\nsummary(modelarea_test1)\n\n\nCall:\nlm(formula = Area ~ Model, data = working_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-31.2449  -5.2449  -0.6744   7.0018  21.7551 \n\nCoefficients:\n                       Estimate Std. Error t value Pr(>|t|)    \n(Intercept)            110.6744     0.6288  176.02   <2e-16 ***\nModelModel A           -24.6762     0.7787  -31.69   <2e-16 ***\nModelPremium Apartment -12.4296     0.7117  -17.46   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 10.91 on 1932 degrees of freedom\nMultiple R-squared:  0.3539,    Adjusted R-squared:  0.3532 \nF-statistic:   529 on 2 and 1932 DF,  p-value: < 2.2e-16\n\n# Picking a better regressor\nmodelarea_test2 <- lm(ResalePrice ~ Model, data = working_data)\nmodelarea_test3 <- lm(ResalePrice ~ Area, data = working_data) # Area has a better R^2\nmodelarea_test4 <- lm(ResalePrice ~ Model + Area, data = working_data) \nsummary(modelarea_test2)\n\n\nCall:\nlm(formula = ResalePrice ~ Model, data = working_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-209848  -59787   -9212   50152  368952 \n\nCoefficients:\n                       Estimate Std. Error t value Pr(>|t|)    \n(Intercept)              580212       4839 119.903   <2e-16 ***\nModelModel A             -76599       5993 -12.782   <2e-16 ***\nModelPremium Apartment    14637       5478   2.672   0.0076 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 83950 on 1932 degrees of freedom\nMultiple R-squared:  0.1877,    Adjusted R-squared:  0.1869 \nF-statistic: 223.2 on 2 and 1932 DF,  p-value: < 2.2e-16\n\nsummary(modelarea_test3)\n\n\nCall:\nlm(formula = ResalePrice ~ Area, data = working_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-197685  -46603     384   42680  307939 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 118178.5    11232.8   10.52   <2e-16 ***\nArea          4635.2      115.1   40.26   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 68680 on 1933 degrees of freedom\nMultiple R-squared:  0.456, Adjusted R-squared:  0.4558 \nF-statistic:  1621 on 1 and 1933 DF,  p-value: < 2.2e-16\n\nsummary(modelarea_test4)\n\n\nCall:\nlm(formula = ResalePrice ~ Model + Area, data = working_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-224497  -37913    1735   37679  278961 \n\nCoefficients:\n                       Estimate Std. Error t value Pr(>|t|)    \n(Intercept)             19265.3    15033.8   1.281      0.2    \nModelModel A            48470.7     5560.8   8.716   <2e-16 ***\nModelPremium Apartment  77635.3     4436.4  17.499   <2e-16 ***\nArea                     5068.4      131.8  38.458   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 63190 on 1931 degrees of freedom\nMultiple R-squared:   0.54, Adjusted R-squared:  0.5393 \nF-statistic: 755.7 on 3 and 1931 DF,  p-value: < 2.2e-16\n\nols_vif_tol(modelarea_test4) # VIF does not confirm severe multicollinearity between the two regressors, we should use both\n\n               Variables Tolerance      VIF\n1           ModelModel A 0.3231541 3.094499\n2 ModelPremium Apartment 0.4241684 2.357554\n3                   Area 0.6461368 1.547660\n\n\n\n\n8 Eliminating multicollinearity - Street vs Cluster\n\n# Identifying independent variables which are highly correlated\nstreetcluster_test1 <- lm(ResalePrice ~ Street, data = working_data)\nstreetcluster_test2 <- lm(ResalePrice ~ Cluster, data = working_data) # Cluster has a better R^2\nstreetcluster_test3 <- lm(ResalePrice ~ Cluster + Street, data = working_data)\nsummary(streetcluster_test1)\n\n\nCall:\nlm(formula = ResalePrice ~ Street, data = working_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-219306  -55182   -7306   44592  361531 \n\nCoefficients:\n                         Estimate Std. Error t value Pr(>|t|)    \n(Intercept)                564317       5095 110.751  < 2e-16 ***\nStreetEDGEFIELD PLAINS     -39135       8049  -4.862 1.25e-06 ***\nStreetPUNGGOL CTRL         -31852       8806  -3.617 0.000306 ***\nStreetPUNGGOL DR             7402       7188   1.030 0.303307    \nStreetPUNGGOL EAST         -50824      22279  -2.281 0.022642 *  \nStreetPUNGGOL FIELD          2007       7708   0.260 0.794608    \nStreetPUNGGOL FIELD WALK   -39504      24436  -1.617 0.106130    \nStreetPUNGGOL PL             7092       9404   0.754 0.450876    \nStreetPUNGGOL RD           -49267      15029  -3.278 0.001064 ** \nStreetPUNGGOL WALK          37952       8361   4.539 5.99e-06 ***\nStreetPUNGGOL WAY           54988       9254   5.942 3.33e-09 ***\nStreetSUMANG LANE           23556       9711   2.426 0.015375 *  \nStreetSUMANG LINK          -10684      16379  -0.652 0.514280    \nStreetSUMANG WALK          -15034      13519  -1.112 0.266243    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 89420 on 1921 degrees of freedom\nMultiple R-squared:  0.08365,   Adjusted R-squared:  0.07745 \nF-statistic: 13.49 on 13 and 1921 DF,  p-value: < 2.2e-16\n\nsummary(streetcluster_test2)\n\n\nCall:\nlm(formula = ResalePrice ~ Cluster, data = working_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-218812  -50104   -8092   44902  383708 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(>|t|)    \n(Intercept)        520104       4325 120.259  < 2e-16 ***\nCluster(200,300]    59988       5804  10.336  < 2e-16 ***\nCluster(300,400]    88707       6790  13.063  < 2e-16 ***\nCluster(600,700]    45281       5478   8.265 2.56e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 88840 on 1931 degrees of freedom\nMultiple R-squared:  0.09077,   Adjusted R-squared:  0.08936 \nF-statistic: 64.26 on 3 and 1931 DF,  p-value: < 2.2e-16\n\nsummary(streetcluster_test3)\n\n\nCall:\nlm(formula = ResalePrice ~ Cluster + Street, data = working_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-207263  -50505   -5415   44354  382673 \n\nCoefficients:\n                         Estimate Std. Error t value Pr(>|t|)    \n(Intercept)                537701       6087  88.340  < 2e-16 ***\nCluster(200,300]            59794       9307   6.425 1.66e-10 ***\nCluster(300,400]            94151      11162   8.435  < 2e-16 ***\nCluster(600,700]            49090       6500   7.552 6.57e-14 ***\nStreetEDGEFIELD PLAINS     -40638       7842  -5.182 2.42e-07 ***\nStreetPUNGGOL CTRL         -45780       8753  -5.230 1.88e-07 ***\nStreetPUNGGOL DR           -16375       7566  -2.164 0.030573 *  \nStreetPUNGGOL EAST         -38645      21758  -1.776 0.075874 .  \nStreetPUNGGOL FIELD         -3900       9381  -0.416 0.677645    \nStreetPUNGGOL FIELD WALK   -12887      24060  -0.536 0.592274    \nStreetPUNGGOL PL           -35212      12966  -2.716 0.006674 ** \nStreetPUNGGOL RD           -53130      14648  -3.627 0.000294 ***\nStreetPUNGGOL WALK         -16368      12644  -1.294 0.195649    \nStreetPUNGGOL WAY            1300      13189   0.099 0.921518    \nStreetSUMANG LANE           -9622      13294  -0.724 0.469287    \nStreetSUMANG LINK          -78218      19386  -4.035 5.68e-05 ***\nStreetSUMANG WALK          -52927      16061  -3.295 0.001001 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 87090 on 1918 degrees of freedom\nMultiple R-squared:  0.1321,    Adjusted R-squared:  0.1249 \nF-statistic: 18.25 on 16 and 1918 DF,  p-value: < 2.2e-16\n\nols_vif_tol(streetcluster_test3) # VIF does not confirm severe multicollinearity between the two regressors, we should use both\n\n                  Variables Tolerance      VIF\n1          Cluster(200,300] 0.2283672 4.378912\n2          Cluster(300,400] 0.2483825 4.026048\n3          Cluster(600,700] 0.4023721 2.485262\n4    StreetEDGEFIELD PLAINS 0.6701838 1.492128\n5        StreetPUNGGOL CTRL 0.6943150 1.440269\n6          StreetPUNGGOL DR 0.5076086 1.970022\n7        StreetPUNGGOL EAST 0.9508407 1.051701\n8       StreetPUNGGOL FIELD 0.4114422 2.430475\n9  StreetPUNGGOL FIELD WALK 0.9428194 1.060649\n10         StreetPUNGGOL PL 0.3774534 2.649334\n11         StreetPUNGGOL RD 0.9024796 1.108058\n12       StreetPUNGGOL WALK 0.2877456 3.475292\n13        StreetPUNGGOL WAY 0.3496502 2.860001\n14        StreetSUMANG LANE 0.3904654 2.561046\n15        StreetSUMANG LINK 0.6222217 1.607144\n16        StreetSUMANG WALK 0.5921631 1.688724\n\n\n\n\n9 Eliminating multicollinearity - LeaseBegin vs LeaseRemain\n\n# Identifying independent variables which are highly correlated\nplot(LeaseBegin, LeaseRemain, xlab = \"LeaseBegin\", ylab = \"LeaseRemain\", main = \"Relationship between LeaseRemain and LeaseBegin\")\n\n\n\nlease_test1 <- lm(LeaseRemain ~ LeaseBegin, data = working_data)\nsummary(lease_test1) # R^2 is almost 1\n\n\nCall:\nlm(formula = LeaseRemain ~ LeaseBegin, data = working_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-11.7343  -3.0100  -0.0789   2.9555  12.3346 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -2.301e+04  3.822e+01  -601.9   <2e-16 ***\nLeaseBegin   1.197e+01  1.899e-02   629.9   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.464 on 1933 degrees of freedom\nMultiple R-squared:  0.9952,    Adjusted R-squared:  0.9952 \nF-statistic: 3.968e+05 on 1 and 1933 DF,  p-value: < 2.2e-16\n\nlease_test2 <- lm(ResalePrice ~ LeaseBegin, data = working_data) # LeaseBegin has a better R^2\nlease_test3 <- lm(ResalePrice ~ LeaseRemain, data = working_data)\nlease_test4 <- lm(ResalePrice ~ LeaseBegin + LeaseRemain, data = working_data)\n\nsummary(lease_test2)\n\n\nCall:\nlm(formula = ResalePrice ~ LeaseBegin, data = working_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-209905  -50123   -4687   46531  389548 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -9934281.3   760825.5  -13.06   <2e-16 ***\nLeaseBegin      5217.7      378.1   13.80   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 88850 on 1933 degrees of freedom\nMultiple R-squared:  0.0897,    Adjusted R-squared:  0.08923 \nF-statistic: 190.5 on 1 and 1933 DF,  p-value: < 2.2e-16\n\nsummary(lease_test3)\n\n\nCall:\nlm(formula = ResalePrice ~ LeaseRemain, data = working_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-209660  -51397   -5249   46700  392834 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 129618.39   34115.36   3.799  0.00015 ***\nLeaseRemain    406.40      31.72  12.813  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 89410 on 1933 degrees of freedom\nMultiple R-squared:  0.07829,   Adjusted R-squared:  0.07781 \nF-statistic: 164.2 on 1 and 1933 DF,  p-value: < 2.2e-16\n\nsummary(lease_test4)\n\n\nCall:\nlm(formula = ResalePrice ~ LeaseBegin + LeaseRemain, data = working_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-201928  -49404   -4001   44832  351466 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -140699723   10010559  -14.05   <2e-16 ***\nLeaseBegin       73230       5205   14.07   <2e-16 ***\nLeaseRemain      -5684        434  -13.10   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 85170 on 1932 degrees of freedom\nMultiple R-squared:  0.1639,    Adjusted R-squared:  0.1631 \nF-statistic: 189.4 on 2 and 1932 DF,  p-value: < 2.2e-16\n\nols_vif_tol(lease_test4) # VIF confirms severe multicollinearity between the two regressors\n\n    Variables   Tolerance      VIF\n1  LeaseBegin 0.004847416 206.2955\n2 LeaseRemain 0.004847416 206.2955\n\n\n\n\n10 Identifying significant regressors\n\ndate_test1 <- lm(ResalePrice ~ Date, data = working_data)\nsummary(date_test1) # Most months are significant at the 1% level, but low explanatory power\n\n\nCall:\nlm(formula = ResalePrice ~ Date, data = working_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-201862  -55022  -13004   45046  370974 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 527217.9     4340.2  121.47   <2e-16 ***\nDate          5964.4      587.6   10.15   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 90740 on 1933 degrees of freedom\nMultiple R-squared:  0.0506,    Adjusted R-squared:  0.05011 \nF-statistic:   103 on 1 and 1933 DF,  p-value: < 2.2e-16\n\nmodel_test1 <- lm(ResalePrice ~ Model, data = working_data)\nsummary(model_test1) # Both models are significant at the 1% level, but low explanatory power\n\n\nCall:\nlm(formula = ResalePrice ~ Model, data = working_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-209848  -59787   -9212   50152  368952 \n\nCoefficients:\n                       Estimate Std. Error t value Pr(>|t|)    \n(Intercept)              580212       4839 119.903   <2e-16 ***\nModelModel A             -76599       5993 -12.782   <2e-16 ***\nModelPremium Apartment    14637       5478   2.672   0.0076 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 83950 on 1932 degrees of freedom\nMultiple R-squared:  0.1877,    Adjusted R-squared:  0.1869 \nF-statistic: 223.2 on 2 and 1932 DF,  p-value: < 2.2e-16\n\nstory_test1 <- lm(ResalePrice ~ Story, data = working_data)\nsummary(story_test1) # Most stories are significant at the 1% level, but low explanatory power\n\n\nCall:\nlm(formula = ResalePrice ~ Story, data = working_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-197363  -54539  -12715   42042  386437 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)     526514       6196  84.983  < 2e-16 ***\nStory04 TO 06    26444       7845   3.371 0.000764 ***\nStory07 TO 09    50849       7756   6.556 7.09e-11 ***\nStory10 TO 12    46201       7760   5.954 3.11e-09 ***\nStory13 TO 15    52415       7908   6.628 4.41e-11 ***\nStory16 TO 18    48777       8684   5.617 2.23e-08 ***\nStory19 TO 21     5343      35203   0.152 0.879372    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 91690 on 1928 degrees of freedom\nMultiple R-squared:  0.03319,   Adjusted R-squared:  0.03018 \nF-statistic: 11.03 on 6 and 1928 DF,  p-value: 4.045e-12\n\narea_test1 <- lm(ResalePrice ~ Area, data = working_data)\nsummary(area_test1) # Area is significant at the 1% level, with decent explanatory power\n\n\nCall:\nlm(formula = ResalePrice ~ Area, data = working_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-197685  -46603     384   42680  307939 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 118178.5    11232.8   10.52   <2e-16 ***\nArea          4635.2      115.1   40.26   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 68680 on 1933 degrees of freedom\nMultiple R-squared:  0.456, Adjusted R-squared:  0.4558 \nF-statistic:  1621 on 1 and 1933 DF,  p-value: < 2.2e-16\n\ncluster_test1 <- lm(ResalePrice ~ Cluster, data = working_data)\nsummary(cluster_test1) # All clusters are significant at the 1% level, but low explanatory power\n\n\nCall:\nlm(formula = ResalePrice ~ Cluster, data = working_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-218812  -50104   -8092   44902  383708 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(>|t|)    \n(Intercept)        520104       4325 120.259  < 2e-16 ***\nCluster(200,300]    59988       5804  10.336  < 2e-16 ***\nCluster(300,400]    88707       6790  13.063  < 2e-16 ***\nCluster(600,700]    45281       5478   8.265 2.56e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 88840 on 1931 degrees of freedom\nMultiple R-squared:  0.09077,   Adjusted R-squared:  0.08936 \nF-statistic: 64.26 on 3 and 1931 DF,  p-value: < 2.2e-16\n\nleasebegin_test1 <- lm(ResalePrice ~ LeaseBegin, data = working_data)\nsummary(leasebegin_test1) # Most lease dates are significant at the 1% level, but low explanatory power\n\n\nCall:\nlm(formula = ResalePrice ~ LeaseBegin, data = working_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-209905  -50123   -4687   46531  389548 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -9934281.3   760825.5  -13.06   <2e-16 ***\nLeaseBegin      5217.7      378.1   13.80   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 88850 on 1933 degrees of freedom\nMultiple R-squared:  0.0897,    Adjusted R-squared:  0.08923 \nF-statistic: 190.5 on 1 and 1933 DF,  p-value: < 2.2e-16\n\n\n\n\n11 Preliminary Model, Non-linear Models and Interaction Terms\n\n# Multiple Regression\nreg_all <- lm(ResalePrice ~ ., data = working_data)\nols_vif_tol(reg_all) # Checking for severe multicollinearity\n\n                  Variables   Tolerance        VIF\n1                      Date 0.434163331   2.303281\n2                Type4 ROOM 0.009575553 104.432611\n3                Type5 ROOM 0.003291400 303.822110\n4                     Block 0.008899460 112.366365\n5    StreetEDGEFIELD PLAINS 0.573690854   1.743099\n6        StreetPUNGGOL CTRL 0.444334265   2.250558\n7          StreetPUNGGOL DR 0.406684806   2.458907\n8        StreetPUNGGOL EAST 0.934633176   1.069938\n9       StreetPUNGGOL FIELD 0.357285267   2.798884\n10 StreetPUNGGOL FIELD WALK 0.913204978   1.095044\n11         StreetPUNGGOL PL 0.355431701   2.813480\n12         StreetPUNGGOL RD 0.840878859   1.189232\n13       StreetPUNGGOL WALK 0.263064746   3.801346\n14        StreetPUNGGOL WAY 0.320182292   3.123221\n15        StreetSUMANG LANE 0.333405781   2.999348\n16        StreetSUMANG LINK 0.514224075   1.944678\n17        StreetSUMANG WALK 0.525446820   1.903142\n18            Story04 TO 06 0.454542713   2.200013\n19            Story07 TO 09 0.445704144   2.243641\n20            Story10 TO 12 0.443969197   2.252409\n21            Story13 TO 15 0.459792518   2.174894\n22            Story16 TO 18 0.547354524   1.826969\n23            Story19 TO 21 0.947471398   1.055441\n24                     Area 0.008364364 119.554822\n25             ModelModel A 0.223257527   4.479132\n26   ModelPremium Apartment 0.208342800   4.799782\n27               LeaseBegin 0.001944631 514.236363\n28              LeaseRemain 0.001896379 527.320808\n29         Cluster(200,300] 0.108690020   9.200477\n30         Cluster(300,400] 0.092781874  10.777967\n31         Cluster(600,700] 0.007387516 135.363505\n\nM1 <- lm(ResalePrice ~ LeaseBegin + Story + Area + Model + Date + Street + Cluster, data = working_data)\nols_vif_tol(M1) # Checking for severe multicollinearity\n\n                  Variables Tolerance      VIF\n1                LeaseBegin 0.2892004 3.457810\n2             Story04 TO 06 0.4563890 2.191113\n3             Story07 TO 09 0.4470292 2.236990\n4             Story10 TO 12 0.4455378 2.244478\n5             Story13 TO 15 0.4630344 2.159667\n6             Story16 TO 18 0.5491029 1.821152\n7             Story19 TO 21 0.9498347 1.052815\n8                      Area 0.5999251 1.666875\n9              ModelModel A 0.2647022 3.777831\n10   ModelPremium Apartment 0.2897671 3.451048\n11                     Date 0.9671214 1.033996\n12   StreetEDGEFIELD PLAINS 0.5855148 1.707899\n13       StreetPUNGGOL CTRL 0.5217752 1.916534\n14         StreetPUNGGOL DR 0.4296619 2.327412\n15       StreetPUNGGOL EAST 0.9401903 1.063614\n16      StreetPUNGGOL FIELD 0.3761296 2.658658\n17 StreetPUNGGOL FIELD WALK 0.9137804 1.094355\n18         StreetPUNGGOL PL 0.3633809 2.751933\n19         StreetPUNGGOL RD 0.8505927 1.175651\n20       StreetPUNGGOL WALK 0.2709001 3.691398\n21        StreetPUNGGOL WAY 0.3224179 3.101565\n22        StreetSUMANG LANE 0.3676561 2.719933\n23        StreetSUMANG LINK 0.5374315 1.860702\n24        StreetSUMANG WALK 0.5312496 1.882354\n25         Cluster(200,300] 0.1816827 5.504101\n26         Cluster(300,400] 0.1753884 5.701632\n27         Cluster(600,700] 0.1901081 5.260166\n\n\n\n# Polynomial\nM1_norm1 <- fitdist(resid(M1), distr = \"norm\")\nplot(M1_norm1)\n\n\n\nplot(Area, resid(M1), xlab = \"Area\", ylab = \"Residuals\", main = \"Relationship between Residuals (M1) and Area\")\n\n\n\npoly_data <- working_data |> \n  mutate(AreaRoot = Area^0.5)\n\nroot <- lm(ResalePrice ~ AreaRoot + LeaseBegin + Story + Model + Date + Street + Cluster, data = poly_data)\nsquare <- lm(ResalePrice ~ Area + I(Area)^2 + LeaseBegin + Story + Model + Date + Street + Cluster, data = poly_data)\n\npoly_BIC <- c(BIC(M1), BIC(root), BIC(square))\nplot(poly_BIC,  type = \"b\")\n\n\n\nwhich.min(poly_BIC)\n\n[1] 1\n\n\n\n# Determination of interaction terms based on relationship between Area and other variables\nplot(LeaseBegin, Area, xlab = \"LeaseBegin\", ylab = \"Area\", main = \"Relationship between Area and LeaseBegin\")\n\n\n\nplot(Model, Area, xlab = \"Model\", ylab = \"Area\", main = \"Relationship between Area and Model\")\n\n\n\nplot(Cluster, Area, xlab = \"Cluster\", ylab = \"Area\", main = \"Relationship between Area and Cluster\")\n\n\n\nplot(Street, Area, xlab = \"Street\", ylab = \"Area\", main = \"Relationship between Area and Street\")\n\n\n\nIE_1 <- lm(ResalePrice ~ Area * LeaseBegin + Story + Model + Date + Street + Cluster, data = working_data)\nIE_2 <- lm(ResalePrice ~ Area + LeaseBegin + Story + Area * Model + Date + Street + Cluster, data = working_data)\nIE_3 <- lm(ResalePrice ~ Area + LeaseBegin + Story + Area + Date + Area * Street + Cluster, data = working_data)\nIE_4 <- lm(ResalePrice ~ Area + LeaseBegin + Story + Area + Date + Street + Area * Cluster, data = working_data)\n\nIE_BIC <- c(BIC(M1), BIC(IE_1), BIC(IE_2), BIC(IE_3), BIC(IE_4))\nIE_BIC\n\n[1] 45699.04 45498.79 45600.09 45886.95 45758.80\n\nplot(IE_BIC, type = \"b\")\n\n\n\nwhich.min(IE_BIC)\n\n[1] 2\n\nIE_5 <- lm(ResalePrice ~ Area * LeaseBegin + Story + Area * Model + Date + Street + Cluster, data = working_data)\nIE_BIC <- c(BIC(M1), BIC(IE_1), BIC(IE_2), BIC(IE_5))\nIE_BIC\n\n[1] 45699.04 45498.79 45600.09 45233.87\n\nplot(IE_BIC, type = \"b\")\n\n\n\nwhich.min(IE_BIC)\n\n[1] 4\n\n\n\n# Determination of interaction terms based on relationship between LeaseBegin and other variables\nIE_6 <- lm(ResalePrice ~ Area * LeaseBegin + Story + Area * Model + Date + Street + LeaseBegin * Cluster, data = working_data)\nIE_7 <- lm(ResalePrice ~ Area * LeaseBegin + Story + Area * Model + Date + Street + Cluster + LeaseBegin * Model, data = working_data)\n\nIE_BIC <- c(BIC(IE_5), BIC(IE_6), BIC(IE_7))\nplot(IE_BIC, type = \"b\")\n\n\n\nwhich.min(IE_BIC)\n\n[1] 3\n\n\n\n\n12 Final model\n\nfinal_model <- IE_7\nfinal_model_summary <- summary(final_model)\nfinal_model_summary\n\n\nCall:\nlm(formula = ResalePrice ~ Area * LeaseBegin + Story + Area * \n    Model + Date + Street + Cluster + LeaseBegin * Model, data = working_data)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-95590 -17249  -1387  15640 212012 \n\nCoefficients:\n                                    Estimate Std. Error t value Pr(>|t|)    \n(Intercept)                        2.244e+07  3.455e+06   6.494 1.06e-10 ***\nArea                              -3.837e+05  3.085e+04 -12.437  < 2e-16 ***\nLeaseBegin                        -1.046e+04  1.741e+03  -6.005 2.28e-09 ***\nStory04 TO 06                      2.278e+04  2.286e+03   9.966  < 2e-16 ***\nStory07 TO 09                      4.357e+04  2.255e+03  19.320  < 2e-16 ***\nStory10 TO 12                      5.273e+04  2.263e+03  23.300  < 2e-16 ***\nStory13 TO 15                      6.004e+04  2.308e+03  26.015  < 2e-16 ***\nStory16 TO 18                      6.581e+04  2.528e+03  26.032  < 2e-16 ***\nStory19 TO 21                      6.117e+04  1.029e+04   5.942 3.34e-09 ***\nModelModel A                       2.400e+06  1.160e+06   2.069  0.03868 *  \nModelPremium Apartment            -7.188e+06  9.275e+05  -7.750 1.49e-14 ***\nDate                               5.786e+03  1.744e+02  33.179  < 2e-16 ***\nStreetEDGEFIELD PLAINS             1.935e+04  2.561e+03   7.558 6.35e-14 ***\nStreetPUNGGOL CTRL                 2.963e+04  3.277e+03   9.041  < 2e-16 ***\nStreetPUNGGOL DR                   3.104e+04  2.643e+03  11.744  < 2e-16 ***\nStreetPUNGGOL EAST                 1.011e+04  6.775e+03   1.492  0.13584    \nStreetPUNGGOL FIELD                1.556e+04  3.050e+03   5.101 3.72e-07 ***\nStreetPUNGGOL FIELD WALK           2.164e+04  7.482e+03   2.892  0.00387 ** \nStreetPUNGGOL PL                   2.326e+04  4.032e+03   5.769 9.27e-09 ***\nStreetPUNGGOL RD                   3.795e+04  4.762e+03   7.968 2.75e-15 ***\nStreetPUNGGOL WALK                 5.257e+03  3.977e+03   1.322  0.18636    \nStreetPUNGGOL WAY                  2.203e+04  4.188e+03   5.260 1.60e-07 ***\nStreetSUMANG LANE                 -5.018e+04  4.285e+03 -11.711  < 2e-16 ***\nStreetSUMANG LINK                  2.102e+04  6.382e+03   3.293  0.00101 ** \nStreetSUMANG WALK                 -4.746e+03  5.173e+03  -0.918  0.35896    \nCluster(200,300]                   2.621e+04  3.237e+03   8.096 1.01e-15 ***\nCluster(300,400]                   3.175e+04  4.068e+03   7.804 9.83e-15 ***\nCluster(600,700]                  -1.486e+03  2.959e+03  -0.502  0.61549    \nArea:LeaseBegin                    1.868e+02  1.525e+01  12.247  < 2e-16 ***\nArea:ModelModel A                  1.192e+04  2.518e+03   4.736 2.34e-06 ***\nArea:ModelPremium Apartment        1.377e+04  2.516e+03   5.475 4.96e-08 ***\nLeaseBegin:ModelModel A           -1.866e+03  6.504e+02  -2.869  0.00417 ** \nLeaseBegin:ModelPremium Apartment  2.828e+03  5.528e+02   5.116 3.44e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 26470 on 1902 degrees of freedom\nMultiple R-squared:  0.9205,    Adjusted R-squared:  0.9192 \nF-statistic: 688.1 on 32 and 1902 DF,  p-value: < 2.2e-16\n\nBIC(final_model)\n\n[1] 45126.82\n\nlength(final_model$coefficients)\n\n[1] 33\n\nlibrary(gtsummary)\ntbl_regression(final_model, \n               intercept = TRUE) %>% \n  add_glance_source_note(\n    label = list(sigma ~ \"\\U03C3\"),\n    include = c(r.squared, adj.r.squared, \n                AIC, statistic,\n                p.value, sigma))\n\n\n\n\n\n  \n  \n    \n      Characteristic\n      Beta\n      95% CI1\n      p-value\n    \n  \n  \n    (Intercept)\n22,435,999\n15,660,171, 29,211,827\n<0.001\n    Area\n-383,678\n-444,181, -323,174\n<0.001\n    LeaseBegin\n-10,458\n-13,873, -7,043\n<0.001\n    Story\n\n\n\n        01 TO 03\n—\n—\n\n        04 TO 06\n22,782\n18,299, 27,265\n<0.001\n        07 TO 09\n43,569\n39,147, 47,992\n<0.001\n        10 TO 12\n52,726\n48,288, 57,164\n<0.001\n        13 TO 15\n60,044\n55,517, 64,571\n<0.001\n        16 TO 18\n65,812\n60,854, 70,770\n<0.001\n        19 TO 21\n61,175\n40,984, 81,365\n<0.001\n    Model\n\n\n\n        Improved\n—\n—\n\n        Model A\n2,399,916\n125,037, 4,674,796\n0.039\n        Premium Apartment\n-7,188,089\n-9,007,177, -5,369,000\n<0.001\n    Date\n5,786\n5,444, 6,128\n<0.001\n    Street\n\n\n\n        EDGEDALE PLAINS\n—\n—\n\n        EDGEFIELD PLAINS\n19,355\n14,332, 24,378\n<0.001\n        PUNGGOL CTRL\n29,630\n23,202, 36,057\n<0.001\n        PUNGGOL DR\n31,040\n25,856, 36,223\n<0.001\n        PUNGGOL EAST\n10,109\n-3,178, 23,396\n0.14\n        PUNGGOL FIELD\n15,559\n9,577, 21,541\n<0.001\n        PUNGGOL FIELD WALK\n21,638\n6,965, 36,312\n0.004\n        PUNGGOL PL\n23,260\n15,353, 31,167\n<0.001\n        PUNGGOL RD\n37,948\n28,608, 47,288\n<0.001\n        PUNGGOL WALK\n5,257\n-2,542, 13,056\n0.2\n        PUNGGOL WAY\n22,027\n13,814, 30,240\n<0.001\n        SUMANG LANE\n-50,183\n-58,588, -41,779\n<0.001\n        SUMANG LINK\n21,017\n8,500, 33,534\n0.001\n        SUMANG WALK\n-4,746\n-14,892, 5,399\n0.4\n    Cluster\n\n\n\n        (100,200]\n—\n—\n\n        (200,300]\n26,206\n19,857, 32,554\n<0.001\n        (300,400]\n31,749\n23,770, 39,727\n<0.001\n        (600,700]\n-1,486\n-7,290, 4,317\n0.6\n    Area * LeaseBegin\n187\n157, 217\n<0.001\n    Area * Model\n\n\n\n        Area * Model A\n11,924\n6,987, 16,862\n<0.001\n        Area * Premium Apartment\n13,773\n8,839, 18,706\n<0.001\n    LeaseBegin * Model\n\n\n\n        LeaseBegin * Model A\n-1,866\n-3,141, -590\n0.004\n        LeaseBegin * Premium Apartment\n2,828\n1,744, 3,912\n<0.001\n  \n  \n    \n      R² = 0.920; Adjusted R² = 0.919; AIC = 44,938; Statistic = 688; p-value = <0.001; σ = 26,472\n    \n  \n  \n    \n      1 CI = Confidence Interval\n    \n  \n\n\n\n\n\n\n13 Model Assumptions\n\nfinal_model_norm1 <- fitdist(resid(final_model), distr = \"norm\")\nplot(final_model_norm1)\n\n\n\nplot(final_model$fitted.values, resid(final_model), xlab = \"Predicted ResalePrice\", ylab = \"Residuals\", main = \"Relationship between Residuals and Predicted ResalePrice\")\n\n\n\nplot(Area, resid(final_model), xlab = \"Area\", ylab = \"Residuals\", main = \"Relationship between Residuals and Area\")\n\n\n\nplot(LeaseBegin, resid(final_model), xlab = \"LeaseBegin\", ylab = \"Residuals\", main = \"Relationship between Residuals and LeaseBegin\")\n\n\n\nplot(Story, resid(final_model), xlab = \"Story\", ylab = \"Residuals\", main = \"Relationship between Residuals and Story\")\n\n\n\nplot(Model, resid(final_model), xlab = \"Model\", ylab = \"Residuals\", main = \"Relationship between Residuals and Model\")\n\n\n\nplot(Date, resid(final_model), xlab = \"Date\", ylab = \"Residuals\", main = \"Relationship between Residuals and Date\")\n\n\n\nplot(Street, resid(final_model), xlab = \"Street\", ylab = \"Residuals\", main = \"Relationship between Residuals and Street\")\n\n\n\nplot(Cluster, resid(final_model), xlab = \"Cluster\", ylab = \"Residuals\", main = \"Relationship between Residuals and Cluster\")"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "personalprojects",
    "section": "",
    "text": "Welcome to my website!\nI am You Yun, a penultimate Economics and Data Science undergraduate student at the Singapore Management University. This website serves to offer a glimpse into the projects I have been working on (including some coursework and personal projects).\nFor instance, some course work I have included in the site includes my work from Geospatial Analytics and Statistical Learning with R. A separate site has been created for Geospatial Analytics as part of the coursework, if you would like to explore further. Beyond my coursework, I am trying to work on learning other methods of analysis on my own, including text analytics and machine learning methods. Most of the data used is publicly sourced, and I invite you to try out some of the projects stated in the website along with me!\nPlease feel free to contact me if you have any questions or would like to discuss potential projects."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "Hi! I am You Yun, a Penultimate Economics and Data Science student at Singapore Management University. I enjoy working with data, and this website serves to document some of the personal projects i am exploring."
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "About Me",
    "section": "Education",
    "text": "Education\nSingapore Management University B.A in Economics with a second major in Data Science | Aug 2021 - May 2025"
  },
  {
    "objectID": "about.html#experience",
    "href": "about.html#experience",
    "title": "About Me",
    "section": "Experience",
    "text": "Experience\nEconomic Development Board Resource and Carbon Intern | Jun 2023 - Aug 2023\nCompetition and Consumer Commission Singapore Business and Economics Intern | May 2022 - Aug 2022"
  },
  {
    "objectID": "Text-analytics/Intro-to-text-analytics/Intro-to-text-analytics.html",
    "href": "Text-analytics/Intro-to-text-analytics/Intro-to-text-analytics.html",
    "title": "About Text Analytics",
    "section": "",
    "text": "In exploring more about text mining and text analytics, we reference the website Text Mining with R by Julia Silge and David Robinson, and highlight some key learning points from the textbook. Thereafter, we apply some of the learnings in the following self initiated projects. This section serves to record some of my key learning points from tutorials available publicly."
  },
  {
    "objectID": "Text-analytics/Intro-to-text-analytics/Intro-to-text-analytics.html#tidy-text-format",
    "href": "Text-analytics/Intro-to-text-analytics/Intro-to-text-analytics.html#tidy-text-format",
    "title": "Intro to Text Analytics",
    "section": "0.2 Tidy Text Format",
    "text": "0.2 Tidy Text Format\nAs described by Hadley Wickham, tidy data has a specific structure:\n\nEach variable is a column\nEach observation is a row\nEach type of observational unit is a table\n\nIn the context of text, we define the tidy format as being a table with one-token-per-row, where a token is a meaningful unit of text.\n\n0.2.1 Unnest token function\nMost often, we are faced with paragraphs of words. But in analysing text, the form we want to create is in a token tidy text form. The textbook provided an example text written by Emily Dickinson:\n\nlibrary(dplyr)\n\ntext <- c(\"Because I could not stop for Death -\",\n          \"He kindly stopped for me -\",\n          \"The Carriage held but just Ourselves -\",\n          \"and Immortality\")\ntext_df<-tibble(line=1:4, text=text)\n\nThis is in a tibble form, which is not yet compatible with tidy text analysis. We need to convert this so that it has one-token-per-document-per-row.\n\nlibrary(tidytext)\ntext_df |> \n  unnest_tokens(word, text)\n\n# A tibble: 20 × 2\n    line word       \n   <int> <chr>      \n 1     1 because    \n 2     1 i          \n 3     1 could      \n 4     1 not        \n 5     1 stop       \n 6     1 for        \n 7     1 death      \n 8     2 he         \n 9     2 kindly     \n10     2 stopped    \n11     2 for        \n12     2 me         \n13     3 the        \n14     3 carriage   \n15     3 held       \n16     3 but        \n17     3 just       \n18     3 ourselves  \n19     4 and        \n20     4 immortality\n\n\nThe unnest_token() arguments are column names: i.e. output column name: word, input column that the text comes from: text.\n\n\n0.2.2 Example: tidying the works of Jane Austen\n\n0.2.2.1 Data wrangling\nThe text of Jane Austen’s 6 completed, published novels from the janeaustenr package (Silge 2016) was used.\n\nlibrary(janeaustenr)\nlibrary(dplyr)\nlibrary(stringr)\n\nausten_books()\n\n# A tibble: 73,422 × 2\n   text                    book               \n * <chr>                   <fct>              \n 1 \"SENSE AND SENSIBILITY\" Sense & Sensibility\n 2 \"\"                      Sense & Sensibility\n 3 \"by Jane Austen\"        Sense & Sensibility\n 4 \"\"                      Sense & Sensibility\n 5 \"(1811)\"                Sense & Sensibility\n 6 \"\"                      Sense & Sensibility\n 7 \"\"                      Sense & Sensibility\n 8 \"\"                      Sense & Sensibility\n 9 \"\"                      Sense & Sensibility\n10 \"CHAPTER 1\"             Sense & Sensibility\n# … with 73,412 more rows\n\n\nUsing mutate(), we annotate a linenumber quantity to keep track of lines in the original format, and a chapter.\n\noriginal_books <- austen_books() |> \n  group_by(book) |> \n  mutate(linenumber = row_number(),\n         chapter = cumsum(str_detect(text, \n                                     regex(\"^chapter [\\\\divxlc]\",\n                                           ignore_case = TRUE)))) %>%\n  ungroup()\n\noriginal_books\n\n# A tibble: 73,422 × 4\n   text                    book                linenumber chapter\n   <chr>                   <fct>                    <int>   <int>\n 1 \"SENSE AND SENSIBILITY\" Sense & Sensibility          1       0\n 2 \"\"                      Sense & Sensibility          2       0\n 3 \"by Jane Austen\"        Sense & Sensibility          3       0\n 4 \"\"                      Sense & Sensibility          4       0\n 5 \"(1811)\"                Sense & Sensibility          5       0\n 6 \"\"                      Sense & Sensibility          6       0\n 7 \"\"                      Sense & Sensibility          7       0\n 8 \"\"                      Sense & Sensibility          8       0\n 9 \"\"                      Sense & Sensibility          9       0\n10 \"CHAPTER 1\"             Sense & Sensibility         10       1\n# … with 73,412 more rows\n\n\nTo create a tidy dataset, we restructure it in the one-token-per-row format, using the unnest_tokens() function\n\nlibrary(tidytext)\ntidy_books<-original_books |> \n  unnest_tokens(word, text)\n\ntidy_books\n\n# A tibble: 725,055 × 4\n   book                linenumber chapter word       \n   <fct>                    <int>   <int> <chr>      \n 1 Sense & Sensibility          1       0 sense      \n 2 Sense & Sensibility          1       0 and        \n 3 Sense & Sensibility          1       0 sensibility\n 4 Sense & Sensibility          3       0 by         \n 5 Sense & Sensibility          3       0 jane       \n 6 Sense & Sensibility          3       0 austen     \n 7 Sense & Sensibility          5       0 1811       \n 8 Sense & Sensibility         10       1 chapter    \n 9 Sense & Sensibility         10       1 1          \n10 Sense & Sensibility         13       1 the        \n# … with 725,045 more rows\n\n\nOften in analysis, we want to remove stop words, which are words not useful in analysis. We can do so using anti_join()\n\ntidy_books<-tidy_books |> \n  anti_join(stop_words)\n\ntidy_books\n\n# A tibble: 217,609 × 4\n   book                linenumber chapter word       \n   <fct>                    <int>   <int> <chr>      \n 1 Sense & Sensibility          1       0 sense      \n 2 Sense & Sensibility          1       0 sensibility\n 3 Sense & Sensibility          3       0 jane       \n 4 Sense & Sensibility          3       0 austen     \n 5 Sense & Sensibility          5       0 1811       \n 6 Sense & Sensibility         10       1 chapter    \n 7 Sense & Sensibility         10       1 1          \n 8 Sense & Sensibility         13       1 family     \n 9 Sense & Sensibility         13       1 dashwood   \n10 Sense & Sensibility         13       1 settled    \n# … with 217,599 more rows\n\n\nWe see that after this code chunk, the number of rows reduced significantly (725k to 217k rows).\n\n\n0.2.2.2 Data analysis\n\nWe can use dplyr’s count() to find the most common words in all the books as a whole\n\n\ntidy_books |> \n  count(word, sort=TRUE)\n\n# A tibble: 13,914 × 2\n   word       n\n   <chr>  <int>\n 1 miss    1855\n 2 time    1337\n 3 fanny    862\n 4 dear     822\n 5 lady     817\n 6 sir      806\n 7 day      797\n 8 emma     787\n 9 sister   727\n10 house    699\n# … with 13,904 more rows\n\n\nSome basic visualisations can be done with ggplot\n\nlibrary(ggplot2)\n\ntidy_books |> \n  count(word, sort=TRUE) |> \n  filter(n>600) |> \n  mutate(word=reorder(word, n)) |> \n  ggplot(aes(n, word))+\n  geom_col()+\n  labs(y=NULL)"
  },
  {
    "objectID": "Text-analytics/Intro-to-text-analytics/Intro-to-text-analytics.html#sentiment-analysis-with-tidy-data",
    "href": "Text-analytics/Intro-to-text-analytics/Intro-to-text-analytics.html#sentiment-analysis-with-tidy-data",
    "title": "Intro to Text Analytics",
    "section": "0.3 Sentiment Analysis with Tidy Data",
    "text": "0.3 Sentiment Analysis with Tidy Data\nIn this section, we are referencing both the website Text Mining with R, as well as Kaggle tutorial on sentimental analysis by Rachel Tatman.\nIn this next section, we hope to learn more about sentiment analysis in R. Sentiment analysis is the computational task of automatically determining what feelings a writer is expressing in text. Approaches that analyst can take to conduct sentiment analysis includes\n\nCreate/find a list of words associated with strongly positive or negative sentiment\nCount the number of positive and negative words in the text\nAnalyse the mix of positive and negative words.\n\nIn the tutorial, we will analyse how the sentiment of the State of address, speech given by the President of the United States\n\n#load in the libraries we'll need\nlibrary(tidyverse)\nlibrary(tidytext)\nlibrary(glue)\nlibrary(stringr)\n\nfiles<-list.files(path=\"data\")\nfiles\n\n  [1] \"Adams_1797.txt\"      \"Adams_1798.txt\"      \"Adams_1799.txt\"     \n  [4] \"Adams_1800.txt\"      \"Adams_1825.txt\"      \"Adams_1826.txt\"     \n  [7] \"Adams_1827.txt\"      \"Adams_1828.txt\"      \"Arthur_1881.txt\"    \n [10] \"Arthur_1882.txt\"     \"Arthur_1883.txt\"     \"Arthur_1884.txt\"    \n [13] \"Buchanan_1857.txt\"   \"Buchanan_1858.txt\"   \"Buchanan_1859.txt\"  \n [16] \"Buchanan_1860.txt\"   \"Buren_1837.txt\"      \"Buren_1838.txt\"     \n [19] \"Buren_1839.txt\"      \"Buren_1840.txt\"      \"Bush_1989.txt\"      \n [22] \"Bush_1990.txt\"       \"Bush_1991.txt\"       \"Bush_1992.txt\"      \n [25] \"Bush_2001.txt\"       \"Bush_2002.txt\"       \"Bush_2003.txt\"      \n [28] \"Bush_2004.txt\"       \"Bush_2005.txt\"       \"Bush_2006.txt\"      \n [31] \"Bush_2007.txt\"       \"Bush_2008.txt\"       \"Carter_1978.txt\"    \n [34] \"Carter_1979.txt\"     \"Carter_1980.txt\"     \"Carter_1981.txt\"    \n [37] \"Cleveland_1885.txt\"  \"Cleveland_1886.txt\"  \"Cleveland_1887.txt\" \n [40] \"Cleveland_1888.txt\"  \"Cleveland_1893.txt\"  \"Cleveland_1894.txt\" \n [43] \"Cleveland_1895.txt\"  \"Cleveland_1896.txt\"  \"Clinton_1993.txt\"   \n [46] \"Clinton_1994.txt\"    \"Clinton_1995.txt\"    \"Clinton_1996.txt\"   \n [49] \"Clinton_1997.txt\"    \"Clinton_1998.txt\"    \"Clinton_1999.txt\"   \n [52] \"Clinton_2000.txt\"    \"Coolidge_1923.txt\"   \"Coolidge_1924.txt\"  \n [55] \"Coolidge_1925.txt\"   \"Coolidge_1926.txt\"   \"Coolidge_1927.txt\"  \n [58] \"Coolidge_1928.txt\"   \"Eisenhower_1954.txt\" \"Eisenhower_1955.txt\"\n [61] \"Eisenhower_1956.txt\" \"Eisenhower_1957.txt\" \"Eisenhower_1958.txt\"\n [64] \"Eisenhower_1959.txt\" \"Eisenhower_1960.txt\" \"Eisenhower_1961.txt\"\n [67] \"Fillmore_1850.txt\"   \"Fillmore_1851.txt\"   \"Fillmore_1852.txt\"  \n [70] \"Ford_1975.txt\"       \"Ford_1976.txt\"       \"Ford_1977.txt\"      \n [73] \"Grant_1869.txt\"      \"Grant_1870.txt\"      \"Grant_1871.txt\"     \n [76] \"Grant_1872.txt\"      \"Grant_1873.txt\"      \"Grant_1874.txt\"     \n [79] \"Grant_1875.txt\"      \"Grant_1876.txt\"      \"Harding_1921.txt\"   \n [82] \"Harding_1922.txt\"    \"Harrison_1889.txt\"   \"Harrison_1890.txt\"  \n [85] \"Harrison_1891.txt\"   \"Harrison_1892.txt\"   \"Hayes_1877.txt\"     \n [88] \"Hayes_1878.txt\"      \"Hayes_1879.txt\"      \"Hayes_1880.txt\"     \n [91] \"Hoover_1929.txt\"     \"Hoover_1930.txt\"     \"Hoover_1931.txt\"    \n [94] \"Hoover_1932.txt\"     \"Jackson_1829.txt\"    \"Jackson_1830.txt\"   \n [97] \"Jackson_1831.txt\"    \"Jackson_1832.txt\"    \"Jackson_1833.txt\"   \n[100] \"Jackson_1834.txt\"    \"Jackson_1835.txt\"    \"Jackson_1836.txt\"   \n[103] \"Jefferson_1801.txt\"  \"Jefferson_1802.txt\"  \"Jefferson_1803.txt\" \n[106] \"Jefferson_1804.txt\"  \"Jefferson_1805.txt\"  \"Jefferson_1806.txt\" \n[109] \"Jefferson_1807.txt\"  \"Jefferson_1808.txt\"  \"Johnson_1865.txt\"   \n[112] \"Johnson_1866.txt\"    \"Johnson_1867.txt\"    \"Johnson_1868.txt\"   \n[115] \"Johnson_1964.txt\"    \"Johnson_1965.txt\"    \"Johnson_1966.txt\"   \n[118] \"Johnson_1967.txt\"    \"Johnson_1968.txt\"    \"Johnson_1969.txt\"   \n[121] \"Kennedy_1962.txt\"    \"Kennedy_1963.txt\"    \"Lincoln_1861.txt\"   \n[124] \"Lincoln_1862.txt\"    \"Lincoln_1863.txt\"    \"Lincoln_1864.txt\"   \n[127] \"Madison_1809.txt\"    \"Madison_1810.txt\"    \"Madison_1811.txt\"   \n[130] \"Madison_1812.txt\"    \"Madison_1813.txt\"    \"Madison_1814.txt\"   \n[133] \"Madison_1815.txt\"    \"Madison_1816.txt\"    \"McKinley_1897.txt\"  \n[136] \"McKinley_1898.txt\"   \"McKinley_1899.txt\"   \"McKinley_1900.txt\"  \n[139] \"Monroe_1817.txt\"     \"Monroe_1818.txt\"     \"Monroe_1819.txt\"    \n[142] \"Monroe_1820.txt\"     \"Monroe_1821.txt\"     \"Monroe_1822.txt\"    \n[145] \"Monroe_1823.txt\"     \"Monroe_1824.txt\"     \"Nixon_1970.txt\"     \n[148] \"Nixon_1971.txt\"      \"Nixon_1972.txt\"      \"Nixon_1973.txt\"     \n[151] \"Nixon_1974.txt\"      \"Obama_2009.txt\"      \"Obama_2010.txt\"     \n[154] \"Obama_2011.txt\"      \"Obama_2012.txt\"      \"Obama_2013.txt\"     \n[157] \"Obama_2014.txt\"      \"Obama_2015.txt\"      \"Obama_2016.txt\"     \n[160] \"Pierce_1853.txt\"     \"Pierce_1854.txt\"     \"Pierce_1855.txt\"    \n[163] \"Pierce_1856.txt\"     \"Polk_1845.txt\"       \"Polk_1846.txt\"      \n[166] \"Polk_1847.txt\"       \"Polk_1848.txt\"       \"Reagan_1982.txt\"    \n[169] \"Reagan_1983.txt\"     \"Reagan_1984.txt\"     \"Reagan_1985.txt\"    \n[172] \"Reagan_1986.txt\"     \"Reagan_1987.txt\"     \"Reagan_1988.txt\"    \n[175] \"Roosevelt_1901.txt\"  \"Roosevelt_1902.txt\"  \"Roosevelt_1903.txt\" \n[178] \"Roosevelt_1904.txt\"  \"Roosevelt_1905.txt\"  \"Roosevelt_1906.txt\" \n[181] \"Roosevelt_1907.txt\"  \"Roosevelt_1908.txt\"  \"Roosevelt_1934.txt\" \n[184] \"Roosevelt_1935.txt\"  \"Roosevelt_1936.txt\"  \"Roosevelt_1937.txt\" \n[187] \"Roosevelt_1938.txt\"  \"Roosevelt_1939.txt\"  \"Roosevelt_1940.txt\" \n[190] \"Roosevelt_1941.txt\"  \"Roosevelt_1942.txt\"  \"Roosevelt_1943.txt\" \n[193] \"Roosevelt_1944.txt\"  \"Roosevelt_1945.txt\"  \"sotu\"               \n[196] \"Taft_1909.txt\"       \"Taft_1910.txt\"       \"Taft_1911.txt\"      \n[199] \"Taft_1912.txt\"       \"Taylor_1849.txt\"     \"Truman_1946.txt\"    \n[202] \"Truman_1947.txt\"     \"Truman_1948.txt\"     \"Truman_1949.txt\"    \n[205] \"Truman_1950.txt\"     \"Truman_1951.txt\"     \"Truman_1952.txt\"    \n[208] \"Truman_1953.txt\"     \"Trump_2017.txt\"      \"Trump_2018.txt\"     \n[211] \"Tyler_1841.txt\"      \"Tyler_1842.txt\"      \"Tyler_1843.txt\"     \n[214] \"Tyler_1844.txt\"      \"Washington_1790.txt\" \"Washington_1791.txt\"\n[217] \"Washington_1792.txt\" \"Washington_1793.txt\" \"Washington_1794.txt\"\n[220] \"Washington_1795.txt\" \"Washington_1796.txt\" \"Wilson_1913.txt\"    \n[223] \"Wilson_1914.txt\"     \"Wilson_1915.txt\"     \"Wilson_1916.txt\"    \n[226] \"Wilson_1917.txt\"     \"Wilson_1918.txt\"     \"Wilson_1919.txt\"    \n[229] \"Wilson_1920.txt\""
  },
  {
    "objectID": "Geospatial/Take-home_Ex01/Take-home_Ex01.html",
    "href": "Geospatial/Take-home_Ex01/Take-home_Ex01.html",
    "title": "Take-home Exercise 1",
    "section": "",
    "text": "In this take home exercise, we are tasked to apply appropriate spatial point pattern analysis methods to discover the geographical distribution of functional and non-functional waterpoints and their colocations if any in Osun state, Nigeria.\nThe main tasks of this exercise includes\n\nExploratory Spatial Data Analaysis\nSecond-order Spatial Point Pattern Analysis\nSpatial Correlation Analysis\n\n\n\nWe first start off by loading the necessary R packages into our platform.\n\npacman::p_load(maptools, sf, raster, spatstat, tmap, tidyverse, funModeling, sfdep)\n\n\n\n\n\n\nThis study will focus of Osun State, Nigeria. The state boundary GIS data of Nigeria can be downloaded from geoBoundaries.\n\n\nWithin the geoboundaries data, we choose to use ADM2 data given that we want to investigate the distribution of water pumps within the LGAs in Osun.\n\ngeoNGA2 <- st_read(dsn = \"data/geospatial\", layer = \"nga_admbnda_adm2_osgof_20190417\") |>  st_transform(crs = 26392) |> \n  arrange(ADM2_EN)\n\nReading layer `nga_admbnda_adm2_osgof_20190417' from data source \n  `/Users/pengyouyun/youyunpeng/personalprojects/Geospatial/Take-home_Ex01/data/geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 774 features and 16 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2.668534 ymin: 4.273007 xmax: 14.67882 ymax: 13.89442\nGeodetic CRS:  WGS 84\n\n#check for duplicates\ngeoNGA2$ADM2_EN[duplicated(geoNGA2$ADM2_EN) == TRUE]\n\n[1] \"Bassa\"    \"Ifelodun\" \"Irepodun\" \"Nasarawa\" \"Obi\"      \"Surulere\"\n\nduplicated_LGA <- geoNGA2$ADM2_EN[duplicated(geoNGA2$ADM2_EN) == TRUE]\n# Get all the indices with names that are included in the duplicated LGA names\nduplicated_indices <- which(geoNGA2$ADM2_EN %in% duplicated_LGA)\n\ngeoNGA2$ADM2_EN[94] <- \"Bassa, Kogi\"\ngeoNGA2$ADM2_EN[95] <- \"Bassa, Plateau\"\ngeoNGA2$ADM2_EN[304] <- \"Ifelodun, Kwara\"\ngeoNGA2$ADM2_EN[305] <- \"Ifelodun, Osun\"\ngeoNGA2$ADM2_EN[355] <- \"Irepodun, Kwara\"\ngeoNGA2$ADM2_EN[356] <- \"Irepodun, Osun\"\ngeoNGA2$ADM2_EN[519] <- \"Nasarawa, Kano\"\ngeoNGA2$ADM2_EN[520] <- \"Nasarawa, Nasarawa\"\ngeoNGA2$ADM2_EN[546] <- \"Obi, Benue\"\ngeoNGA2$ADM2_EN[547] <- \"Obi, Nasarawa\"\ngeoNGA2$ADM2_EN[693] <- \"Surulere, Lagos\"\ngeoNGA2$ADM2_EN[694] <- \"Surulere, Oyo\"\n\nThe code chunk below filters out geoNGA2 into Osun state Local Government Area\n\nosun_LGA <- c(\"Aiyedade\",\"Aiyedire\",\"Atakumosa East\",   \"Atakumosa West\",   \n              \"Ede North\",  \"Ede South\",    \"Egbedore\", \"Ejigbo\",   \"Ife Central\",  \n              \"Ife East\",   \"Ife North\",    \"Ife South\",    \"Ifedayo\",  \"Ila\",\n              \"Ifelodun, Osun\",\"Irepodun, Osun\",\"Ilesha East\",  \"Ilesha West\",\n              \"Irewole\",    \"Isokan\",   \"Iwo\",  \"Obokun\",   \"Odo-Otin\", \"Ola-oluwa\",    \n              \"Olorunda\",   \"Oriade\",   \"Orolu\",    \"Osogbo\", \"Boripe\", \"Boluwaduro\")\n\nbd <- geoNGA2 |> \n  filter(ADM2_EN %in% osun_LGA) #create border sf that filters out Osun LGAs\n\nqtm(bd) #checking if the border data is correctly filtered\n\n\n\n\n\n\n\n\nFor the purpose of this assignment, data from WPdx Global Data Repositories will be used.\n\n\nAgain, in the waterpoint data can be narrowed down to only osun state.\n\nwp_osun <- read_csv(\"data/aspatial/WPdx.csv\") %>%\n  filter(`#clean_country_name` == \"Nigeria\", `#clean_adm1`==\"Osun\")\n\nWith our packages and data in place, we can now start with our analysis!"
  },
  {
    "objectID": "Geospatial/Take-home_Ex01/Take-home_Ex01.html#data-conversion-from-sf-to-ppp",
    "href": "Geospatial/Take-home_Ex01/Take-home_Ex01.html#data-conversion-from-sf-to-ppp",
    "title": "Take-home Exercise 1",
    "section": "2.1 Data conversion from sf to ppp",
    "text": "2.1 Data conversion from sf to ppp\n\n2.1.1 Converting water point data into sf point features\nFirst we need to convert the wkt field into sfc field by using st_as_sfc() data type. Next we will convert the tibble data.frame into an sf object by isomh st_sf(). it is also important for us to include the referencing system of the data into the sf object. In this case, it has the CRS of WGS 84, so we set the crs to EPSG code 4326.\n\nwp_osun$Geometry = st_as_sfc(wp_osun$`New Georeferenced Column`)\nwp_osun\n\n# A tibble: 5,557 × 71\n   row_id `#source`      #lat_…¹ #lon_…² #repo…³ #stat…⁴ #wate…⁵ #wate…⁶ #wate…⁷\n    <dbl> <chr>            <dbl>   <dbl> <chr>   <chr>   <chr>   <chr>   <chr>  \n 1 429123 GRID3             8.02    5.06 08/29/… Unknown <NA>    <NA>    Tapsta…\n 2  70566 Federal Minis…    7.32    4.79 05/11/… No      Protec… Well    Mechan…\n 3  70578 Federal Minis…    7.76    4.56 05/11/… No      Boreho… Well    Mechan…\n 4  66401 Federal Minis…    8.03    4.64 04/30/… No      Boreho… Well    Mechan…\n 5 422190 GRID3             7.87    4.88 08/29/… Unknown <NA>    <NA>    Tapsta…\n 6 422064 GRID3             7.7     4.89 08/29/… Unknown <NA>    <NA>    Tapsta…\n 7  65607 Federal Minis…    7.89    4.71 05/12/… No      Boreho… Well    Mechan…\n 8  68989 Federal Minis…    7.51    4.27 05/07/… No      Boreho… Well    <NA>   \n 9  67708 Federal Minis…    7.48    4.35 04/29/… Yes     Boreho… Well    Mechan…\n10  66419 Federal Minis…    7.63    4.50 05/08/… Yes     Boreho… Well    Hand P…\n# … with 5,547 more rows, 62 more variables: `#water_tech_category` <chr>,\n#   `#facility_type` <chr>, `#clean_country_name` <chr>, `#clean_adm1` <chr>,\n#   `#clean_adm2` <chr>, `#clean_adm3` <chr>, `#clean_adm4` <chr>,\n#   `#install_year` <dbl>, `#installer` <chr>, `#rehab_year` <lgl>,\n#   `#rehabilitator` <lgl>, `#management_clean` <chr>, `#status_clean` <chr>,\n#   `#pay` <chr>, `#fecal_coliform_presence` <chr>,\n#   `#fecal_coliform_value` <dbl>, `#subjective_quality` <chr>, …\n\nwp_sf <-  st_sf(wp_osun, crs=4326) #convert to sf, tell R what crs used for projection \nwp_sf\n\nSimple feature collection with 5557 features and 70 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 4.032004 ymin: 7.060309 xmax: 5.06 ymax: 8.061898\nGeodetic CRS:  WGS 84\n# A tibble: 5,557 × 71\n   row_id `#source`      #lat_…¹ #lon_…² #repo…³ #stat…⁴ #wate…⁵ #wate…⁶ #wate…⁷\n *  <dbl> <chr>            <dbl>   <dbl> <chr>   <chr>   <chr>   <chr>   <chr>  \n 1 429123 GRID3             8.02    5.06 08/29/… Unknown <NA>    <NA>    Tapsta…\n 2  70566 Federal Minis…    7.32    4.79 05/11/… No      Protec… Well    Mechan…\n 3  70578 Federal Minis…    7.76    4.56 05/11/… No      Boreho… Well    Mechan…\n 4  66401 Federal Minis…    8.03    4.64 04/30/… No      Boreho… Well    Mechan…\n 5 422190 GRID3             7.87    4.88 08/29/… Unknown <NA>    <NA>    Tapsta…\n 6 422064 GRID3             7.7     4.89 08/29/… Unknown <NA>    <NA>    Tapsta…\n 7  65607 Federal Minis…    7.89    4.71 05/12/… No      Boreho… Well    Mechan…\n 8  68989 Federal Minis…    7.51    4.27 05/07/… No      Boreho… Well    <NA>   \n 9  67708 Federal Minis…    7.48    4.35 04/29/… Yes     Boreho… Well    Mechan…\n10  66419 Federal Minis…    7.63    4.50 05/08/… Yes     Boreho… Well    Hand P…\n# … with 5,547 more rows, 62 more variables: `#water_tech_category` <chr>,\n#   `#facility_type` <chr>, `#clean_country_name` <chr>, `#clean_adm1` <chr>,\n#   `#clean_adm2` <chr>, `#clean_adm3` <chr>, `#clean_adm4` <chr>,\n#   `#install_year` <dbl>, `#installer` <chr>, `#rehab_year` <lgl>,\n#   `#rehabilitator` <lgl>, `#management_clean` <chr>, `#status_clean` <chr>,\n#   `#pay` <chr>, `#fecal_coliform_presence` <chr>,\n#   `#fecal_coliform_value` <dbl>, `#subjective_quality` <chr>, …\n\n\nHowever, we want to convert the coordinate reference system to Nigeria’s projected coordinate system. We use st_transform(), and include the EPSG code for Nigeria’s projected coordinate system: 26392.\n\nwp_sf <- wp_sf %>%\n  st_transform(crs = 26392)\n\nqtm(wp_sf) #quick view\n\n\n\n\n\n\n2.1.2 Data wrangling for waterpoint data\nIn cleaning the waterpoint data, we first rename the column from #status_clean to status_clean for easier handling in subsequent steps. select() of dplyr is used to include status_clean in the output sf data.frame. - mutate() and replace_na() are used to recode all the NA values in status_clean into unknown.\n\nwp_sf_nga <- wp_sf |> \n  rename(status_clean = '#status_clean') |> \n  select(status_clean) |> \n  mutate(status_clean = replace_na(\n    status_clean, \"unknown\"\n  ))\n\n\n\n2.1.3 Extracting water point data\nNow we are ready to extract the water point data according to their status.\nThe code chunk below is used to extract functional water points.\n\nwp_functional <- wp_sf_nga |> \n  filter(status_clean %in%\n           c(\"Functional\",\n             \"Functional but not in use\",\n             \"Functional but needs repair\"))\n\nThe code chunk below is used to extract nonfunctional waterpoint.\n\nwp_nonfunctional <- wp_sf_nga |> \n  filter(status_clean %in% \n           c(\"Abandoned/Decommissioned\",\n             \"Abandoned\",\n             \"Non-Functional due to dry season\",\n             \"Non-Functional\",\n             \"Non functional due to dry season\"))\n\nThe code chunk below is used to extract water point with unknown status.\n\nwp_unknown <- wp_sf_nga |> \n  filter(status_clean %in% \n           c(\"unknown\"))\n\nNext, the code chunk below is used to perform a quick EDA on the derived sf data.frames.\n\nfreq(data = wp_functional,\n     input = 'status_clean')\n\n\n\n\n                 status_clean frequency percentage cumulative_perc\n1                  Functional      2319      88.17           88.17\n2 Functional but needs repair       248       9.43           97.60\n3   Functional but not in use        63       2.40          100.00\n\n\n\nfreq(data = wp_nonfunctional,\n     input = 'status_clean')\n\n\n\n\n                      status_clean frequency percentage cumulative_perc\n1                   Non-Functional      2008      92.15           92.15\n2 Non-Functional due to dry season       151       6.93           99.08\n3                        Abandoned        15       0.69           99.77\n4         Abandoned/Decommissioned         5       0.23          100.00"
  },
  {
    "objectID": "Geospatial/Take-home_Ex01/Take-home_Ex01.html#data-wrangling-to-prepare-data-for-spatstat",
    "href": "Geospatial/Take-home_Ex01/Take-home_Ex01.html#data-wrangling-to-prepare-data-for-spatstat",
    "title": "Take-home Exercise 1",
    "section": "2.2 Data Wrangling to Prepare Data for Spatstat",
    "text": "2.2 Data Wrangling to Prepare Data for Spatstat\n\n2.2.1 Converting sf data frames to sp’s Spatial Class\nThe code chunk below uses as_Spatial(). of sf package to convert the three geospatial data from simple feature data frame to sp’s Spatial* class.\n\nNGA_bd_sc<-as(bd, \"Spatial\")\nNGA_bd_sc\n\nclass       : SpatialPolygonsDataFrame \nfeatures    : 30 \nextent      : 176503.2, 291043.8, 331434.7, 454520.1  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=4 +lon_0=8.5 +k=0.99975 +x_0=670553.98 +y_0=0 +a=6378249.145 +rf=293.465 +towgs84=-92,-93,122,0,0,0,0 +units=m +no_defs \nvariables   : 16\nnames       :    Shape_Leng,       Shape_Area,  ADM2_EN, ADM2_PCODE, ADM2_REF, ADM2ALT1EN, ADM2ALT2EN, ADM1_EN, ADM1_PCODE, ADM0_EN, ADM0_PCODE,  date, validOn, validTo,        SD_EN, ... \nmin values  : 0.26445678806, 0.00248649736648, Aiyedade,   NG030001, Aiyedade,         NA,         NA,    Osun,      NG030, Nigeria,         NG, 17134,   18003,      NA, Osun Central, ... \nmax values  :  1.8470166597,  0.0737271661922,   Osogbo,   NG030030,   Osogbo,         NA,         NA,    Osun,      NG030, Nigeria,         NG, 17134,   18003,      NA,    Osun West, ... \n\nfunc<-as(wp_functional, \"Spatial\")\nfunc\n\nclass       : SpatialPointsDataFrame \nfeatures    : 2630 \nextent      : 177285.9, 290751, 343128.1, 450859.7  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=4 +lon_0=8.5 +k=0.99975 +x_0=670553.98 +y_0=0 +a=6378249.145 +rf=293.465 +towgs84=-92,-93,122,0,0,0,0 +units=m +no_defs \nvariables   : 1\nnames       :              status_clean \nmin values  :                Functional \nmax values  : Functional but not in use \n\nnonfunc<-as(wp_nonfunctional, \"Spatial\")\nnonfunc\n\nclass       : SpatialPointsDataFrame \nfeatures    : 2179 \nextent      : 180539, 290616, 340054.1, 450780.1  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=4 +lon_0=8.5 +k=0.99975 +x_0=670553.98 +y_0=0 +a=6378249.145 +rf=293.465 +towgs84=-92,-93,122,0,0,0,0 +units=m +no_defs \nvariables   : 1\nnames       :                     status_clean \nmin values  :                        Abandoned \nmax values  : Non-Functional due to dry season \n\n\n\n\n2.2.2 Converting the Spatial* class into generic sp format\nspatstat requires the analytical data in ppp object form. There is no direct way to convert a Spatial* classes into ppp object. We need to convert the Spatial classes* into Spatial object first.\nThe codes chunk below converts the Spatial* classes into generic sp objects.\n\nNGA_bd_sp <- as(NGA_bd_sc, \"SpatialPolygons\")\n\nfuncsp<-as(func, \"SpatialPoints\")\nnonfuncsp<-as(nonfunc, \"SpatialPoints\")\n\n\n\n2.2.3 Convert generic sp format into spatstat’s ppp format\n\nfuncppp<-as(funcsp, \"ppp\")\nnonfuncppp<-as(nonfuncsp,\"ppp\")\n\n\n\n2.2.4 Checking for Duplicated Points\nWe can check for the duplication in a ppp object by using the code chunk below. We see that there are no duplicated points.\n\nany(duplicated(funcppp))\n\n[1] FALSE\n\nany(duplicated(nonfuncppp))\n\n[1] FALSE\n\n\n\n\n2.2.5 Creating Owin Object\nTo confine our analysis with the geographical area Osun, we create an object called owin in spatstat to represent the polygonal region.\nThe code chunk below is used to convert NGA_bd_sp into owin object of spatstat.\n\nNGA_owin<- as(NGA_bd_sp, \"owin\")\n\n\n\n2.2.6 Combine point events object and owin object\nIn this last step of geospatial data wrangling, we will extract the waterpoints that are located within Osun by using the code chunk below.\n\n#funcppp\nfuncppp1<-funcppp[NGA_owin]\nsummary(funcppp1)\n\nPlanar point pattern:  2529 points\nAverage intensity 2.928471e-07 points per square unit\n\nCoordinates are given to 2 decimal places\ni.e. rounded to the nearest multiple of 0.01 units\n\nWindow: polygonal boundary\n30 separate polygons (no holes)\n            vertices      area relative.area\npolygon 1        204 766084000       0.08870\npolygon 2         81 304399000       0.03520\npolygon 3         97 465688000       0.05390\npolygon 4        124 373051000       0.04320\npolygon 5         60 149473000       0.01730\npolygon 6         84 144820000       0.01680\npolygon 7         50 102243000       0.01180\npolygon 8         72 216002000       0.02500\npolygon 9        112 269897000       0.03130\npolygon 10       125 365142000       0.04230\npolygon 11        83 111191000       0.01290\npolygon 12       126 192557000       0.02230\npolygon 13       219 904397000       0.10500\npolygon 14       174 741131000       0.08580\npolygon 15        81 138742000       0.01610\npolygon 16        65 119452000       0.01380\npolygon 17        90 280205000       0.03240\npolygon 18        69  69814600       0.00808\npolygon 19        69  42727500       0.00495\npolygon 20        49  30458800       0.00353\npolygon 21        62 263505000       0.03050\npolygon 22        93 438930000       0.05080\npolygon 23        87 274127000       0.03170\npolygon 24       105 509979000       0.05910\npolygon 25        98 292058000       0.03380\npolygon 26        64 327765000       0.03800\npolygon 27       133 108945000       0.01260\npolygon 28       122 462169000       0.05350\npolygon 29        94 109715000       0.01270\npolygon 30        95  61239800       0.00709\nenclosing rectangle: [176503.22, 291043.82] x [331434.7, 454520.1] units\n                     (114500 x 123100 units)\nWindow area = 8635910000 square units\nFraction of frame area: 0.613\n\nplot(funcppp1)\n\n\n\n#nonfuncppp\nnonfuncppp1<-nonfuncppp[NGA_owin]\nsummary(nonfuncppp1)\n\nPlanar point pattern:  2059 points\nAverage intensity 2.384232e-07 points per square unit\n\nCoordinates are given to 2 decimal places\ni.e. rounded to the nearest multiple of 0.01 units\n\nWindow: polygonal boundary\n30 separate polygons (no holes)\n            vertices      area relative.area\npolygon 1        204 766084000       0.08870\npolygon 2         81 304399000       0.03520\npolygon 3         97 465688000       0.05390\npolygon 4        124 373051000       0.04320\npolygon 5         60 149473000       0.01730\npolygon 6         84 144820000       0.01680\npolygon 7         50 102243000       0.01180\npolygon 8         72 216002000       0.02500\npolygon 9        112 269897000       0.03130\npolygon 10       125 365142000       0.04230\npolygon 11        83 111191000       0.01290\npolygon 12       126 192557000       0.02230\npolygon 13       219 904397000       0.10500\npolygon 14       174 741131000       0.08580\npolygon 15        81 138742000       0.01610\npolygon 16        65 119452000       0.01380\npolygon 17        90 280205000       0.03240\npolygon 18        69  69814600       0.00808\npolygon 19        69  42727500       0.00495\npolygon 20        49  30458800       0.00353\npolygon 21        62 263505000       0.03050\npolygon 22        93 438930000       0.05080\npolygon 23        87 274127000       0.03170\npolygon 24       105 509979000       0.05910\npolygon 25        98 292058000       0.03380\npolygon 26        64 327765000       0.03800\npolygon 27       133 108945000       0.01260\npolygon 28       122 462169000       0.05350\npolygon 29        94 109715000       0.01270\npolygon 30        95  61239800       0.00709\nenclosing rectangle: [176503.22, 291043.82] x [331434.7, 454520.1] units\n                     (114500 x 123100 units)\nWindow area = 8635910000 square units\nFraction of frame area: 0.613\n\nplot(nonfuncppp1)\n\n\n\n\n\n\n2.2.7 Rescale ppp data into km\nIn the code chunk below, rescale() is used to convert the unit of measurement from meter to kilometer\n\nfuncppp1.km<-rescale(funcppp1, 1000, \"km\")\nnonfuncppp1.km<-rescale(nonfuncppp1, 1000, \"km\")\n\nWith our data cleaned and in thr right format, we can move on to compute the Kernel Density Estimation!"
  },
  {
    "objectID": "Geospatial/Take-home_Ex01/Take-home_Ex01.html#first-order-spatial-point-pattern-analysis",
    "href": "Geospatial/Take-home_Ex01/Take-home_Ex01.html#first-order-spatial-point-pattern-analysis",
    "title": "Take-home Exercise 1",
    "section": "2.3 First-order Spatial Point Pattern Analysis",
    "text": "2.3 First-order Spatial Point Pattern Analysis\n\n2.3.1 Computing kernel density Estimation using adaptive bandwidth selection method\nIn spatial analysis, the choice of bandwidth is important for determining the smoothness of a surface fit to the data. The bandwidth determines the width of the smoothing kernel used in spatial smoothing techniques like kernel density estimation or local regression. We compare between two types of bandwidth methods, fixed and adaptive bandwidth, to be applied to this situation.\nFixed bandwidth methods use a constant value for the bandwidth throughout the analysis, regardless of the distribution of the data. This can be useful when the data has a consistent structure, but if the data is highly variable or has multiple modes, a constant bandwidth may not provide an adequate fit to the data.\nAdaptive bandwidth methods, on the other hand, use a variable bandwidth that adjusts based on the local structure of the data. This allows for more flexibility in the fitting process.\nFrom our initial plots, we can see that there is some evidence of clustering, leading to our choice of using adaptive bandwidth in our analysis.\n\nfunckde_adaptive<- adaptive.density(funcppp1.km, method=\"kernel\")\n\nnonfunckde_adaptive<- adaptive.density(nonfuncppp1.km, method=\"kernel\")\n\npar(mfrow=c(1,2))\nplot(funckde_adaptive, main=\"Functional Waterpoint KDE using Adaptive Bandwidth\")\nplot(nonfunckde_adaptive, main=\"Non-Functional Waterpoint KDE using Adaptive Bandwidth\")\n\n\n\n\n\n\n2.3.2 Convert KDE into grid object\nConverting KDE into a gridded object that is suitable for mapping purposes.\n\ngridded_kde_funckde <- as.SpatialGridDataFrame.im(funckde_adaptive)\n\ngridded_kde_nonfunckde <- as.SpatialGridDataFrame.im(nonfunckde_adaptive)\n\n\n\n2.3.3 Convert gridded output into raster\nNext we convert the gridded kernal density objects into RasterLayer objet using raster() of raster package.\n\nfunckde_raster<-raster(gridded_kde_funckde)\nfunckde_raster\n\nclass      : RasterLayer \ndimensions : 128, 128, 16384  (nrow, ncol, ncell)\nresolution : 0.8948485, 0.9616045  (x, y)\nextent     : 176.5032, 291.0438, 331.4347, 454.5201  (xmin, xmax, ymin, ymax)\ncrs        : NA \nsource     : memory\nnames      : v \nvalues     : 1.320603e-16, 23.9989  (min, max)\n\nnonfunckde_raster<-raster(gridded_kde_nonfunckde)\nnonfunckde_raster\n\nclass      : RasterLayer \ndimensions : 128, 128, 16384  (nrow, ncol, ncell)\nresolution : 0.8948485, 0.9616045  (x, y)\nextent     : 176.5032, 291.0438, 331.4347, 454.5201  (xmin, xmax, ymin, ymax)\ncrs        : NA \nsource     : memory\nnames      : v \nvalues     : 6.85125e-17, 20.92404  (min, max)\n\n\nNotice that the crs is NA\n\n\n2.3.4 Assigning Projection Systems\nThe code chunk below is used to include the CRS information funckde_raster and nonfunckde_raster.\n\nprojection(funckde_raster) <- CRS(\"+init=EPSG:26392 +units=km\")\nfunckde_raster\n\nclass      : RasterLayer \ndimensions : 128, 128, 16384  (nrow, ncol, ncell)\nresolution : 0.8948485, 0.9616045  (x, y)\nextent     : 176.5032, 291.0438, 331.4347, 454.5201  (xmin, xmax, ymin, ymax)\ncrs        : +proj=tmerc +lat_0=4 +lon_0=8.5 +k=0.99975 +x_0=670553.98 +y_0=0 +a=6378249.145 +rf=293.465 +units=km +no_defs \nsource     : memory\nnames      : v \nvalues     : 1.320603e-16, 23.9989  (min, max)\n\nres(funckde_raster)\n\n[1] 0.8948485 0.9616045\n\nprojection(nonfunckde_raster) <- CRS(\"+init=EPSG:26392 +units=km\")\nnonfunckde_raster\n\nclass      : RasterLayer \ndimensions : 128, 128, 16384  (nrow, ncol, ncell)\nresolution : 0.8948485, 0.9616045  (x, y)\nextent     : 176.5032, 291.0438, 331.4347, 454.5201  (xmin, xmax, ymin, ymax)\ncrs        : +proj=tmerc +lat_0=4 +lon_0=8.5 +k=0.99975 +x_0=670553.98 +y_0=0 +a=6378249.145 +rf=293.465 +units=km +no_defs \nsource     : memory\nnames      : v \nvalues     : 6.85125e-17, 20.92404  (min, max)\n\nres(nonfunckde_raster)\n\n[1] 0.8948485 0.9616045\n\n\n\n\n2.3.5 Viewing object in tmap\nFinally, we will display the raster in cartographic quality map using tmap package.\n\ntmap_mode(\"view\")\ntm_basemap(server =\"OpenStreetMap\")+ \ntm_shape(funckde_raster) + \n  tm_raster(\"v\") +\n  tm_layout(main.title = \"Raster Plot of Functional Waterpoint KDE\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE)\n\n\n\n\ntmap_mode(\"view\")\ntm_basemap(server =\"OpenStreetMap\")+ \ntm_shape(nonfunckde_raster) + \n  tm_raster(\"v\") +\n  tm_layout(main.title = \"Raster Plot of Non-Functional Waterpoint KDE\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE)\n\n\n\n\n\nChange tmap mode to plot.\n\ntmap_mode(\"plot\")"
  },
  {
    "objectID": "Geospatial/Take-home_Ex01/Take-home_Ex01.html#data-wrangling",
    "href": "Geospatial/Take-home_Ex01/Take-home_Ex01.html#data-wrangling",
    "title": "Take-home Exercise 1",
    "section": "4.1 Data Wrangling",
    "text": "4.1 Data Wrangling\nWe first start with a plot of the distribution of the waterpoints using wp_sf_nga and bd objects defined earlier.\n\ntmap_mode(\"view\")\ntm_shape(bd)+\n  tm_polygons()+\n  tm_shape(wp_sf_nga)+\n  tm_dots(col=\"status_clean\")\n\n\n\n\n\n#streamlining data into functional and non-functional waterpoints only\n\nWe realise that under the column “status_clean”, there are too many categories, which can be difficult for interpretation especially when we want to calculate colocation quotients. We will conduct data wrangling in the next step to define the waterpoints distinctly into “functional” and “non-functional”.\n\nnonfunc_df<-wp_sf_nga |> \nfilter(status_clean %in% \n           c(\"Abandoned/Decommissioned\",\n             \"Abandoned\",\n             \"Non-Functional due to dry season\",\n             \"Non-Functional\",\n             \"Non functional due to dry season\")) |> \n  mutate(status_redefined=\"Non-functional\")\n\nfunc_df<-wp_sf_nga |> \nfilter(status_clean %in%\n           c(\"Functional\",\n             \"Functional but not in use\",\n             \"Functional but needs repair\")) |> \n  mutate(status_redefined=\"Functional\")\n\n#creating a status_redefined column that states if water point is either functional or non functional\n\ndf<-bind_rows(func_df, nonfunc_df) |> \n  mutate(status_redefined=factor(status_redefined)) |> \n  select(Geometry, status_redefined)\n#combining data frames into 1 df\n\nWith our new dataframe, we continue to plot a graph showing the the functional and non-functional water points using tmap.\n\ntmap_mode(\"view\")\ntm_shape(bd)+\n  tm_polygons()+\n  tm_shape(df)+\n  tm_dots(col=\"status_redefined\",\n          size=0.01,\n          border.col=\"black\",\n           border.lwd=0.5)\n\n\n\n\n\n#plotting the combined dataframe"
  },
  {
    "objectID": "Geospatial/Take-home_Ex01/Take-home_Ex01.html#local-colocation-coefficient",
    "href": "Geospatial/Take-home_Ex01/Take-home_Ex01.html#local-colocation-coefficient",
    "title": "Take-home Exercise 1",
    "section": "4.2 Local Colocation coefficient",
    "text": "4.2 Local Colocation coefficient\nAccording to this website, the colocation analysis tool measures local patterns of spatial association between two categories of point features using the colocation quotient statistic.\nEach feature in the Category of Interest (category A) is evaluated individually for colocation with the presence of the Neighboring Category (category B) found within its neighborhood. In general, if the proportion of B points within the neighborhood of A is more than the global proportion of B, the colocation quotient will be high. If the neighborhood of A contains many other A points or many other categories other than B, the colocation between the Category of Interest (category A) and the Neighboring Category (category B) will be small.\nIn our analysis, our category of interest (A) is functional waterpoints, and neighboring category (B) is non-functional waterpoints.\n\n4.2.1 Preparing the vector list\n\nFW<-df |> \n  filter(status_redefined == \"Functional\")\nA<- FW$status_redefined\n\nNFW<-df |> \n  filter(status_redefined == \"Non-functional\")\nB<- NFW$status_redefined\n\n\n\n4.2.2 Preparing nearest neighbour list\nIn the code chunk below, st_knn() of sfdep package is used to determine the k (i.e. 6) nearest neighbours for given point geometry.\n\nnb<-include_self(\n  st_knn(st_geometry(df), 6)\n)\n\n\n\n4.2.3 Computing Kernel Weights\nIn the code chunk below, st_kernel_weights() of sfdep package is used to derive a weights list by using a kernel function.\n\nwt<-st_kernel_weights(nb,\n                      df,\n                      \"gaussian\",\n                      adaptive=TRUE)\n\n\n\n4.2.4 Computing LCLQ\nIn the code chunk below local_colocation() us used to compute the LCLQ values for each Water point event.\n\nLCLQ<-local_colocation(A, B, nb, wt, 39)\n\n\n\n4.2.5 Joining output Table\nBefore we can plot the LCLQ values their p-values, we need to join the output of local_colocation() to the stores sf data.frame. However, a quick check of LCLQ data-frame, we can’t find any field can be used as the join field. As a result, cbind() of Base R is useed.\n\nLCLQ_WP<-cbind(df,LCLQ)\n\n\n\n4.2.6 Plotting LCLQ values\nIn the code chunk below, tmap functions are used to plot the LCLQ analysis.\n\n#plot the graph\ntmap_mode(\"view\")\ntm_shape(bd) +\n  tm_polygons() + \ntm_shape(LCLQ_WP) +\n  tm_dots(col=\"Non.functional\")+\n  tm_view(set.zoom.limits = c(9,13))+\ntm_shape(LCLQ_WP) +\n  tm_dots(col=\"p_sim_Non.functional\")+\n  tm_view(set.zoom.limits = c(9,13))\n\n\n\n\n\n\n\n\n4.2.7 Statistical conclusion\nFrom the statistical table, we see that the colocation coefficient is less than 1 but extremely close to one for some points.\nFrom this website, features that have colocation quotients less than one are less likely to have category B within their neighborhood. If a feature has a colocation quotient equal to one, it means the proportion of categories within their neighborhood is a good representation of the proportion of categories throughout the entire study area.\nTherefore, it is likely that there is some correlation between location of functional and non functional water points. Additionally given that the p-value is less than 0.05 for the selected points, we can say that the result is statistically significant, and that Functional and non-functional water points are dependent with each other.\n\ntmap_mode(\"plot\")"
  },
  {
    "objectID": "Geospatial/Take-home_Ex01/Take-home_Ex01.html#performing-appropriate-tests-using-second-order-spatial-point-pattern-analysis-technique",
    "href": "Geospatial/Take-home_Ex01/Take-home_Ex01.html#performing-appropriate-tests-using-second-order-spatial-point-pattern-analysis-technique",
    "title": "Take-home Exercise 1",
    "section": "4.3 Performing appropriate tests using second order spatial point pattern analysis technique",
    "text": "4.3 Performing appropriate tests using second order spatial point pattern analysis technique\nWe will use the Cross-K Function to look into this relationship\nThe test hypotheses are:\n\nHo = The distribution of functional and non-functional waterpoints are spatially independent from each other (ie randomly distributed).\nH 1= The ditribution of functional and non-functional waterpoints are not independent from each other (ie not randomly distributed).\n\nWe will set a 95% confidence interval for the purpose of this study.\n\n4.3.1 Conversion of LCLQ data into ppp\nIn this analysis, we seek to perform marked point pattern analysis, based on the associated categorical measurement “status_redefined” in the waterpoint data.\n\ndf_spatialpoint<-df |>\n  as(\"Spatial\") |> \n  as(\"SpatialPointsDataFrame\") #creating spatial point data frame from sf\n\ndf_spatialpoint@data$status_redefined<-as.factor(df_spatialpoint@data$status_redefined) #creating a factor column for status_redefined\n\ndf_ppp<-df_spatialpoint |> \n  as(\"ppp\") #converting the spatial data frame into a ppp object\n\ndf_ppp_owin<-df_ppp[NGA_owin] #creating an owin object\n\nplot(df_ppp_owin, main = \"df_ppp\", which.marks = \"status_redefined\") #creating a quick plot to visualise the ppp object\n\n\n\n\n\n\n4.3.2 Using Cross K function to check for distribution trend\nWe use the cross-K function to analyse the trend of distribution of both functional and non-functional waterpoints.\n\nLcross.csr <- envelope(df_ppp_owin, \n                                 Lcross, \n                                 i=\"Functional\", \n                                 j=\"Non-functional\", \n                                 correction=\"border\", \n                                 nsim=39)\n\nGenerating 39 simulations of CSR  ...\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38,  39.\n\nDone.\n\n\nWe can then plot our result\n\nplot(Lcross.csr, \n     xlim = c(0,10000))\n\n\n\n\n\n\n4.3.3 Statistical conclusions\nFrom the graph above, we can conclude that there is evidence of spatial dependence between 0 to 5000, and 7000 to 8000 r values. More specifically, between 0 to 5000, functional and non-functional waterpoints tend to cluster, while between 7000 to 8000, they tend to be evenly distributed."
  },
  {
    "objectID": "Geospatial/Take-home_Ex01/Take-home_Ex01.html#importing-data",
    "href": "Geospatial/Take-home_Ex01/Take-home_Ex01.html#importing-data",
    "title": "Take-home Exercise 1",
    "section": "5.1 Importing data",
    "text": "5.1 Importing data\nIn researching for population data for Osun state, we chose to use population density from this website.\nWe save the webpage as a html file, and open it using microsoft excel, to generate the table in xls format. Subsequently, we save the excel file into csv format for analysis in R. The file will be saved with the name “pop_data_nga.csv”.\n\npop_data<-read.csv(\"data/pop_data_nga.csv\")\n\nosun_pop <- pop_data %>% \n  rename(shapeName = `Local.gov..area.`, \n         HASC = `HASC....`,\n         Capital = `Capital.....`, \n         Population = `Population....`,\n         State = `State....`) |> \n  filter(State == \"Osun\")\n\nTo plot the population data, we use ADM2 data which defines the specific geoboundaries of LGAs in states.\n\n#Joining data from both data frames, preserving sf  properties\ngeoNGA2_osun<-bd |> \n  left_join(osun_pop, by=c(\"ADM2_EN\"=\"shapeName\"))"
  },
  {
    "objectID": "Geospatial/Take-home_Ex01/Take-home_Ex01.html#plotting-waterpoint-data-and-population-data",
    "href": "Geospatial/Take-home_Ex01/Take-home_Ex01.html#plotting-waterpoint-data-and-population-data",
    "title": "Take-home Exercise 1",
    "section": "5.2 Plotting waterpoint data and population data",
    "text": "5.2 Plotting waterpoint data and population data\nWith our data ready, we can plot the projected population using tmap functions.\n\ntm_shape(geoNGA2_osun)+\n  tm_fill(\"Density\",\n          style = \"quantile\", \n          palette = \"-Blues\",\n          title = \"Population Density of Osun LGAs\")+\n  tm_layout(main.title = \"Population Density of Osun LGAs\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_scale_bar() +\n  tm_grid(alpha =0.2)+\n  tm_shape(filter(df,status_redefined==\"Functional\"))+\n  tm_dots(col=\"green\",\n          size=0.01,\n          border.col=\"black\",\n           border.lwd=0.5,\n          alpha=0.5)+\n  tm_shape(filter(df,status_redefined==\"Non-functional\"))+\n  tm_dots(col=\"red\",\n          size=0.01,\n          border.col=\"black\",\n           border.lwd=0.5,\n          alpha=0.5)\n\n\n\n\nChange tmap_mode to plot\n\ntmap_mode(\"plot\")"
  },
  {
    "objectID": "Geospatial/Take-home_Ex01/Take-home_Ex01.html#conclusions",
    "href": "Geospatial/Take-home_Ex01/Take-home_Ex01.html#conclusions",
    "title": "Take-home Exercise 1",
    "section": "5.3 Conclusions",
    "text": "5.3 Conclusions\nFrom the plot, we see that high population density areas such as Ife Central, Ede North, Boripe, Ilesha West and Orolu have high coincidence of non-functional waterpoints. This could possibly imply the overuse of waterpoints by the larger population. More importantly, this finding can be used by government agencies who are pioritising repair schedules for the non-functional waterpoints."
  },
  {
    "objectID": "Geospatial/Take-home_Ex03/Take-home_Ex03.html",
    "href": "Geospatial/Take-home_Ex03/Take-home_Ex03.html",
    "title": "Take-home Exercise 3: Predicting HDB Public Housing Resale Pricies using Geographically Weighted Methods",
    "section": "",
    "text": "Pandemic-induced supply chain constraints coinciding with the booming demand for flats have driven HDB resale prices up by 10.3% in 2022. For young couples deciding to buy a home to stay in, it seems nearly impossible to do so especially with the exorbitantly high price tags attached to flats recently.\nThis has caused many to think about what are the factors that affect the prices of HDB resale flats– some of which may be geographical in nature. For instance, some may pioritise of living near the local MRT, while others may focus on the location of schools within the vicinity of a flat.\nIn this take-home exercise, we will build a predictive model for the month of January and February 2023 in Singapore. The predictive models must be built by using by using conventional OLS method and GWR methods. Lastly, we will compare the performance of the conventional OLS method versus the geographical weighted methods.\nThe assignment submission is split into the following sections\n\nInstalling packages\nData Collection\nExploratory Data Analysis\nData Wrangling\nRegressions\nPredictions\nReferences"
  },
  {
    "objectID": "Geospatial/Take-home_Ex03/Take-home_Ex03.html#installing-packages",
    "href": "Geospatial/Take-home_Ex03/Take-home_Ex03.html#installing-packages",
    "title": "Take-home Exercise 3: Predicting HDB Public Housing Resale Pricies using Geographically Weighted Methods",
    "section": "2 Installing packages",
    "text": "2 Installing packages\n\n\nCode\npacman::p_load(olsrr, corrplot, ggpubr, sf, spdep, GWmodel, tmap, tidyverse, gtsummary, sfdep, onemapsgapi, stringr, SpatialML, devtools, tidymodels, kableExtra)\n\n\nThe uses of each package installed can be found below:\n\nData CollectionData WranglingData VisualisationRegressions\n\n\n\nkableExtra: an extension of kable, used for table customisation\nonemapsgapi: used to query Singapore-specifif spatial data, alongside additional functionalities\nhttr: used to make API calls, such as GET requests\n\n\n\n\nsf: used for importing, managing and processing geospatial data\nspdep: used to create spatial weight matrix objects, global and local spatial autocorrelation statistics and related calculations\nsfdep: builds on spdep to create sn sf and tidyverse friendly interface to the package\ntidyverse: collection of R packages designed for data wrangling\nstringr: provides a cohesive set of functions designed to make working with strings easily\n\n\n\n\ntmap: used for creating thematic maps, such as chloropleth and bubble maps\n\n\n\n\nolsrr: used for building least square regression models\ncorrplot + ggpubr: Used for multivariate data visualisation and analysis\nGWmodel: provides a collection of localised spatial statistical methods such as summary statistic, principal components analysis, discriminant analysis and various forms of GW regression\ngtsummary: provides an elegant and flexible way to create publication-ready analytical and summary tables using the R programming language\ndevtools: used for installing any R packages which is not available in RCRAN\nSpatialML: allows for a geographically weighted random forest regression to include a function to find the optimal bandwidth\n\n\n\n\n\n2.1 OneMapAPI: usage\nIn this exercise, the collection of the coordinate data of singapore-specific places can tap on the database implemented by OneMapSg. 🏠\nReferencing one of our senior Megan’s work, the steps to intialise this processes is as follows:\n\nMake sure you have your token value after signing up for a OneMapSG account\nSearch for the specified theme with search_themes(token, “searchval”).\nCheck the theme status with get_theme_status(token, “themename”)\nFrom here, we can convert our tibble dataframe to simple features dataframe. All the themes for this project use Lat and Lng as the latitude and longitude respectively, and our project coordinates system should be in the WGS84 system, aka ESPG code 4326. Thus, themesf <- st_as_sf(themetibble, coords=c(\"Lng\", \"Lat\"), crs=4326)\n\n\n\nCode\nlibrary(sf)\nlibrary(onemapsgapi)\n\ntoken <- \"your value\"\nsearch_themes(token, \"searchval\")\nget_theme_status(token, \"themename\")\nthemetibble <- get_theme(token, \"themename\")\nthemesf <- st_as_sf(themetibble, coords=c(\"Lng\", \"Lat\"), crs=4326)\n\n\n\n\nThis token will expire on 2023-08-21 15:20:52\n\n\n# A tibble: 477 × 7\n   THEMENAME                       QUERY…¹ ICON  CATEG…² THEME…³ EXPIR…⁴ PUBLI…⁵\n   <chr>                           <chr>   <chr> <chr>   <chr>   <named> <named>\n 1 MCE KPE Speed Camera            mce_kp… icon… Commun… LAND T… <chr>   <chr>  \n 2 MCE KPE Speed Camera            mce_kp… icon… Commun… LAND T… <int>   <int>  \n 3 MCE KPE Speed Camera            mce_kp… icon… Commun… LAND T… <chr>   <chr>  \n 4 Singapore Police Force Mobile … spf_msc Spee… Commun… SINGAP… <chr>   <chr>  \n 5 Singapore Police Force Mobile … spf_msc Spee… Commun… SINGAP… <int>   <int>  \n 6 Singapore Police Force Mobile … spf_msc Spee… Commun… SINGAP… <chr>   <chr>  \n 7 Liquor Control Zone(s) proclai… liquor… Cres… Commun… SINGAP… <chr>   <chr>  \n 8 Liquor Control Zone(s) proclai… liquor… Cres… Commun… SINGAP… <int>   <int>  \n 9 Liquor Control Zone(s) proclai… liquor… Cres… Commun… SINGAP… <chr>   <chr>  \n10 Singapore Police Force - 32nd … as_ist… Cres… Commun… SINGAP… <chr>   <chr>  \n# … with 467 more rows, and abbreviated variable names ¹​QUERYNAME, ²​CATEGORY,\n#   ³​THEME_OWNER, ⁴​EXPIRY_DATE, ⁵​PUBLISHED_DATE\n\n\nWith that in mind, lets move on to data collection! 🙃"
  },
  {
    "objectID": "Geospatial/Take-home_Ex03/Take-home_Ex03.html#data-collection-1",
    "href": "Geospatial/Take-home_Ex03/Take-home_Ex03.html#data-collection-1",
    "title": "Take-home Exercise 3: Predicting HDB Public Housing Resale Pricies using Geographically Weighted Methods",
    "section": "3 Data Collection",
    "text": "3 Data Collection\nThe following data would be used for the formation of our regression\n\n\nCode\n# initialise a dataframe of our aspatial and geospatial dataset details\ndatasets <- data.frame(\n  Type=c(\"Aspatial\",\n         \"Geospatial\",\n         \n         \"Geospatial - Extracted\",\n         \"Geospatial - Extracted\",\n         \"Geospatial - Extracted\",\n         \"Geospatial - Extracted\",\n         \"Geospatial - Extracted\",\n         \"Geospatial - Extracted\",\n         \"Geospatial - Extracted\",\n         \n         \"Geospatial - Selfsourced\",\n         \"Geospatial - Selfsourced\",\n         \"Geospatial - Selfsourced\",\n         \"Geospatial - Selfsourced\",\n         \"Geospatial - Selfsourced\"),\n  \n  Name=c(\"Resale Flat Prices\",\n         \"Master Plan 2019 Subzone Boundary (Web)\",\n         \n         \"Childcare Services\",\n         \"Eldercare Services\",\n         \"Hawker Centres\",\n         \"Kindergartens\",\n         \"Parks\",\n         \"Libraries\",\n         \"Sport Facilities\",\n         \n         \"Bus Stop Locations Aug 2021\",\n         \"MRT & LRT Locations Aug 2021\",\n         \"Supermarkets\",\n         \"Shopping Mall SVY21 Coordinates\", \n         \"Primary School\"),\n  \n  Format=c(\".csv\", \n           \".shp\",\n           \n           \".shp\", \n           \".shp\", \n           \".shp\", \n           \".shp\",\n           \".shp\", \n           \".shp\",\n           \".shp\",\n           \n           \".shp\",\n           \".kml\",\n           \".shp\",\n           \".shp\",\n           \".csv\"),\n  \n  Source=c(\"[data.gov.sg](https://data.gov.sg/dataset/resale-flat-prices)\",\n           \"[data.gov.sg](https://data.gov.sg/dataset/master-plan-2014-subzone-boundary-web)\",\n           \n           \"[OneMap API](https://www.onemap.gov.sg/docs/)\",\n           \"[OneMap API](https://www.onemap.gov.sg/docs/)\",\n           \"[OneMap API](https://www.onemap.gov.sg/docs/)\",\n           \"[OneMap API](https://www.onemap.gov.sg/docs/)\",\n           \"[OneMap API](https://www.onemap.gov.sg/docs/)\",\n           \"[OneMap API](https://www.onemap.gov.sg/docs/)\",\n           \"[OneMap API](https://www.onemap.gov.sg/docs/)\",\n           \n           \"[datamall.lta](https://datamall.lta.gov.sg/content/datamall/en/search_datasets.html?searchText=bus%20stop)\",\n           \"[data.gov](https://data.gov.sg/dataset/lta-mrt-station-exit)\",\n           \"[Onemap.gov](https://www.onemap.gov.sg/main/v2/essentialamenities)\",\n           \"[Valery Lim's Github](https://github.com/ValaryLim/Mall-Coordinates-Web-Scraper/blob/master/mall_coordinates_updated.csv)\",\n           \"[data.gov](https://data.gov.sg/dataset/school-directory-and-information)\")\n  )\n\nlibrary(knitr)\nlibrary(kableExtra)\nkable(datasets, caption=\"Datasets Used\") %>%\n  kable_material(\"hover\", latex_options=\"scale_down\")\n\n\n\n\nDatasets Used\n \n  \n    Type \n    Name \n    Format \n    Source \n  \n \n\n  \n    Aspatial \n    Resale Flat Prices \n    .csv \n    [data.gov.sg](https://data.gov.sg/dataset/resale-flat-prices) \n  \n  \n    Geospatial \n    Master Plan 2019 Subzone Boundary (Web) \n    .shp \n    [data.gov.sg](https://data.gov.sg/dataset/master-plan-2014-subzone-boundary-web) \n  \n  \n    Geospatial - Extracted \n    Childcare Services \n    .shp \n    [OneMap API](https://www.onemap.gov.sg/docs/) \n  \n  \n    Geospatial - Extracted \n    Eldercare Services \n    .shp \n    [OneMap API](https://www.onemap.gov.sg/docs/) \n  \n  \n    Geospatial - Extracted \n    Hawker Centres \n    .shp \n    [OneMap API](https://www.onemap.gov.sg/docs/) \n  \n  \n    Geospatial - Extracted \n    Kindergartens \n    .shp \n    [OneMap API](https://www.onemap.gov.sg/docs/) \n  \n  \n    Geospatial - Extracted \n    Parks \n    .shp \n    [OneMap API](https://www.onemap.gov.sg/docs/) \n  \n  \n    Geospatial - Extracted \n    Libraries \n    .shp \n    [OneMap API](https://www.onemap.gov.sg/docs/) \n  \n  \n    Geospatial - Extracted \n    Sport Facilities \n    .shp \n    [OneMap API](https://www.onemap.gov.sg/docs/) \n  \n  \n    Geospatial - Selfsourced \n    Bus Stop Locations Aug 2021 \n    .shp \n    [datamall.lta](https://datamall.lta.gov.sg/content/datamall/en/search_datasets.html?searchText=bus%20stop) \n  \n  \n    Geospatial - Selfsourced \n    MRT & LRT Locations Aug 2021 \n    .kml \n    [data.gov](https://data.gov.sg/dataset/lta-mrt-station-exit) \n  \n  \n    Geospatial - Selfsourced \n    Supermarkets \n    .shp \n    [Onemap.gov](https://www.onemap.gov.sg/main/v2/essentialamenities) \n  \n  \n    Geospatial - Selfsourced \n    Shopping Mall SVY21 Coordinates \n    .shp \n    [Valery Lim's Github](https://github.com/ValaryLim/Mall-Coordinates-Web-Scraper/blob/master/mall_coordinates_updated.csv) \n  \n  \n    Geospatial - Selfsourced \n    Primary School \n    .csv \n    [data.gov](https://data.gov.sg/dataset/school-directory-and-information) \n  \n\n\n\n\n\n\n3.1 Aspatial data\nFor the purpose of this take-home exercise, HDB Resale Flat Prices provided by Data.gov.sg should be used as the core data set.\nThe code chunk below reads the csv file into our R environment.\n\n\nCode\nresale<-read_csv(\"data/aspatial/resale-flat-prices-based-on-registration-date-from-jan-2017-onwards.csv\")\nhead(resale,5)\n\n\n\n3.1.1 Filtering to include transactional periods\nThe study should focus on either three-room, four-room or five-room flat and transaction period should be from 1st January 2021 to 31st December 2022. The test data should be January and February 2023 resale prices.\nTherefore, we filter the dataset to include transactions occuring from Jan 2021 to Feb 2023.\n\n\nCode\n# transaction period from 01-Jan-21 to 31-Feb-23\n# 4-room flats \nresale <- resale %>% \n  filter(flat_type == \"4 ROOM\") %>%\n  filter(month >= \"2021-01\" & month <= \"2022-12\" | month >= \"2023-01\" & month <= \"2023-02\")\n\n\n\n\nCode\nresale$street_name <- gsub(\"ST\\\\.\", \"SAINT\", resale$street_name)\n\n\n\n\n3.1.2 Creating function to get longitude and lattitude\nNotice that the data provided in the resale flat data does not include coordinates! We need to create our own geocoding function. With reference to our senior Megan’s work once again, these are the steps required in creating a geocoding function.\n\nCombine the block and street name into an address\nPass the address as the searchVal in our query\nSend the query to OneMapSG search Note: Since we don’t need all the address details, we can set getAddrDetails as ‘N’\nConvert response (JSON object) to text\nSave response in text form as a dataframe\nWe only need to retain the latitude and longitude for our output\n\n\n\nCode\nlibrary(httr)\nlibrary(rjson)\ngeocode <- function(block, streetname) {\n  base_url <- \"https://developers.onemap.sg/commonapi/search\"\n  address <- paste(block, streetname, sep = \" \")\n  query <- list(\"searchVal\" = address, \n                \"returnGeom\" = \"Y\",\n                \"getAddrDetails\" = \"N\",\n                \"pageNum\" = \"1\")\n  \n  res <- GET(base_url, query = query)\n  restext<-content(res, as=\"text\")\n  \n  output <- fromJSON(restext)  %>% \n    as.data.frame %>%\n    select(results.LATITUDE, results.LONGITUDE)\n\n  return(output)\n}\n\n\n\n\n3.1.3 Passing code through resale df\nWith our function in place, we then create a loop to implement the geocoding function through our transaction data. ✌🏼\n\n\nCode\nresale$LATITUDE <- 0\nresale$LONGITUDE <- 0\n\nfor (i in 1:nrow(resale)){\n  temp_output <- geocode(resale[i, 4], resale[i, 5])\n  \n  resale$LATITUDE[i] <- temp_output$results.LATITUDE\n  resale$LONGITUDE[i] <- temp_output$results.LONGITUDE\n}\n\n\n\n\n3.1.4 Save as an RDS object\nEspecially given the extensive dataframe, this would take quite long to load! to avoid having to reload the output again, we save this as a RDS object 😵‍💫.\n\n\nCode\nsaveRDS(resale, file=\"resale\", compress=FALSE)\n\n\nnow we can read the resale file easily! 🤭\n\n\nCode\nresale<-readRDS(\"resale\")\n\n\n\n\n\n3.2 Geospatial Data\nOur geospatial data can be split into several categories\n\n\n\n\n\n\n\n\n\n\nBase\nExtracted\nSourced\n\n\n\n\nDescription\ngeographical boundaries\nIndependent variable data extracted using OneMap API\nIndependent variable data extracted from the internet\n\n\nLayer names\n\nMPSZ-2019\n\nchildcare\nelder care\nhawker centre\nkindergartens\nparks\nlibraries\nsport facilities\n\nbus stop\nshopping mall\nsupermarket\nmrt\nprimary school\ngood primary school\n\n\n\n\nFeel free to explore the codes we used to extract the data in the different panels below! 🤗\n\nExtractedSourced\n\n\n\n\nCode\nlibrary(sf)\nlibrary(onemapsgapi)\n\n# extracting eldercare data as an sf object into R\neldercare<-get_theme(token,\"eldercare\")\neldercare.sf <- st_as_sf(eldercare, coords=c(\"Lng\", \"Lat\"), crs=4326)\n\n# creating a saved sf object in data file for easy reference\nst_write(obj = eldercare.sf,\n         dsn = \"data/geospatial/extracted\",\n         layer = \"eldercare\",\n         driver = \"ESRI Shapefile\")\n\n\n\n\nCode\n# extracting eldercare data as an sf object into R\nhawkercentre<-get_theme(token,\"hawkercentre\")\nhawkercentre.sf <- st_as_sf(hawkercentre, coords=c(\"Lng\", \"Lat\"), crs=4326)\n\n# creating a saved sf object in data file for easy reference\nst_write(obj = hawkercentre.sf,\n         dsn = \"data/geospatial/extracted\",\n         layer = \"hawkercentre\",\n         driver = \"ESRI Shapefile\")\n\n\n\n\nCode\n# extracting childcare data as an sf object into R\nchildcare<-get_theme(token,\"childcare\")\nchildcare.sf <- st_as_sf(childcare, coords=c(\"Lng\", \"Lat\"), crs=4326)\n\n# creating a saved sf object in data file for easy reference\nst_write(obj = childcare.sf,\n         dsn = \"data/geospatial/extracted\",\n         layer = \"childcare\",\n         driver = \"ESRI Shapefile\")\n\n\n\n\nCode\n# extracting kindergartens data as an sf object into R\nkindergartens<-get_theme(token,\"kindergartens\")\nkindergartens.sf <- st_as_sf(kindergartens, coords=c(\"Lng\", \"Lat\"), crs=4326)\n\n# creating a saved sf object in data file for easy reference\nst_write(obj = kindergartens.sf,\n         dsn = \"data/geospatial/extracted\",\n         layer = \"kindergartens\",\n         driver = \"ESRI Shapefile\")\n\n\n\n\nCode\n# extracting parks data as an sf object into R\nparks<-get_theme(token,\"nationalparks\")\nparks.sf <- st_as_sf(parks, coords=c(\"Lng\", \"Lat\"), crs=4326)\n\n# creating a saved sf object in data file for easy reference\nst_write(obj = parks.sf,\n         dsn = \"data/geospatial/extracted\",\n         layer = \"parks\",\n         driver = \"ESRI Shapefile\")\n\n\n\n\nCode\n# extracting library data as an sf object into R\nlibrary<-get_theme(token,\"libraries\")\nlibrary.sf <- st_as_sf(library, coords=c(\"Lng\", \"Lat\"), crs=4326)\n\n# creating a saved sf object in data file for easy reference\nst_write(obj = library.sf,\n         dsn = \"data/geospatial/extracted\",\n         layer = \"libraries\",\n         driver = \"ESRI Shapefile\")\n\n\nExtracting and sports facility data:\n\n\nCode\nsport<-get_theme(token,\"sportsg_sport_facilities\")\n\nsport\n\n\n\nWhen we browse the sports data, we see that the lat and lng columns are not configured in the way we expect it to be. In the Lng column, the longitude of the location is first placed followed by a “|” and then the latitude of the location. 🥲\nNotice that the Lng and Lat are not in the format we want. We create a function to extract the Lng numbers from the string, using str_extract from the package stringr.\nnote the use of the following syntax\n\n“\\\\d+” extracts one or more digits that occur before the | character\n“\\.?” matches the decimal point occuring in lng records\n“\\d*” matches any digits that come after the decimal point\n\n\n\nCode\nlng.mutate<-function(df){\n  df |> \n    mutate(Lng=str_extract(Lng, \"\\\\d+\\\\.?\\\\d*\"))\n}\n\n\n\n\nCode\n# extracting sports data as an sf object into R\nsport<-get_theme(token,\"sportsg_sport_facilities\")\nsport<-lng.mutate(sport)\n\nsport.sf <- st_as_sf(sport, coords=c(\"Lng\", \"Lat\"), crs=4326)\n\n# creating a saved sf object in data file for easy reference\nst_write(obj = sport.sf,\n         dsn = \"data/geospatial/extracted\",\n         layer = \"sportsg_sport_facilities\",\n         driver = \"ESRI Shapefile\")\n\n\n\n\n\nBus stop data Extracted from: datamall.lta\nSupermarkets extracted data from: Onemap.gov\nShopping mall data Extracted from: ValeryLim’s Github\n\nsince it is in a csv file, we convert it into a shape file and save it into our extracted data file in the code chunk below\n\n\nCode\nmall.df<- read.csv(\"data/geospatial/sourced/mall_coordinates_updated.csv\") |> \n  select(latitude, longitude, name) |> \n  dplyr::relocate(name)\n\nmall.sf <- st_as_sf(mall.df, coords=c(\"longitude\", \"latitude\"), crs=4326)\n\nst_write(obj = mall.sf,\n         dsn = \"data/geospatial/sourced\",\n         layer = \"shoppingmall\",\n         driver = \"ESRI Shapefile\")\n\n\n\nPrimary school data extracted from data.gov\n\nSince only postal code and address data is given, we use the OneMapAPI in the code below to obtain coordinates of the schools.\n\n\nCode\nprimarysch.df<- read.csv(\"data/geospatial/sourced/general-information-of-schools.csv\") |> \n  dplyr::filter(mainlevel_code==\"PRIMARY\") |> \n  dplyr::select(school_name, postal_code) |> \n  dplyr::mutate(postal_code=ifelse(nchar(as.character(postal_code)) == 5, \n                             paste0(\"0\", as.character(postal_code)), \n                             as.character(postal_code)))\n\n#use onemap api\nlibrary(httr)\nlibrary(rjson)\ngeocode.data.gov <- function(postalcode) {\n  base_url <- \"https://developers.onemap.sg/commonapi/search\"\n  query <- list(\"searchVal\" = postalcode, \n                \"returnGeom\" = \"Y\",\n                \"getAddrDetails\" = \"N\",\n                \"pageNum\" = \"1\")\n  \n  res <- GET(base_url, query = query)\n  restext<-content(res, as=\"text\")\n  \n  output <- fromJSON(restext) |>  \n    as.data.frame() |> \n    dplyr::select(results.LATITUDE, results.LONGITUDE)\n\n  return(output)\n}\n\n#create loop to run data through\nprimarysch.df$LATITUDE <- 0\nprimarysch.df$LONGITUDE <- 0\n\nfor (i in 1:nrow(primarysch.df)){\n  temp_output <- geocode.data.gov(primarysch.df[i, 2])\n  \n  primarysch.df$LATITUDE[i] <- temp_output$results.LATITUDE\n  primarysch.df$LONGITUDE[i] <- temp_output$results.LONGITUDE\n}\n\nprimarysch.sf <- st_as_sf(primarysch.df, coords=c(\"LONGITUDE\", \"LATITUDE\"), crs=4326)\n\nst_write(obj = primarysch.sf,\n         dsn = \"data/geospatial/sourced\",\n         layer = \"primarysch\",\n         driver = \"ESRI Shapefile\")\n\n\n\nGood primary school - filtered from primarysch.sf\n\nBased on the rankings of this website, we take the first 10 schools\n\n\nCode\nprimarysch.sf <- st_read(dsn = \"data/geospatial/sourced\", layer = \"primarysch\")\ngoodprimarysch.sf <- primarysch.sf |> \n  filter(school_name %in% c(\"NANYANG PRIMARY SCHOOL\",\n                            \"TAO NAN SCHOOL\",\n                            \"CANOSSA CATHOLIC PRIMARY SCHOOL\",\n                            \"NAN HUA PRIMARY SCHOOL\",\n                            \"ST. HILDA'S PRIMARY SCHOOL\",\n                            \"HENRY PARK PRIMARY SCHOOL\",\n                            \"ANGLO-CHINESE SCHOOL (PRIMARY)\",\n                            \"RAFFLES GIRLS' PRIMARY SCHOOL\",\n                            \"PEI HWA PRESBYTERIAN PRIMARY SCHOOL\"\n                            ))\n\nst_write(obj = goodprimarysch.sf,\n         dsn = \"data/geospatial/sourced\",\n         layer = \"goodprimarysch\",\n         driver = \"ESRI Shapefile\")\n\n\n\nMrt data: retrieved from data.gov\n\nThe data given to us is in “.kml” format, we convert it to an sf object.\n\n\nCode\nmrt.sf<-st_read(dsn= \"data/geospatial/sourced/lta-mrt-station-exit-kml.kml\") |> \n  st_zm()\n\nst_write(obj = mrt.sf,\n         dsn = \"data/geospatial/sourced\",\n         layer = \"mrt\",\n         driver = \"ESRI Shapefile\",\n         append = FALSE)\n\n\n\n\n\n\n\n3.3 Reading collected geospatial data into R\n\nBaseExtractedSourced\n\n\n\n\nCode\nmpsz.sf <- st_read(dsn = \"data/geospatial/base\", layer = \"MPSZ-2019\")\n\n\nReading layer `MPSZ-2019' from data source \n  `/Users/pengyouyun/youyunpeng/personalprojects/Geospatial/Take-home_Ex03/data/geospatial/base' \n  using driver `ESRI Shapefile'\nSimple feature collection with 332 features and 6 fields\nGeometry type: MULTIPOLYGON\nDimension:     XYZ\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nz_range:       zmin: 0 zmax: 0\nProjected CRS: SVY21 / Singapore TM\n\n\n\n\n\n\nCode\neldercare.sf <- st_read(dsn = \"data/geospatial/extracted\", layer = \"eldercare\")\n\n\nReading layer `eldercare' from data source \n  `/Users/pengyouyun/youyunpeng/personalprojects/Geospatial/Take-home_Ex03/data/geospatial/extracted' \n  using driver `ESRI Shapefile'\nSimple feature collection with 133 features and 4 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 103.7119 ymin: 1.271472 xmax: 103.9561 ymax: 1.439561\nGeodetic CRS:  WGS 84\n\n\nCode\nhawkercentre.sf <- st_read(dsn = \"data/geospatial/extracted\", layer = \"hawkercentre\")\n\n\nReading layer `hawkercentre' from data source \n  `/Users/pengyouyun/youyunpeng/personalprojects/Geospatial/Take-home_Ex03/data/geospatial/extracted' \n  using driver `ESRI Shapefile'\nSimple feature collection with 125 features and 18 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 103.6974 ymin: 1.272716 xmax: 103.9882 ymax: 1.449017\nGeodetic CRS:  WGS 84\n\n\nCode\nchildcare.sf<-  st_read(dsn = \"data/geospatial/extracted\", layer = \"childcare\")\n\n\nReading layer `childcare' from data source \n  `/Users/pengyouyun/youyunpeng/personalprojects/Geospatial/Take-home_Ex03/data/geospatial/extracted' \n  using driver `ESRI Shapefile'\nSimple feature collection with 1925 features and 5 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 103.6878 ymin: 1.247759 xmax: 103.9897 ymax: 1.462134\nGeodetic CRS:  WGS 84\n\n\nCode\nkindergartens.sf<-  st_read(dsn = \"data/geospatial/extracted\", layer = \"kindergartens\")\n\n\nReading layer `kindergartens' from data source \n  `/Users/pengyouyun/youyunpeng/personalprojects/Geospatial/Take-home_Ex03/data/geospatial/extracted' \n  using driver `ESRI Shapefile'\nSimple feature collection with 448 features and 5 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 103.6887 ymin: 1.247759 xmax: 103.9717 ymax: 1.455452\nGeodetic CRS:  WGS 84\n\n\nCode\nparks.sf<-  st_read(dsn = \"data/geospatial/extracted\", layer = \"parks\")\n\n\nReading layer `parks' from data source \n  `/Users/pengyouyun/youyunpeng/personalprojects/Geospatial/Take-home_Ex03/data/geospatial/extracted' \n  using driver `ESRI Shapefile'\nSimple feature collection with 421 features and 2 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 103.6929 ymin: 1.214491 xmax: 104.0538 ymax: 1.462094\nGeodetic CRS:  WGS 84\n\n\nCode\nlibrary.sf<-  st_read(dsn = \"data/geospatial/extracted\", layer = \"libraries\")\n\n\nReading layer `libraries' from data source \n  `/Users/pengyouyun/youyunpeng/personalprojects/Geospatial/Take-home_Ex03/data/geospatial/extracted' \n  using driver `ESRI Shapefile'\nSimple feature collection with 31 features and 13 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 103.7045 ymin: 1.263922 xmax: 103.9494 ymax: 1.448197\nGeodetic CRS:  WGS 84\n\n\nCode\nsport.sf<-  st_read(dsn = \"data/geospatial/extracted\", layer = \"sportsg_sport_facilities\")\n\n\nReading layer `sportsg_sport_facilities' from data source \n  `/Users/pengyouyun/youyunpeng/personalprojects/Geospatial/Take-home_Ex03/data/geospatial/extracted' \n  using driver `ESRI Shapefile'\nSimple feature collection with 35 features and 8 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 103.6937 ymin: 1.287668 xmax: 103.9524 ymax: 1.435755\nGeodetic CRS:  WGS 84\n\n\n\n\n\n\nCode\nbusstop.sf <- st_read(dsn = \"data/geospatial/sourced\", layer = \"BusStop\")\n\n\nReading layer `BusStop' from data source \n  `/Users/pengyouyun/youyunpeng/personalprojects/Geospatial/Take-home_Ex03/data/geospatial/sourced' \n  using driver `ESRI Shapefile'\nSimple feature collection with 5159 features and 3 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 3970.122 ymin: 26482.1 xmax: 48284.56 ymax: 52983.82\nProjected CRS: SVY21\n\n\nCode\nshoppingmall.sf<-st_read(dsn = \"data/geospatial/sourced\", layer = \"shoppingmall\")\n\n\nReading layer `shoppingmall' from data source \n  `/Users/pengyouyun/youyunpeng/personalprojects/Geospatial/Take-home_Ex03/data/geospatial/sourced' \n  using driver `ESRI Shapefile'\nSimple feature collection with 184 features and 1 field\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 103.6784 ymin: 1.263797 xmax: 103.9897 ymax: 1.448227\nGeodetic CRS:  WGS 84\n\n\nCode\nsupermarket.sf<-st_read(dsn = \"data/geospatial/sourced\", layer = \"SUPERMARKETS\")\n\n\nReading layer `SUPERMARKETS' from data source \n  `/Users/pengyouyun/youyunpeng/personalprojects/Geospatial/Take-home_Ex03/data/geospatial/sourced' \n  using driver `ESRI Shapefile'\nSimple feature collection with 526 features and 8 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 4901.188 ymin: 25529.08 xmax: 46948.22 ymax: 49233.6\nProjected CRS: SVY21\n\n\nCode\nmrt.sf<-st_read(dsn = \"data/geospatial/sourced\", layer = \"mrt\")\n\n\nReading layer `mrt' from data source \n  `/Users/pengyouyun/youyunpeng/personalprojects/Geospatial/Take-home_Ex03/data/geospatial/sourced' \n  using driver `ESRI Shapefile'\nSimple feature collection with 474 features and 2 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 103.6368 ymin: 1.264972 xmax: 103.9893 ymax: 1.449157\nGeodetic CRS:  WGS 84\n\n\nCode\nprimarysch.sf<-st_read(dsn = \"data/geospatial/sourced\", layer = \"primarysch\")\n\n\nReading layer `primarysch' from data source \n  `/Users/pengyouyun/youyunpeng/personalprojects/Geospatial/Take-home_Ex03/data/geospatial/sourced' \n  using driver `ESRI Shapefile'\nSimple feature collection with 183 features and 2 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 103.6878 ymin: 1.274958 xmax: 103.9628 ymax: 1.456608\nGeodetic CRS:  WGS 84\n\n\nCode\ngoodprimarysch.sf<-st_read(dsn = \"data/geospatial/sourced\", layer = \"goodprimarysch\")\n\n\nReading layer `goodprimarysch' from data source \n  `/Users/pengyouyun/youyunpeng/personalprojects/Geospatial/Take-home_Ex03/data/geospatial/sourced' \n  using driver `ESRI Shapefile'\nSimple feature collection with 9 features and 2 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 103.7611 ymin: 1.305285 xmax: 103.937 ymax: 1.34968\nGeodetic CRS:  WGS 84\n\n\n\n\n\n\n\n3.4 Data pre-processing\n\n3.4.1 Removing unnecessary columns\nFor locational factor dataframes, we only need to know the name of the facility and its geometry column.\n\n\nCode\neldercare.sf<-select(eldercare.sf, 1)\n\nhawkercentre.sf<-select(hawkercentre.sf, 1)\n\nchildcare.sf<-select(childcare.sf, 1)\n\nparks.sf<-select(parks.sf, 1)\n\nlibrary.sf<-select(library.sf, 1)\n\nsport.sf<-select(sport.sf, 1)\n\nshoppingmall.sf<-select(shoppingmall.sf, 1)\n\nbusstop.sf<-select(busstop.sf, 1)\n\nsupermarket.sf<- select(supermarket.sf, 1)\n\nprimarysch.sf<-select(primarysch.sf, 1)\n\ngoodprimarysch.sf<-select(goodprimarysch.sf, 1)\n\nmrt.sf<-select(mrt.sf, 1)\n\n\n\n\n3.4.2 Check for invalid geometries\n\n\nCode\nlength(which(st_is_valid(eldercare.sf) == FALSE))\n\n\n[1] 0\n\n\nCode\nlength(which(st_is_valid(hawkercentre.sf) == FALSE))\n\n\n[1] 0\n\n\nCode\nlength(which(st_is_valid(childcare.sf) == FALSE))\n\n\n[1] 0\n\n\nCode\nlength(which(st_is_valid(kindergartens.sf) == FALSE))\n\n\n[1] 0\n\n\nCode\nlength(which(st_is_valid(parks.sf) == FALSE))\n\n\n[1] 0\n\n\nCode\nlength(which(st_is_valid(library.sf) == FALSE))\n\n\n[1] 0\n\n\nCode\nlength(which(st_is_valid(sport.sf) == FALSE))\n\n\n[1] 0\n\n\nCode\nlength(which(st_is_valid(busstop.sf) == FALSE))\n\n\n[1] 0\n\n\nCode\nlength(which(st_is_valid(shoppingmall.sf) == FALSE))\n\n\n[1] 0\n\n\nCode\nlength(which(st_is_valid(supermarket.sf) == FALSE))\n\n\n[1] 0\n\n\nCode\nlength(which(st_is_valid(primarysch.sf) == FALSE))\n\n\n[1] 0\n\n\nCode\nlength(which(st_is_valid(goodprimarysch.sf) == FALSE))\n\n\n[1] 0\n\n\nCode\nlength(which(st_is_valid(mrt.sf) == FALSE))\n\n\n[1] 0\n\n\nCode\nlength(which(st_is_valid(mpsz.sf) == FALSE))\n\n\n[1] 6\n\n\nWe see that mpsz has 6 invalid geometries. Lets address them and check again\n\n\nCode\nmpsz.sf <- st_make_valid(mpsz.sf)\nlength(which(st_is_valid(mpsz.sf) == FALSE))\n\n\n[1] 0\n\n\n\n\n3.4.3 Check for missing values\n\n\nCode\neldercare.sf[rowSums(is.na(eldercare.sf))!=0,]\n\n\nSimple feature collection with 0 features and 1 field\nBounding box:  xmin: NA ymin: NA xmax: NA ymax: NA\nGeodetic CRS:  WGS 84\n[1] NAME     geometry\n<0 rows> (or 0-length row.names)\n\n\n\n\nCode\nhawkercentre.sf[rowSums(is.na(hawkercentre.sf))!=0,]\n\n\nSimple feature collection with 0 features and 1 field\nBounding box:  xmin: NA ymin: NA xmax: NA ymax: NA\nGeodetic CRS:  WGS 84\n[1] NAME     geometry\n<0 rows> (or 0-length row.names)\n\n\n\n\nCode\nchildcare.sf[rowSums(is.na(childcare.sf))!=0,]\n\n\nSimple feature collection with 0 features and 1 field\nBounding box:  xmin: NA ymin: NA xmax: NA ymax: NA\nGeodetic CRS:  WGS 84\n[1] NAME     geometry\n<0 rows> (or 0-length row.names)\n\n\n\n\nCode\nkindergartens.sf[rowSums(is.na(kindergartens.sf))!=0,]\n\n\nSimple feature collection with 0 features and 5 fields\nBounding box:  xmin: NA ymin: NA xmax: NA ymax: NA\nGeodetic CRS:  WGS 84\n[1] NAME     DESCRIP  ADDRESSP ADDRESSS ICON_NA  geometry\n<0 rows> (or 0-length row.names)\n\n\n\n\nCode\nparks.sf[rowSums(is.na(parks.sf))!=0,]\n\n\nSimple feature collection with 0 features and 1 field\nBounding box:  xmin: NA ymin: NA xmax: NA ymax: NA\nGeodetic CRS:  WGS 84\n[1] NAME     geometry\n<0 rows> (or 0-length row.names)\n\n\n\n\nCode\nlibrary.sf[rowSums(is.na(library.sf))!=0,]\n\n\nSimple feature collection with 0 features and 1 field\nBounding box:  xmin: NA ymin: NA xmax: NA ymax: NA\nGeodetic CRS:  WGS 84\n[1] NAME     geometry\n<0 rows> (or 0-length row.names)\n\n\n\n\nCode\nsport.sf[rowSums(is.na(sport.sf))!=0,]\n\n\nSimple feature collection with 0 features and 1 field\nBounding box:  xmin: NA ymin: NA xmax: NA ymax: NA\nGeodetic CRS:  WGS 84\n[1] NAME     geometry\n<0 rows> (or 0-length row.names)\n\n\n\n\nCode\nbusstop.sf[rowSums(is.na(busstop.sf))!=0,]\n\n\nSimple feature collection with 0 features and 1 field\nBounding box:  xmin: NA ymin: NA xmax: NA ymax: NA\nProjected CRS: SVY21\n[1] BUS_STOP_N geometry  \n<0 rows> (or 0-length row.names)\n\n\n\n\nCode\nshoppingmall.sf[rowSums(is.na(shoppingmall.sf))!=0,]\n\n\nSimple feature collection with 0 features and 1 field\nBounding box:  xmin: NA ymin: NA xmax: NA ymax: NA\nGeodetic CRS:  WGS 84\n[1] name     geometry\n<0 rows> (or 0-length row.names)\n\n\n\n\nCode\nsupermarket.sf[rowSums(is.na(supermarket.sf))!=0,]\n\n\nSimple feature collection with 0 features and 1 field\nBounding box:  xmin: NA ymin: NA xmax: NA ymax: NA\nProjected CRS: SVY21\n[1] LIC_NAME geometry\n<0 rows> (or 0-length row.names)\n\n\n\n\nCode\nprimarysch.sf[rowSums(is.na(primarysch.sf))!=0,]\n\n\nSimple feature collection with 0 features and 1 field\nBounding box:  xmin: NA ymin: NA xmax: NA ymax: NA\nGeodetic CRS:  WGS 84\n[1] schl_nm  geometry\n<0 rows> (or 0-length row.names)\n\n\n\n\nCode\ngoodprimarysch.sf[rowSums(is.na(goodprimarysch.sf))!=0,]\n\n\nSimple feature collection with 0 features and 1 field\nBounding box:  xmin: NA ymin: NA xmax: NA ymax: NA\nGeodetic CRS:  WGS 84\n[1] schl_nm  geometry\n<0 rows> (or 0-length row.names)\n\n\n\n\nCode\nmrt.sf[rowSums(is.na(mrt.sf))!=0,]\n\n\nSimple feature collection with 0 features and 1 field\nBounding box:  xmin: NA ymin: NA xmax: NA ymax: NA\nGeodetic CRS:  WGS 84\n[1] Name     geometry\n<0 rows> (or 0-length row.names)\n\n\n\n\n\n3.5 Verifying + Transforming Coordinate system\n\nBaseExtractedSourced\n\n\n\n\nCode\nst_crs(mpsz.sf)\n\n\nCoordinate Reference System:\n  User input: SVY21 / Singapore TM \n  wkt:\nPROJCRS[\"SVY21 / Singapore TM\",\n    BASEGEOGCRS[\"SVY21\",\n        DATUM[\"SVY21\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4757]],\n    CONVERSION[\"Singapore Transverse Mercator\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"northing (N)\",north,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"easting (E)\",east,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Cadastre, engineering survey, topographic mapping.\"],\n        AREA[\"Singapore - onshore and offshore.\"],\n        BBOX[1.13,103.59,1.47,104.07]],\n    ID[\"EPSG\",3414]]\n\n\n\n\n\n\nCode\nst_crs(eldercare.sf)\n\n\nCoordinate Reference System:\n  User input: WGS 84 \n  wkt:\nGEOGCRS[\"WGS 84\",\n    DATUM[\"World Geodetic System 1984\",\n        ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n            LENGTHUNIT[\"metre\",1]]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"latitude\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"longitude\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    ID[\"EPSG\",4326]]\n\n\nCode\nst_crs(hawkercentre.sf)\n\n\nCoordinate Reference System:\n  User input: WGS 84 \n  wkt:\nGEOGCRS[\"WGS 84\",\n    DATUM[\"World Geodetic System 1984\",\n        ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n            LENGTHUNIT[\"metre\",1]]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"latitude\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"longitude\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    ID[\"EPSG\",4326]]\n\n\nCode\nst_crs(childcare.sf)\n\n\nCoordinate Reference System:\n  User input: WGS 84 \n  wkt:\nGEOGCRS[\"WGS 84\",\n    DATUM[\"World Geodetic System 1984\",\n        ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n            LENGTHUNIT[\"metre\",1]]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"latitude\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"longitude\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    ID[\"EPSG\",4326]]\n\n\nCode\nst_crs(kindergartens.sf)\n\n\nCoordinate Reference System:\n  User input: WGS 84 \n  wkt:\nGEOGCRS[\"WGS 84\",\n    DATUM[\"World Geodetic System 1984\",\n        ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n            LENGTHUNIT[\"metre\",1]]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"latitude\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"longitude\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    ID[\"EPSG\",4326]]\n\n\nCode\nst_crs(parks.sf)\n\n\nCoordinate Reference System:\n  User input: WGS 84 \n  wkt:\nGEOGCRS[\"WGS 84\",\n    DATUM[\"World Geodetic System 1984\",\n        ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n            LENGTHUNIT[\"metre\",1]]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"latitude\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"longitude\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    ID[\"EPSG\",4326]]\n\n\nCode\nst_crs(library.sf)\n\n\nCoordinate Reference System:\n  User input: WGS 84 \n  wkt:\nGEOGCRS[\"WGS 84\",\n    DATUM[\"World Geodetic System 1984\",\n        ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n            LENGTHUNIT[\"metre\",1]]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"latitude\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"longitude\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    ID[\"EPSG\",4326]]\n\n\nCode\nst_crs(sport.sf)\n\n\nCoordinate Reference System:\n  User input: WGS 84 \n  wkt:\nGEOGCRS[\"WGS 84\",\n    DATUM[\"World Geodetic System 1984\",\n        ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n            LENGTHUNIT[\"metre\",1]]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"latitude\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"longitude\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    ID[\"EPSG\",4326]]\n\n\n\n\n\n\nCode\nst_crs(busstop.sf)\n\n\nCoordinate Reference System:\n  User input: SVY21 \n  wkt:\nPROJCRS[\"SVY21\",\n    BASEGEOGCRS[\"WGS 84\",\n        DATUM[\"World Geodetic System 1984\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]],\n            ID[\"EPSG\",6326]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"Degree\",0.0174532925199433]]],\n    CONVERSION[\"unnamed\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"(E)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]],\n        AXIS[\"(N)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]]]\n\n\nCode\nst_crs(shoppingmall.sf)\n\n\nCoordinate Reference System:\n  User input: WGS 84 \n  wkt:\nGEOGCRS[\"WGS 84\",\n    DATUM[\"World Geodetic System 1984\",\n        ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n            LENGTHUNIT[\"metre\",1]]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"latitude\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"longitude\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    ID[\"EPSG\",4326]]\n\n\nCode\nst_crs(supermarket.sf)\n\n\nCoordinate Reference System:\n  User input: SVY21 \n  wkt:\nPROJCRS[\"SVY21\",\n    BASEGEOGCRS[\"WGS 84\",\n        DATUM[\"World Geodetic System 1984\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]],\n            ID[\"EPSG\",6326]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"Degree\",0.0174532925199433]]],\n    CONVERSION[\"unnamed\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"(E)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]],\n        AXIS[\"(N)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]]]\n\n\nCode\nst_crs(primarysch.sf)\n\n\nCoordinate Reference System:\n  User input: WGS 84 \n  wkt:\nGEOGCRS[\"WGS 84\",\n    DATUM[\"World Geodetic System 1984\",\n        ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n            LENGTHUNIT[\"metre\",1]]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"latitude\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"longitude\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    ID[\"EPSG\",4326]]\n\n\nCode\nst_crs(goodprimarysch.sf)\n\n\nCoordinate Reference System:\n  User input: WGS 84 \n  wkt:\nGEOGCRS[\"WGS 84\",\n    DATUM[\"World Geodetic System 1984\",\n        ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n            LENGTHUNIT[\"metre\",1]]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"latitude\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"longitude\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    ID[\"EPSG\",4326]]\n\n\nCode\nst_crs(mrt.sf)\n\n\nCoordinate Reference System:\n  User input: WGS 84 \n  wkt:\nGEOGCRS[\"WGS 84\",\n    DATUM[\"World Geodetic System 1984\",\n        ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n            LENGTHUNIT[\"metre\",1]]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"latitude\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"longitude\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    ID[\"EPSG\",4326]]\n\n\nNote: our projected CRS should be SVY21 (EPSG code 3414). However, some of the extracted data sets are in WGS84, and others are in SVY21 with the incorrect EPSG code (EPSG 9001)\n\n\n\n\n\n3.6 Transforming CRS\nWith the invalid geometries found earlier, we use the functions st_set_crs() and st_transform() to correct them.\n\n\nCode\n#st_set_crs() assigns proper EPSG code\nbusstop.sf<-st_set_crs(busstop.sf, 3414)\n\n\nWarning: st_crs<- : replacing crs does not reproject data; use st_transform for\nthat\n\n\nCode\nsupermarket.sf<-st_set_crs(supermarket.sf, 3414)\n\n\nWarning: st_crs<- : replacing crs does not reproject data; use st_transform for\nthat\n\n\nCode\n#st_transform() changes CRS from one to another\neldercare.sf<-st_transform(eldercare.sf, crs=3414)\nhawkercentre.sf<-st_transform(hawkercentre.sf, crs=3414)\nchildcare.sf<-st_transform(childcare.sf, crs=3414)\nkindergartens.sf<-st_transform(kindergartens.sf, crs=3414)\nparks.sf<-st_transform(parks.sf, crs=3414)\nlibrary.sf<-st_transform(library.sf, crs=3414)\nsport.sf<-st_transform(sport.sf, crs=3414)\nshoppingmall.sf<-st_transform(shoppingmall.sf, crs=3414)\nprimarysch.sf<-st_transform(primarysch.sf, crs=3414)\ngoodprimarysch.sf<-st_transform(goodprimarysch.sf, crs=3414)\nmrt.sf<-st_transform(mrt.sf, crs=3414)\n\n\n\n\n3.7 Checking crs\nSubsequently, we check again to make sure the data is in the proper CRS.\n\nBaseExtractedSourced\n\n\n\n\nCode\nst_crs(mpsz.sf)\n\n\nCoordinate Reference System:\n  User input: SVY21 / Singapore TM \n  wkt:\nPROJCRS[\"SVY21 / Singapore TM\",\n    BASEGEOGCRS[\"SVY21\",\n        DATUM[\"SVY21\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4757]],\n    CONVERSION[\"Singapore Transverse Mercator\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"northing (N)\",north,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"easting (E)\",east,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Cadastre, engineering survey, topographic mapping.\"],\n        AREA[\"Singapore - onshore and offshore.\"],\n        BBOX[1.13,103.59,1.47,104.07]],\n    ID[\"EPSG\",3414]]\n\n\n\n\n\n\nCode\nst_crs(eldercare.sf)\n\n\nCoordinate Reference System:\n  User input: EPSG:3414 \n  wkt:\nPROJCRS[\"SVY21 / Singapore TM\",\n    BASEGEOGCRS[\"SVY21\",\n        DATUM[\"SVY21\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4757]],\n    CONVERSION[\"Singapore Transverse Mercator\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"northing (N)\",north,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"easting (E)\",east,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Cadastre, engineering survey, topographic mapping.\"],\n        AREA[\"Singapore - onshore and offshore.\"],\n        BBOX[1.13,103.59,1.47,104.07]],\n    ID[\"EPSG\",3414]]\n\n\nCode\nst_crs(hawkercentre.sf)\n\n\nCoordinate Reference System:\n  User input: EPSG:3414 \n  wkt:\nPROJCRS[\"SVY21 / Singapore TM\",\n    BASEGEOGCRS[\"SVY21\",\n        DATUM[\"SVY21\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4757]],\n    CONVERSION[\"Singapore Transverse Mercator\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"northing (N)\",north,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"easting (E)\",east,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Cadastre, engineering survey, topographic mapping.\"],\n        AREA[\"Singapore - onshore and offshore.\"],\n        BBOX[1.13,103.59,1.47,104.07]],\n    ID[\"EPSG\",3414]]\n\n\nCode\nst_crs(childcare.sf)\n\n\nCoordinate Reference System:\n  User input: EPSG:3414 \n  wkt:\nPROJCRS[\"SVY21 / Singapore TM\",\n    BASEGEOGCRS[\"SVY21\",\n        DATUM[\"SVY21\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4757]],\n    CONVERSION[\"Singapore Transverse Mercator\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"northing (N)\",north,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"easting (E)\",east,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Cadastre, engineering survey, topographic mapping.\"],\n        AREA[\"Singapore - onshore and offshore.\"],\n        BBOX[1.13,103.59,1.47,104.07]],\n    ID[\"EPSG\",3414]]\n\n\nCode\nst_crs(kindergartens.sf)\n\n\nCoordinate Reference System:\n  User input: EPSG:3414 \n  wkt:\nPROJCRS[\"SVY21 / Singapore TM\",\n    BASEGEOGCRS[\"SVY21\",\n        DATUM[\"SVY21\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4757]],\n    CONVERSION[\"Singapore Transverse Mercator\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"northing (N)\",north,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"easting (E)\",east,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Cadastre, engineering survey, topographic mapping.\"],\n        AREA[\"Singapore - onshore and offshore.\"],\n        BBOX[1.13,103.59,1.47,104.07]],\n    ID[\"EPSG\",3414]]\n\n\nCode\nst_crs(parks.sf)\n\n\nCoordinate Reference System:\n  User input: EPSG:3414 \n  wkt:\nPROJCRS[\"SVY21 / Singapore TM\",\n    BASEGEOGCRS[\"SVY21\",\n        DATUM[\"SVY21\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4757]],\n    CONVERSION[\"Singapore Transverse Mercator\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"northing (N)\",north,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"easting (E)\",east,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Cadastre, engineering survey, topographic mapping.\"],\n        AREA[\"Singapore - onshore and offshore.\"],\n        BBOX[1.13,103.59,1.47,104.07]],\n    ID[\"EPSG\",3414]]\n\n\nCode\nst_crs(library.sf)\n\n\nCoordinate Reference System:\n  User input: EPSG:3414 \n  wkt:\nPROJCRS[\"SVY21 / Singapore TM\",\n    BASEGEOGCRS[\"SVY21\",\n        DATUM[\"SVY21\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4757]],\n    CONVERSION[\"Singapore Transverse Mercator\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"northing (N)\",north,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"easting (E)\",east,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Cadastre, engineering survey, topographic mapping.\"],\n        AREA[\"Singapore - onshore and offshore.\"],\n        BBOX[1.13,103.59,1.47,104.07]],\n    ID[\"EPSG\",3414]]\n\n\nCode\nst_crs(sport.sf)\n\n\nCoordinate Reference System:\n  User input: EPSG:3414 \n  wkt:\nPROJCRS[\"SVY21 / Singapore TM\",\n    BASEGEOGCRS[\"SVY21\",\n        DATUM[\"SVY21\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4757]],\n    CONVERSION[\"Singapore Transverse Mercator\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"northing (N)\",north,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"easting (E)\",east,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Cadastre, engineering survey, topographic mapping.\"],\n        AREA[\"Singapore - onshore and offshore.\"],\n        BBOX[1.13,103.59,1.47,104.07]],\n    ID[\"EPSG\",3414]]\n\n\n\n\n\n\nCode\nst_crs(busstop.sf)\n\n\nCoordinate Reference System:\n  User input: EPSG:3414 \n  wkt:\nPROJCRS[\"SVY21 / Singapore TM\",\n    BASEGEOGCRS[\"SVY21\",\n        DATUM[\"SVY21\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4757]],\n    CONVERSION[\"Singapore Transverse Mercator\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"northing (N)\",north,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"easting (E)\",east,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Cadastre, engineering survey, topographic mapping.\"],\n        AREA[\"Singapore - onshore and offshore.\"],\n        BBOX[1.13,103.59,1.47,104.07]],\n    ID[\"EPSG\",3414]]\n\n\nCode\nst_crs(shoppingmall.sf)\n\n\nCoordinate Reference System:\n  User input: EPSG:3414 \n  wkt:\nPROJCRS[\"SVY21 / Singapore TM\",\n    BASEGEOGCRS[\"SVY21\",\n        DATUM[\"SVY21\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4757]],\n    CONVERSION[\"Singapore Transverse Mercator\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"northing (N)\",north,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"easting (E)\",east,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Cadastre, engineering survey, topographic mapping.\"],\n        AREA[\"Singapore - onshore and offshore.\"],\n        BBOX[1.13,103.59,1.47,104.07]],\n    ID[\"EPSG\",3414]]\n\n\nCode\nst_crs(supermarket.sf)\n\n\nCoordinate Reference System:\n  User input: EPSG:3414 \n  wkt:\nPROJCRS[\"SVY21 / Singapore TM\",\n    BASEGEOGCRS[\"SVY21\",\n        DATUM[\"SVY21\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4757]],\n    CONVERSION[\"Singapore Transverse Mercator\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"northing (N)\",north,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"easting (E)\",east,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Cadastre, engineering survey, topographic mapping.\"],\n        AREA[\"Singapore - onshore and offshore.\"],\n        BBOX[1.13,103.59,1.47,104.07]],\n    ID[\"EPSG\",3414]]\n\n\nCode\nst_crs(primarysch.sf)\n\n\nCoordinate Reference System:\n  User input: EPSG:3414 \n  wkt:\nPROJCRS[\"SVY21 / Singapore TM\",\n    BASEGEOGCRS[\"SVY21\",\n        DATUM[\"SVY21\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4757]],\n    CONVERSION[\"Singapore Transverse Mercator\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"northing (N)\",north,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"easting (E)\",east,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Cadastre, engineering survey, topographic mapping.\"],\n        AREA[\"Singapore - onshore and offshore.\"],\n        BBOX[1.13,103.59,1.47,104.07]],\n    ID[\"EPSG\",3414]]\n\n\nCode\nst_crs(goodprimarysch.sf)\n\n\nCoordinate Reference System:\n  User input: EPSG:3414 \n  wkt:\nPROJCRS[\"SVY21 / Singapore TM\",\n    BASEGEOGCRS[\"SVY21\",\n        DATUM[\"SVY21\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4757]],\n    CONVERSION[\"Singapore Transverse Mercator\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"northing (N)\",north,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"easting (E)\",east,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Cadastre, engineering survey, topographic mapping.\"],\n        AREA[\"Singapore - onshore and offshore.\"],\n        BBOX[1.13,103.59,1.47,104.07]],\n    ID[\"EPSG\",3414]]\n\n\nCode\nst_crs(mrt.sf)\n\n\nCoordinate Reference System:\n  User input: EPSG:3414 \n  wkt:\nPROJCRS[\"SVY21 / Singapore TM\",\n    BASEGEOGCRS[\"SVY21\",\n        DATUM[\"SVY21\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4757]],\n    CONVERSION[\"Singapore Transverse Mercator\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"northing (N)\",north,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"easting (E)\",east,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Cadastre, engineering survey, topographic mapping.\"],\n        AREA[\"Singapore - onshore and offshore.\"],\n        BBOX[1.13,103.59,1.47,104.07]],\n    ID[\"EPSG\",3414]]\n\n\n\n\n\nEverything looks fine! 🙃We are now done with wrangling our collected datasets!"
  },
  {
    "objectID": "Geospatial/Take-home_Ex03/Take-home_Ex03.html#exploratory-data-analysis",
    "href": "Geospatial/Take-home_Ex03/Take-home_Ex03.html#exploratory-data-analysis",
    "title": "Take-home Exercise 3: Predicting HDB Public Housing Resale Pricies using Geographically Weighted Methods",
    "section": "4 Exploratory Data Analysis",
    "text": "4 Exploratory Data Analysis\nLets do a quick EDA for the data we collected!\n\nTransportation 🚃Education 📚Amenities 🚲\n\n\n\n\nCode\n#transport graph\ntmap_mode(\"plot\")\n\n\ntmap mode set to plotting\n\n\nCode\ntm_shape(mpsz.sf) +\n  tm_borders(alpha = 0.5) +\n  tmap_options(check.and.fix = TRUE) +\ntm_shape(busstop.sf) +\n  tm_dots(col=\"red\", size=0.05, alpha=0.5) +\ntm_shape(mrt.sf) +\n  tm_dots(col=\"green\", alpha=1)+\n  tm_layout(main.title = \"Transportation\",\n          main.title.position = \"center\",\n          main.title.size = 1.2,\n          frame = TRUE)\n\n\n\n\n\n\n\n\n\nCode\n#education\ntmap_mode(\"plot\")\n\n\ntmap mode set to plotting\n\n\nCode\ntm_shape(mpsz.sf) +\n  tm_borders(alpha = 0.5) +\n  tmap_options(check.and.fix = TRUE) +\ntm_shape(primarysch.sf) +\n  tm_dots(col=\"pink\", size=0.05, alpha=0.5) +\ntm_shape(goodprimarysch.sf) +\n  tm_dots(col=\"red\", size=0.05, alpha=1) +\ntm_shape(kindergartens.sf) +\n  tm_dots(col=\"purple\", size=0.05, alpha=0.5) +\ntm_shape(childcare.sf) +\n  tm_dots(col=\"yellow\", size=0.05, alpha=0.2) +\n  tm_layout(main.title = \"Education\",\n          main.title.position = \"center\",\n          main.title.size = 1.2,\n          frame = TRUE)\n\n\n\n\n\n\n\n\n\nCode\n#ammenities\ntmap_mode(\"plot\")\n\n\ntmap mode set to plotting\n\n\nCode\ntm_shape(mpsz.sf) +\n  tm_borders(alpha = 0.5) +\n  tmap_options(check.and.fix = TRUE) +\ntm_shape(parks.sf) +\n  tm_dots(col=\"green\", size=0.05, alpha=0.5) +\ntm_shape(hawkercentre.sf) +\n  tm_dots(col=\"blue\", size=0.05, alpha=0.5) +\ntm_shape(library.sf) +\n  tm_dots(col=\"red\", size=0.05, alpha=0.5) +\ntm_shape(sport.sf) +\n  tm_dots(col=\"grey\", size=0.05, alpha=0.2) +\ntm_shape(shoppingmall.sf) +\n  tm_dots(col=\"purple\", size=0.05, alpha=0.2) +\ntm_shape(supermarket.sf) +\n  tm_dots(col=\"brown\", size=0.05, alpha=0.2) +\ntm_shape(eldercare.sf) +\n  tm_dots(col=\"navy\", size=0.05, alpha=0.2) +\n  tm_layout(main.title = \"Ammenities\",\n          main.title.position = \"center\",\n          main.title.size = 1.2,\n          frame = TRUE)"
  },
  {
    "objectID": "Geospatial/Take-home_Ex03/Take-home_Ex03.html#data-wrangling-1",
    "href": "Geospatial/Take-home_Ex03/Take-home_Ex03.html#data-wrangling-1",
    "title": "Take-home Exercise 3: Predicting HDB Public Housing Resale Pricies using Geographically Weighted Methods",
    "section": "5 Data Wrangling",
    "text": "5 Data Wrangling\n\n5.1 Structural Factors\nThis includes the inherent structural factors of the flats, including Area, Floor Level and Lease Remaining years.\n\n5.1.1 Floor Level - create as ordinal\nTo simplify analysis, we create the an ordinal column for the floor level variable. This code chunk matches the story levels to the list, and gives it an ordinal value based on its relative positioning in the list given (1 would be allocated to “01 TO 03”, 2 allocated to “04 TO 06”….).\n\n\nCode\n# Define the story levels and ordinal values \nstory_levels <- c(\"01 TO 03\", \"04 TO 06\", \"07 TO 09\", \"10 TO 12\", \"13 TO 15\", \"16 TO 18\", \"19 TO 21\", \"22 TO 24\", \"25 TO 27\", \"28 TO 30\", \"31 TO 33\", \"34 TO 36\", \"37 TO 39\", \"40 TO 42\", \"43 TO 45\", \"46 TO 48\", \"49 TO 51\") \nstory_ordinal <- seq_along(story_levels)  # Create the ordinal variable based on the story column \nresale$Story_Ordinal <- story_ordinal[match(resale$storey_range, story_levels)]  # Set the labels for the ordinal variable \nlevels(resale$Story_Ordinal) <- story_levels\n\n# Lastly, we mutate the ordinal variable as a numeric\nresale<- resale |> \n  mutate(Story_Ordinal=as.numeric(Story_Ordinal))\n\n\n\n\n5.1.2 Remaining lease - mutate to years\nThe code chunk below seeks to convert the remaining lease period to in terms of years.\n\n\nCode\nstr_list <- str_split(resale$remaining_lease, \" \")\n\nfor (i in 1:length(str_list)) {\n  if (length(unlist(str_list[i])) > 2) {\n      year <- as.numeric(unlist(str_list[i])[1])\n      month <- as.numeric(unlist(str_list[i])[3])\n      resale$remaining_lease[i] <- year + round(month/12, 2)\n  }\n  else {\n    year <- as.numeric(unlist(str_list[i])[1])\n    resale$remaining_lease[i] <- year\n  }\n}\n\n\n\n\n\n5.2 Locational Factors\nThis includes factors that concern location of amenities to the flat in question. For instance, Proximity to CBD or number of primary schools withi 350 m from the flat in question.\n\n5.2.1 CBD Location\nWe need to factor in the proximity to CBD in the downtown core. As such let’s take the coordinates of Downtown Core to be the coordinates of the CBD\n\n\nCode\nlat <- 1.287953\nlng <- 103.851784\n\ncbd.sf <- data.frame(lat, lng) %>%\n  st_as_sf(coords = c(\"lng\", \"lat\"), crs=4326) %>%\n  st_transform(crs=3414)\n\n\n\n\n5.2.2 Proximity Distance Calculation\n\n5.2.2.1 Converting resale df into sf object\n\n\nCode\nresale.sf <- st_as_sf(resale, \n                      coords = c(\"LONGITUDE\", \n                                 \"LATITUDE\"), \n                      crs=4326) %>%\n  #afterwards, we transform it to SVY21, our desired CRS\n  st_transform(crs = 3414)\n\n\nOne of the things we need to find is the proximity to particular facilities - which we can compute with st_distance(), and find the closest facility (shortest distance) with the rowMins() function of our matrixStats package. The values will be appended to the data frame as a new column.\n\n\nCode\nlibrary(units)\nlibrary(matrixStats)\nproximity <- function(df1, df2, varname) {\n  dist_matrix <- st_distance(df1, df2) |> \n    drop_units()\n  df1[,varname] <- rowMins(dist_matrix)\n  return(df1)\n}\n\n\n\n\n5.2.2.2 Implementation\n\n\nCode\nresale.sf <- \n  proximity(resale.sf, cbd.sf, \"PROX_CBD\")\n\nresale.sf <- \n  proximity(resale.sf, eldercare.sf, \"PROX_ELDERCARE\")\n\nresale.sf <- \n  proximity(resale.sf, hawkercentre.sf, \"PROX_HAWKERCENTRE\")\n\nresale.sf <- \n  proximity(resale.sf, childcare.sf, \"PROX_CHILDCARE\")\n\nresale.sf <- \n  proximity(resale.sf, kindergartens.sf, \"PROX_KINDERGARTEN\")\n\nresale.sf <- \n  proximity(resale.sf, parks.sf, \"PROX_PARK\")\n\nresale.sf <- \n  proximity(resale.sf, library.sf, \"PROX_LIBRARY\")\n\nresale.sf <- \n  proximity(resale.sf, sport.sf, \"PROX_SPORT\")\n\nresale.sf <- \n  proximity(resale.sf, busstop.sf, \"PROX_BUSSTOP\")\n\nresale.sf <- \n  proximity(resale.sf, shoppingmall.sf, \"PROX_SHOPPINGMALL\")\n\nresale.sf <- \n  proximity(resale.sf, supermarket.sf, \"PROX_SUPERMARKET\")\n\nresale.sf <- \n  proximity(resale.sf, primarysch.sf, \"PROX_PRIMARYSCH\")\n\nresale.sf <- \n  proximity(resale.sf, goodprimarysch.sf, \"PROX_GOODPRIMARYSCH\")\n\nresale.sf <- \n  proximity(resale.sf, mrt.sf, \"PROX_MRT\")\n\n\n\n\n\n5.2.3 Facility Count within radius calculation\nBesides proximity, which calculates the shortest distance, we also want to find the number of facilities within a particular radius.\n\n\nCode\nnum_radius <- function(df1, df2, varname, radius) {\n  dist_matrix <- st_distance(df1, df2) %>%\n    drop_units() %>%\n    as.data.frame()\n  df1[,varname] <- rowSums(dist_matrix <= radius)\n  return(df1)\n}\n\n\n\n5.2.3.1 Implementation\n\n\nCode\nresale.sf <- \n  num_radius(resale.sf, busstop.sf, \"NUM_BUSSTOP\", 350)\n  \nresale.sf <-\n  num_radius(resale.sf, childcare.sf, \"NUM_CHILDCARE\", 350)\n\nresale.sf <-\n  num_radius(resale.sf, kindergartens.sf, \"NUM_KINDERGARTEN\", 350)\n\nresale.sf <-\n  num_radius(resale.sf, primarysch.sf, \"NUM_PRIMARYSCH\", 1000)\n\n\n\n\n\n5.2.4 Saving Resale.sf as a rds file\nWe are finally done with the inclusion of our prox and num variables 😫. We can now save our rds object into our model file for easy retrieval!\n\n\nCode\nsaveRDS(resale.sf, \"data/model/resale.sf.rds\")\n\n\n\n\n5.2.5 Reading rds file\n\n\nCode\nresale.sf<-read_rds(\"data/model/resale.sf.rds\")\n\nresale.sf<-resale.sf |> \n  mutate(LEASE_YRS=as.double(LEASE_YRS))"
  },
  {
    "objectID": "Geospatial/Take-home_Ex03/Take-home_Ex03.html#regressions-1",
    "href": "Geospatial/Take-home_Ex03/Take-home_Ex03.html#regressions-1",
    "title": "Take-home Exercise 3: Predicting HDB Public Housing Resale Pricies using Geographically Weighted Methods",
    "section": "6 Regressions",
    "text": "6 Regressions\nWith all our data prepared, we are now ready to look into regressing the data to create our prediction models! In particular, we explored the use of OLS, geographical weighted regression, random forest and geographical random forest in this section. 🌳🌲🌴\n\n6.1 Data Preparation\n\n6.1.1 Building a correlation matrix\nIn order to avoid issues of multicollinearity in regression models, we first visualise our data using a correlation matrix.\n\n\nCode\nresale_nogeo <- resale.sf |> \n  st_drop_geometry() |> \n  select_if(is.numeric) |> \n  select(-PRICE)\ncorrplot::corrplot(cor(resale_nogeo), \n                   diag = FALSE, \n                   order = \"AOE\",\n                   tl.pos = \"td\", \n                   tl.cex = 0.5, \n                   method = \"number\", \n                   type = \"upper\")\n\n\n\n\n\nWe realise that the variables “lease_commence_date” which is the beginning year of lease, and “LEASE_YRS” which is the number of years left in the lease, are perfectly collinear with a correlation coeffecient of 1. We choose to drop “lease_commence_date” to avoid the problem of perfect collinearity. This creates our final resale df (“resale_final”).\n\n\nCode\nresale_final <- resale.sf |> \n  select(-lease_commence_date)\n\n\n\n\n6.1.2 Splitting data into test and training set\nFrom the assignment brief, our training data consists of transactions that occurred from 1st January 2021 to 31st December 2022, while test data occurred from January and February 2023.\n\n\nCode\ntrain_data<-resale_final |> \n  filter(month >= \"2021-01\" & month <= \"2022-12\")\n\ntest_data<-resale_final |> \n  filter(month >= \"2023-01\" & month <= \"2023-02\")\n\n\n\n\n\n6.2 Building a non spatial multiple linear regression\nFirst we explored the creation of an OLS model in the code chunk below:\n\n\nCode\nprice_mlr<- lm(PRICE~ Story_Ordinal + LEASE_YRS + AREA_SQM + PROX_CBD + PROX_ELDERCARE + PROX_HAWKERCENTRE + PROX_PARK + PROX_LIBRARY + PROX_SPORT + PROX_BUSSTOP + PROX_SHOPPINGMALL + PROX_SUPERMARKET + PROX_GOODPRIMARYSCH + PROX_MRT + NUM_CHILDCARE + NUM_KINDERGARTEN + NUM_PRIMARYSCH, data=train_data)\n#removed flat type given that we only consider 4 room flats\n#remove geographical variables given GWR\n#removed month <- questionable interpretability of coefficient (if generated)\n\nsummary(price_mlr)\n\n# saving the result as an rds object\nwrite_rds(price_mlr, \"data/model/price_mlr.rds\")\n\n\nWe can read the output below\n\n\nCode\nprice_mlr_summary<-read_rds(\"data/model/price_mlr.rds\")\nprice_mlr_summary\n\n\n\nCall:\nlm(formula = PRICE ~ Story_Ordinal + LEASE_YRS + AREA_SQM + PROX_CBD + \n    PROX_ELDERCARE + PROX_HAWKERCENTRE + PROX_PARK + PROX_LIBRARY + \n    PROX_SPORT + PROX_BUSSTOP + PROX_SHOPPINGMALL + PROX_SUPERMARKET + \n    PROX_GOODPRIMARYSCH + PROX_MRT + NUM_CHILDCARE + NUM_KINDERGARTEN + \n    NUM_PRIMARYSCH, data = train_data)\n\nCoefficients:\n        (Intercept)        Story_Ordinal            LEASE_YRS  \n          8.701e+04            1.606e+04            4.435e+03  \n           AREA_SQM             PROX_CBD       PROX_ELDERCARE  \n          3.552e+03           -1.631e+01           -8.064e+00  \n  PROX_HAWKERCENTRE            PROX_PARK         PROX_LIBRARY  \n         -2.115e+01            6.458e+00           -2.693e+01  \n         PROX_SPORT         PROX_BUSSTOP    PROX_SHOPPINGMALL  \n          2.054e+00           -1.732e+01           -1.261e+01  \n   PROX_SUPERMARKET  PROX_GOODPRIMARYSCH             PROX_MRT  \n          1.088e+01           -3.043e-01           -1.519e+01  \n      NUM_CHILDCARE     NUM_KINDERGARTEN       NUM_PRIMARYSCH  \n         -2.821e+03            7.866e+03           -1.098e+04  \n\n\nCode\n# a more elegant way of visualising the summary table can be done through using the gtsummary package!\nprice_mlr<-read_rds(\"data/model/price_mlr.rds\")\ngtsummary::tbl_regression(price_mlr)\n\n\n\n\n\n\n  \n  \n    \n      Characteristic\n      Beta\n      95% CI1\n      p-value\n    \n  \n  \n    Story_Ordinal\n16,063\n15,624, 16,502\n<0.001\n    LEASE_YRS\n4,435\n4,359, 4,510\n<0.001\n    AREA_SQM\n3,552\n3,417, 3,688\n<0.001\n    PROX_CBD\n-16\n-17, -16\n<0.001\n    PROX_ELDERCARE\n-8.1\n-9.8, -6.4\n<0.001\n    PROX_HAWKERCENTRE\n-21\n-23, -19\n<0.001\n    PROX_PARK\n6.5\n4.2, 8.7\n<0.001\n    PROX_LIBRARY\n-27\n-29, -25\n<0.001\n    PROX_SPORT\n2.1\n0.67, 3.4\n0.004\n    PROX_BUSSTOP\n-17\n-33, -1.5\n0.031\n    PROX_SHOPPINGMALL\n-13\n-15, -9.9\n<0.001\n    PROX_SUPERMARKET\n11\n4.8, 17\n<0.001\n    PROX_GOODPRIMARYSCH\n-0.30\n-0.69, 0.08\n0.12\n    PROX_MRT\n-15\n-18, -13\n<0.001\n    NUM_CHILDCARE\n-2,821\n-3,295, -2,348\n<0.001\n    NUM_KINDERGARTEN\n7,866\n6,864, 8,869\n<0.001\n    NUM_PRIMARYSCH\n-10,982\n-11,670, -10,294\n<0.001\n  \n  \n  \n    \n      1 CI = Confidence Interval\n    \n  \n\n\n\n\n\n\n6.3 GWR predictive method\ngwr method will be used to calibrate a model to predict the HDB resale prices.\nWe first need to convert the sf data.frame to a SpatialPointDataFrame\n\n\nCode\ntrain_data_sp<- sf::as_Spatial(train_data)\ntrain_data_sp\n\n\nclass       : SpatialPointsDataFrame \nfeatures    : 23656 \nextent      : 11519.79, 42645.18, 28217.39, 48741.06  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +ellps=WGS84 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs \nvariables   : 29\nnames       :   PRICE,   month,       town, flat_type, block,   street_name, storey_range, AREA_SQM,    flat_model, LEASE_YRS, Story_Ordinal,         PROX_CBD,      PROX_ELDERCARE, PROX_HAWKERCENTRE,      PROX_CHILDCARE, ... \nmin values  :  250000, 2021-01, ANG MO KIO,    4 ROOM,     1,  ADMIRALTY DR,     01 TO 03,       70, Adjoined flat,      44.5,             1, 999.393538715878, 1.9938461723793e-05,  30.6031805185926, 1.4128908036435e-05, ... \nmax values  : 1370000, 2022-12,     YISHUN,    4 ROOM,    9B, YUNG SHENG RD,     49 TO 51,      145,       Type S1,     97.33,            17, 19650.0691667807,    3301.63731683139,  2867.63031236184,    547.386819517238, ... \n\n\n\n6.3.1 Computing adaptive bandwidth\nNext, bw.gwr() of GWmodel package will be used to determine the optimal bandwidth to be used.\n\n\nCode\nbw_adaptive <- bw.gwr(PRICE~ Story_Ordinal + LEASE_YRS + AREA_SQM + PROX_CBD + PROX_ELDERCARE + PROX_HAWKERCENTRE + PROX_PARK + PROX_LIBRARY + PROX_SPORT + PROX_BUSSTOP + PROX_SHOPPINGMALL + PROX_SUPERMARKET + PROX_GOODPRIMARYSCH + PROX_MRT + NUM_CHILDCARE + NUM_KINDERGARTEN + NUM_PRIMARYSCH,\n                  data=train_data_sp,\n                  approach=\"CV\",\n                  kernel=\"gaussian\",\n                  adaptive=TRUE,\n                  longlat=FALSE)\n\n# saving the result as an rds object\nwrite_rds(bw_adaptive, \"data/model/bw_adaptive.rds\")\n\n\nThe code below can be used to display the model output.\n\n\nCode\nbw_adaptive <- read_rds(\"data/model/bw_adaptive.rds\")\n\n\n\n\n6.3.2 Constructing gwr model\nForming model\n\n\nCode\ngwr_adaptive <- gwr.basic(formula = PRICE~ Story_Ordinal + LEASE_YRS + AREA_SQM + PROX_CBD + PROX_ELDERCARE + PROX_HAWKERCENTRE + PROX_PARK + PROX_LIBRARY + PROX_SPORT + PROX_BUSSTOP + PROX_SHOPPINGMALL + PROX_SUPERMARKET + PROX_GOODPRIMARYSCH + PROX_MRT + NUM_CHILDCARE + NUM_KINDERGARTEN + NUM_PRIMARYSCH,\n                          data=train_data_sp,\n                          bw=bw_adaptive, \n                          kernel = 'gaussian', \n                          adaptive=TRUE,\n                          longlat = FALSE)\n\n# saving the result as an rds object\nwrite_rds(gwr_adaptive, \"data/model/gwr_adaptive.rds\")\n\n\nThe code below can be used to display the model output.\n\n\nCode\ngwr_adaptive <- read_rds(\"data/model/gwr_adaptive.rds\")\n\n\n\n\n\n6.4 Preparing coordinates data\n\n6.4.1 Extracting coordinates data\nThe code chunk below extracts the x, y coordinates of the full training and test data sets.\n\n\nCode\ncoords<-st_coordinates(resale_final)\ncoords_train<-st_coordinates(train_data)\ncoords_test<-st_coordinates(test_data)\n\n\nSave into rds\n\n\nCode\nwrite_rds(coords_train, \"data/model/coords_train.rds\")\nwrite_rds(coords_test, \"data/model/coords_test.rds\")\n\n\nRead RDS\n\n\nCode\ncoords_train<-read_rds(\"data/model/coords_train.rds\")\ncoords_test<-read_rds(\"data/model/coords_test.rds\")\n\n\n\n\n6.4.2 Dropping geometry field\nDrop geometry column of the sf dataframe by using st_drop_geometry()\n\n\nCode\ntrain_data<-train_data |> \n  st_drop_geometry()\n\nwrite_rds(train_data, \"data/model/train_data.rds\")\n\n\nRead RDS\n\n\nCode\ntrain_data<-read_rds(\"data/model/train_data.rds\")\n\n\n\n\n\n6.5 Calibrating Random Forest Model\nIn this section, we will to calibrate a model to predict HDB resale price by using random forest function of ranger package. 🌳🌲🌴🌳🌲🌴\n\n6.5.1 forming model\n\n\nCode\nset.seed(1234) \nrf<- ranger(PRICE~ Story_Ordinal + LEASE_YRS + AREA_SQM + PROX_CBD + PROX_ELDERCARE + PROX_HAWKERCENTRE + PROX_PARK + PROX_LIBRARY + PROX_SPORT + PROX_BUSSTOP + PROX_SHOPPINGMALL + PROX_SUPERMARKET + PROX_GOODPRIMARYSCH + PROX_MRT + NUM_CHILDCARE + NUM_KINDERGARTEN + NUM_PRIMARYSCH, data=train_data)\n\nsummary(rf)\n\n# saving the result as an rds object\nwrite_rds(rf, \"data/model/rf.rds\")\n\n\nread RDS object\n\n\nCode\nrf<-read_rds(\"data/model/rf.rds\")\nprint(rf)\n\n\nRanger result\n\nCall:\n ranger(PRICE ~ Story_Ordinal + LEASE_YRS + AREA_SQM + PROX_CBD +      PROX_ELDERCARE + PROX_HAWKERCENTRE + PROX_PARK + PROX_LIBRARY +      PROX_SPORT + PROX_BUSSTOP + PROX_SHOPPINGMALL + PROX_SUPERMARKET +      PROX_GOODPRIMARYSCH + PROX_MRT + NUM_CHILDCARE + NUM_KINDERGARTEN +      NUM_PRIMARYSCH, data = train_data) \n\nType:                             Regression \nNumber of trees:                  500 \nSample size:                      23656 \nNumber of independent variables:  17 \nMtry:                             4 \nTarget node size:                 5 \nVariable importance mode:         none \nSplitrule:                        variance \nOOB prediction error (MSE):       1051737948 \nR squared (OOB):                  0.9372898 \n\n\n\n\n\n6.6 Calibrating Geographic random forest using grf\n\n6.6.1 Computing bandwidth for grf\nMinimise trees to trees=50 for ease of computation.\n\n\nCode\n# determining bandwidth for model\nbw_grf_adaptive<-grf.bw(formula = PRICE~ Story_Ordinal + LEASE_YRS + AREA_SQM + PROX_CBD + PROX_ELDERCARE + PROX_HAWKERCENTRE + PROX_PARK + PROX_LIBRARY + PROX_SPORT + PROX_BUSSTOP + PROX_SHOPPINGMALL + PROX_SUPERMARKET + PROX_GOODPRIMARYSCH + PROX_MRT + NUM_CHILDCARE + NUM_KINDERGARTEN + NUM_PRIMARYSCH, dataset = train_data, kernel = \"adaptive\", coords = coords_train, trees=50)\n\n# saving the result as an rds object\nwrite_rds(bw_grf_adaptive, \"data/model/bw_grf_adaptive.rds\")\n\n\nwe were unable to gain the bandwidth result even after extensive waiting. For the assignment requirements, we will consider the bandwidths generated and choose the one that gives the highest R2 value. The output generated can be seen in the screenshots attached:\n\n\nFrom our data gathered, we find that the optimal bandwidth within the bandwidths calculated is when bandwidth = 1211. We then assign this to the bw_grf_adaptive object\n\n\nCode\nbw_grf_adaptive<-1211\n\n\n\n\n6.6.2 Generating model\n\n\nCode\nset.seed(1234)\ngwRF_adaptive<-grf(formula = PRICE~ Story_Ordinal + LEASE_YRS + AREA_SQM + PROX_CBD + PROX_ELDERCARE + PROX_HAWKERCENTRE + PROX_PARK + PROX_LIBRARY + PROX_SPORT + PROX_BUSSTOP + PROX_SHOPPINGMALL + PROX_SUPERMARKET + PROX_GOODPRIMARYSCH + PROX_MRT + NUM_CHILDCARE + NUM_KINDERGARTEN + NUM_PRIMARYSCH,\n                   dframe=train_data,\n                   bw=bw_grf_adaptive, # need to know how to calculate using function\n                   kernel=\"adaptive\",\n                   coords=coords_train,\n                   ntree=50)\n\n# saving the result as an rds object\nwrite_rds(gwRF_adaptive, \"data/model/gwRF_adaptive.rds\")\n\n\nThe output rds file generated is 31.7GB, which is larger than what is recommended to push into Git. Therefore, we append screenshots of our outputs instead. 🌳🌲🌴🌳🌲🌴"
  },
  {
    "objectID": "Geospatial/Take-home_Ex03/Take-home_Ex03.html#predictions",
    "href": "Geospatial/Take-home_Ex03/Take-home_Ex03.html#predictions",
    "title": "Take-home Exercise 3: Predicting HDB Public Housing Resale Pricies using Geographically Weighted Methods",
    "section": "7 Predictions",
    "text": "7 Predictions\n\n7.1 Preparing the test data\nThe code chunk below will be used to combine the test data with the corresponding coordinates data.\n\n\nCode\ntest_data <- cbind(test_data, coords_test) %>%\n  st_drop_geometry()\n\nwrite_rds(test_data, \"data/model/test_data.rds\")\n\n\n\n\n7.2 Predicting with test data\nNext, predict.grf() of spatialML package will be used to predict the resale value by using the test data and gwRF_adaptive model calibrated earlier.\n\n\nCode\ngwRF_pred <- predict.grf(gwRF_adaptive, \n                           test_data, \n                           x.var.name=\"X\",\n                           y.var.name=\"Y\", \n                           local.w=1,\n                           global.w=0)\n\nGRF_pred <- write_rds(gwRF_pred, \"data/model/GRF_pred.rds\")\n\n\n\n\n7.3 Converting the prediction output into a dataframe\nThe output of the predict.grf() is a vector of predicted values. It is wiser to convert it into a data frame for further visualisation and analysis.\n\n\nCode\nGRF_pred <- read_rds(\"data/model/GRF_pred.rds\")\nGRF_pred_df <- as.data.frame(GRF_pred)\n\n\nIn the code chunk below, cbind() is used to append the predicted values onto test_data.\n\n\nCode\ntest_data_p <- cbind(test_data, GRF_pred_df)\n\n#save this as an rds object\nwrite_rds(test_data_p, \"data/model/test_data_p.rds\")\n\n\n\n\n7.4 Calculating Root Mean Square Error\nThe root mean square error (RMSE) allows us to measure how far predicted values are from observed values in a regression analysis. In the code chunk below, rmse() of Metrics package is used to compute the RMSE.\n\n\nCode\nMetrics::rmse(test_data_p$PRICE, \n     test_data_p$GRF_pred)\n\n\n[1] 44545.08\n\n\n\n\n7.5 Visualising the predicted values\nAlternatively, scatterplot can be used to visualise the actual resale price and the predicted resale price by using the code chunk below.\n\n\nCode\nggplot(data = test_data_p,\n       aes(x = GRF_pred,\n           y = PRICE)) +\n  geom_point()"
  },
  {
    "objectID": "Geospatial/Take-home_Ex03/Take-home_Ex03.html#conclusions-comparisons",
    "href": "Geospatial/Take-home_Ex03/Take-home_Ex03.html#conclusions-comparisons",
    "title": "Take-home Exercise 3: Predicting HDB Public Housing Resale Pricies using Geographically Weighted Methods",
    "section": "8 Conclusions: Comparisons",
    "text": "8 Conclusions: Comparisons\nIn the last objective of this take home assignment, a comparison between the conventional OLS method versus the geographically weighted methods should be done.\nAs this task is focused on predictive modelling, a comparison between the OLS method (a method predominantly for explanatory models) and geographical random forest (for predictive modelling with geographical elements).\n\n\n\n\n\n\n\n\n\nOLS\nGeographical random forest\n\n\n\n\nMain purpose\nExplanatory model for standard regressions\nPredictive model for regressions with geographical elements\n\n\nSeeks to minimise\nresidual sum of squares\nmean squared error\n\n\n\nWith a basic understanding of how each model works, we hypothesise that the gwRF model will be the most appropriate for analysis, as it accounts for geographical elements included in the resale flat data, as well as the predictive aspect of this model.\nTo estimate the predictive power of each of the models, we plot each of the predicted values against actual prices in the test data set, and calculate the root MSEs for each of the models.\n\nOLSGeographical Random Forest\n\n\n\n\nCode\nprice_mlr<-read_rds(\"data/model/price_mlr.rds\")\nmlr_pred_df<-predict(price_mlr, test_data)\ntest_data_mlr <- cbind(test_data, mlr_pred_df)\n\nMetrics::rmse(test_data_mlr$PRICE, \n     test_data_mlr$mlr_pred_df)\n\n\n[1] 83524.62\n\n\nCode\nggplot(data = test_data_mlr,\n       aes(x = mlr_pred_df,\n           y = PRICE)) +\n  geom_point()\n\n\n\n\n\n\n\n\n\nCode\n#detailed code chunks can be found in earlier sections\nMetrics::rmse(test_data_p$PRICE, \n     test_data_p$GRF_pred)\n\n\n[1] 44545.08\n\n\nCode\nggplot(data = test_data_p,\n       aes(x = GRF_pred,\n           y = PRICE)) +\n  geom_point()\n\n\n\n\n\n\n\n\nFrom this comparisons, we conclude that the predictive power of geographical weighted random forest was the most powerful to predict resale prices. this affirms our hypothesis that geographical weighted random forest models are the most appropriate for modelling resale flat data. 🌳🌲🌴\nHowever, our analysis in this assignment was limited🙁:\n\nA key flaw was the limited number of trees we used in each forest generated in the gwRF model. Due to limited capacity of our devices, we set ntree = 50 instead of a higher value. A more comprehensive model could have been created with a stronger device that has greater computational power."
  },
  {
    "objectID": "Geospatial/Take-home_Ex03/Take-home_Ex03.html#references",
    "href": "Geospatial/Take-home_Ex03/Take-home_Ex03.html#references",
    "title": "Take-home Exercise 3: Predicting HDB Public Housing Resale Pricies using Geographically Weighted Methods",
    "section": "9 References",
    "text": "9 References\nIn the course of this assignment, reference was taken from our senior Megans work. Check out her website here 🤗: https://is415-msty.netlify.app/posts/2021-10-25-take-home-exercise-3/\nWe also referenced Prof Kam’s in class exercise 9 in this assignment: https://is415-gaa-tskam.netlify.app/in-class_ex/in-class_ex09/in-class_ex09_gwml"
  },
  {
    "objectID": "Geospatial/Take-home_Ex03/data/geospatial/base/MPSZ-2019.html",
    "href": "Geospatial/Take-home_Ex03/data/geospatial/base/MPSZ-2019.html",
    "title": "personalprojects",
    "section": "",
    "text": "<!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’>"
  },
  {
    "objectID": "Geospatial/Take-home_Ex02/Take-home_Ex02.html",
    "href": "Geospatial/Take-home_Ex02/Take-home_Ex02.html",
    "title": "Take-home Exercise 2",
    "section": "",
    "text": "Exploratory Spatial Data Analysis (ESDA) hold tremendous potential to address complex problems facing society. In this study, we are tasked to apply appropriate Local Indicators of Spatial Association (LISA) and Emerging Hot Spot Analysis (EHSA) to undercover the spatio-temporal trends of COVID-19 vaccination in DKI Jakarta.\nThe main tasks of this exercise includes\n\nData Wrangling\nChoropleth mapping\nLocal Gi Analysis\nEmerging hotspot analysis\n\n\n\nWe first start off by loading the necessary R packages into our platform.\n\npacman::p_load(sf, sfdep, tmap, tidyverse, knitr, kableExtra)"
  },
  {
    "objectID": "Geospatial/Take-home_Ex02/Take-home_Ex02.html#load-data",
    "href": "Geospatial/Take-home_Ex02/Take-home_Ex02.html#load-data",
    "title": "Take-home Exercise 2",
    "section": "2.1 Load data",
    "text": "2.1 Load data\nThe datasets that we will be using are listed below:\n\n# initialise a dataframe of our geospatial and aspatial data details\ndatasets <- data.frame(\n  Type=c(\"Geospatial\",\n         \"Aspatial\"),\n  Name=c(\"[Shapefile (SHP Batas Desa Provinsi Sumatera Barat)](https://www.indonesia-geospasial.com/2020/04/download-shapefile-shp-batas-desa.html)\",\n         \"[Open Data Vaksinasi Provinsi DKI Jarkarta](https://riwayat-file-vaksinasi-dki-jakarta-jakartagis.hub.arcgis.com/)\"),\n  Format=c(\"Shapefile\", \n           \".xlsx\"),\n  Description=c(\"Sub-districts in Indonesia\",\n                \"Monthly vaccination Data in Jarkata\")\n  )\n\n# with reference to this guide on kableExtra:\n# https://cran.r-project.org/web/packages/kableExtra/vignettes/awesome_table_in_html.html\n# kable_material is the name of the kable theme\n# 'hover' for to highlight row when hovering, 'scale_down' to adjust table to fit page width\nlibrary(knitr)\nlibrary(kableExtra)\nkable(head(datasets), caption=\"Datasets Used\") %>%\n  kable_material(\"hover\", latex_options=\"scale_down\")\n\n\n\nDatasets Used\n \n  \n    Type \n    Name \n    Format \n    Description \n  \n \n\n  \n    Geospatial \n    [Shapefile (SHP Batas Desa Provinsi Sumatera Barat)](https://www.indonesia-geospasial.com/2020/04/download-shapefile-shp-batas-desa.html) \n    Shapefile \n    Sub-districts in Indonesia \n  \n  \n    Aspatial \n    [Open Data Vaksinasi Provinsi DKI Jarkarta](https://riwayat-file-vaksinasi-dki-jakarta-jakartagis.hub.arcgis.com/) \n    .xlsx \n    Monthly vaccination Data in Jarkata \n  \n\n\n\n\n\n\n2.1.1 Geospatial data\nThe data set can be downloaded at Indonesia Geospatial portal, specifically at this page.\n\n2.1.1.1 Reading Geospatial data into R\nFor the purpose of this study, DKI Jakarta administration boundary 2019 will be used.\n\ngeoJAR <- st_read(dsn = \"data/geospatial/\",\n                  layer= \"BATAS_DESA_DESEMBER_2019_DUKCAPIL_DKI_JAKARTA\")\n\nReading layer `BATAS_DESA_DESEMBER_2019_DUKCAPIL_DKI_JAKARTA' from data source \n  `/Users/pengyouyun/youyunpeng/personalprojects/Geospatial/Take-home_Ex02/data/geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 269 features and 161 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 106.3831 ymin: -6.370815 xmax: 106.9728 ymax: -5.184322\nGeodetic CRS:  WGS 84\n\n\nFrom the output message, we learn that:\n\nGeometry type is multipolygon\n269 features, 161 fields\nAssigned CRS is WGS 84, the ‘World Geodetic System 1984’.\n\n\n\n2.1.1.2 Checking CRS\n\nst_crs(geoJAR)\n\nCoordinate Reference System:\n  User input: WGS 84 \n  wkt:\nGEOGCRS[\"WGS 84\",\n    DATUM[\"World Geodetic System 1984\",\n        ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n            LENGTHUNIT[\"metre\",1]]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"latitude\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"longitude\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    ID[\"EPSG\",4326]]\n\n\nThe assigned coordinate system is WGS 84, which is not appropriate as this is an Indonesia-specific geospatial dataset. It should be using the national CRS of indonesia, GDN95, with EPSG code 23845.\n\n# transforms the CRS to DGN95, ESPG code 23845\ngeoJAR <- st_transform(geoJAR, 23845)\n\nst_crs(geoJAR)\n\nCoordinate Reference System:\n  User input: EPSG:23845 \n  wkt:\nPROJCRS[\"DGN95 / Indonesia TM-3 zone 54.1\",\n    BASEGEOGCRS[\"DGN95\",\n        DATUM[\"Datum Geodesi Nasional 1995\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4755]],\n    CONVERSION[\"Indonesia TM-3 zone 54.1\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",139.5,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",0.9999,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",200000,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",1500000,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"easting (X)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"northing (Y)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Cadastre.\"],\n        AREA[\"Indonesia - onshore east of 138°E.\"],\n        BBOX[-9.19,138,-1.49,141.01]],\n    ID[\"EPSG\",23845]]\n\nqtm(geoJAR)\n\n\n\n\n\n\n2.1.1.3 Filling up White Spaces\nWe realise there are 2 missing areas that are listed in the geospatial data where its fields in KAB_KOTA and KODE_DESA are missing. We fill it in with dummy variables below, to ensure that it is not removed by a blanket exclude to all NA values.\n\ngeoJAR$KAB_KOTA[243]<-\"JAKARTA UTARA\"\ngeoJAR$KAB_KOTA[244]<-\"JAKARTA UTARA\"\n\ngeoJAR$KODE_DESA[243]<-\"3188888801\"\ngeoJAR$KODE_DESA[244]<-\"3188888802\"\n\n\n\n2.1.1.4 Removal of outer islands\nAs per the assignment requirements, the outer islands are not relevant to our analysis.\n\n# filtering out the island\ngeoJAR <- filter(geoJAR, KAB_KOTA != \"KEPULAUAN SERIBU\") # removing rows with the variable KEPULAUAN SERIBU, which translates to thousand islands\ngeoJAR\n\nSimple feature collection with 263 features and 161 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -3644275 ymin: 663887.8 xmax: -3606237 ymax: 701380.1\nProjected CRS: DGN95 / Indonesia TM-3 zone 54.1\nFirst 10 features:\n   OBJECT_ID  KODE_DESA               DESA   KODE    PROVINSI      KAB_KOTA\n1      25477 3173031006          KEAGUNGAN 317303 DKI JAKARTA JAKARTA BARAT\n2      25478 3173031007             GLODOK 317303 DKI JAKARTA JAKARTA BARAT\n3      25397 3171031003      HARAPAN MULIA 317103 DKI JAKARTA JAKARTA PUSAT\n4      25400 3171031006       CEMPAKA BARU 317103 DKI JAKARTA JAKARTA PUSAT\n5      25390 3171021001         PASAR BARU 317102 DKI JAKARTA JAKARTA PUSAT\n6      25391 3171021002       KARANG ANYAR 317102 DKI JAKARTA JAKARTA PUSAT\n7      25394 3171021005 MANGGA DUA SELATAN 317102 DKI JAKARTA JAKARTA PUSAT\n8      25386 3171011003       PETOJO UTARA 317101 DKI JAKARTA JAKARTA PUSAT\n9      25403 3171041001              SENEN 317104 DKI JAKARTA JAKARTA PUSAT\n10     25408 3171041006             BUNGUR 317104 DKI JAKARTA JAKARTA PUSAT\n     KECAMATAN         DESA_KELUR JUMLAH_PEN JUMLAH_KK LUAS_WILAY KEPADATAN\n1   TAMAN SARI          KEAGUNGAN      21609      7255       0.36     60504\n2   TAMAN SARI             GLODOK       9069      3273       0.37     24527\n3    KEMAYORAN      HARAPAN MULIA      29085      9217       0.53     54465\n4    KEMAYORAN       CEMPAKA BARU      41913     13766       0.97     42993\n5  SAWAH BESAR         PASAR BARU      15793      5599       1.76      8971\n6  SAWAH BESAR       KARANG ANYAR      33383     11276       0.47     71628\n7  SAWAH BESAR MANGGA DUA SELATAN      35906     12817       1.31     27463\n8       GAMBIR       PETOJO UTARA      21828      7328       1.14     19144\n9        SENEN              SENEN       8643      3049       0.82     10594\n10       SENEN             BUNGUR      23001      7944       0.67     34418\n   PERPINDAHA JUMLAH_MEN PERUBAHAN WAJIB_KTP SILAM KRISTEN KHATOLIK HINDU BUDHA\n1         102         68     20464     16027 15735    2042      927    15  2888\n2          25         52      8724      7375  1842    2041     1460     9  3716\n3         131        104     27497     20926 26328    1710      531    42   469\n4         170        151     38323     30264 36813    3392     1082   127   495\n5          58         36     15006     12559  7401    3696     1602   622  2462\n6         113         60     31014     24784 23057    4058     2100    25  4134\n7         178         92     33021     26730 23424    5130     2575    27  4740\n8          87         83     19275     16478 15355    3061     1838     9  1559\n9          56         21      8306      6298  5450    1991      705   115   381\n10        128         70     21652     16987 17431    3099     1258    47  1143\n   KONGHUCU KEPERCAYAA  PRIA WANITA BELUM_KAWI KAWIN CERAI_HIDU CERAI_MATI   U0\n1         2          0 11049  10560      10193 10652        255        509 1572\n2         1          0  4404   4665       4240  4364        136        329  438\n3         5          0 14696  14389      14022 13450        430       1183 2232\n4         1          3 21063  20850      20336 19487        523       1567 3092\n5        10          0  7833   7960       7578  7321        217        677  802\n6         9          0 16887  16496      15860 15945        381       1197 2220\n7        10          0 18338  17568      17239 17198        476        993 2399\n8         4          2 10955  10873      10551 10070        305        902 1406\n9         1          0  4446   4197       4360  3915        101        267  585\n10        1         22 11679  11322      11010 11231        206        554 1679\n     U5  U10  U15  U20  U25  U30  U35  U40  U45  U50  U55  U60 U65 U70 U75\n1  1751 1703 1493 1542 1665 1819 1932 1828 1600 1408 1146  836 587 312 415\n2   545  524  521  543  628  691  782  675  607  619  602  614 555 311 414\n3  2515 2461 2318 2113 2170 2363 2595 2371 2250 1779 1379 1054 654 411 420\n4  3657 3501 3486 3098 3024 3188 3662 3507 3391 2696 1909 1397 970 631 704\n5   995 1016 1106 1081 1002 1236 1422 1200 1163 1099  979  880 747 488 577\n6  2687 2653 2549 2313 2446 2735 3034 2689 2470 2129 1843 1386 958 554 717\n7  2953 2754 2666 2515 2725 3122 3385 3037 2597 2282 1930 1394 932 573 642\n8  1625 1625 1718 1612 1612 1707 1806 1746 1672 1427 1258  968 706 412 528\n9   758  714  672  583  670  797  796  740  577  495  437  331 234 129 125\n10 1794 1797 1789 1667 1773 1957 1952 1785 1667 1489 1219  941 647 386 459\n   TIDAK_BELU BELUM_TAMA TAMAT_SD SLTP  SLTA DIPLOMA_I DIPLOMA_II DIPLOMA_IV\n1        3426       1964     2265 3660  8463        81        428       1244\n2        1200        481      655 1414  3734        23        273       1241\n3        4935       2610     2346 3167 12172        84       1121       2477\n4        7328       3763     2950 5138 16320       179       1718       4181\n5        2121       1278     1169 2236  5993        43        573       2199\n6        5075       3241     4424 5858 12448        85        604       1582\n7        6089       3184     3620 6159 14080        83        740       1850\n8        3290       1951     1660 3008  8743        63        734       2189\n9        1401        768      877 1417  2936        27        280        804\n10       3506       2065     1609 2918 10155        79        708       1832\n   STRATA_II STRATA_III BELUM_TIDA APARATUR_P TENAGA_PEN WIRASWASTA PERTANIAN\n1         74          4       3927         81         70       8974         1\n2         46          2       1388         10         43       3832         0\n3        166          7       5335        513        288      10662         1\n4        315         21       8105        931        402      14925         3\n5        168         13       2676        156         81       6145         1\n6         63          3       5985        132        123      12968         2\n7         92          9       6820         79         73      14714         5\n8        174         16       3809        145        109       8549         1\n9        125          8       1574        369         30       3175         0\n10       122          7       3948        609        137       8284         0\n   NELAYAN AGAMA_DAN PELAJAR_MA TENAGA_KES PENSIUNAN LAINNYA    GENERATED\n1        0         6       4018         28        57    4447 30 Juni 2019\n2        0         6       1701         29        50    2010 30 Juni 2019\n3        2         5       6214         80       276    5709 30 Juni 2019\n4        0        40       9068        142       498    7799 30 Juni 2019\n5        1        49       3135         60        59    3430 30 Juni 2019\n6        1        10       6823         48        56    7235 30 Juni 2019\n7        2        11       6866         55        75    7206 30 Juni 2019\n8        0        54       4731         68        97    4265 30 Juni 2019\n9        0        15       1779         89        53    1559 30 Juni 2019\n10       0        16       5063         93       146    4705 30 Juni 2019\n   KODE_DES_1 BELUM_ MENGUR_ PELAJAR_ PENSIUNA_1 PEGAWAI_ TENTARA KEPOLISIAN\n1  3173031006   3099    4447     3254         80       48       4         10\n2  3173031007   1032    2026     1506         65        5       0          1\n3  3171031003   4830    5692     6429        322      366      41         16\n4  3171031006   7355    7692     8957        603      612      57         42\n5  3171021001   2390    3500     3185         70       65      74          2\n6  3171021002   5330    7306     6993         75       73      20         17\n7  3171021005   5605    7042     6858         97       48      12          7\n8  3171011003   3365    4357     4719        132       89      11          9\n9  3171041001   1553    1627     1701         67       91      90        165\n10 3171041006   3924    4731     4885        165      174     340         15\n   PERDAG_ PETANI PETERN_ NELAYAN_1 INDUSTR_ KONSTR_ TRANSP_ KARYAW_ KARYAW1\n1       31      0       0         1        7       3       2    6735       9\n2        5      0       0         0        3       0       0    3034       2\n3        1      1       0         1        4       2       7    7347      74\n4        3      2       0         0        3       6       4   10185     231\n5        2      1       0         0        0       1       0    4319      16\n6        3      1       0         0        1       1       0    9405      13\n7        1      1       0         1        7       5       3   10844      10\n8        0      2       0         0        0       0       0    6909      24\n9        1      0       0         0        2       2       0    1959      17\n10       2      0       0         0        2       5       0    5661      29\n   KARYAW1_1 KARYAW1_12 BURUH BURUH_ BURUH1 BURUH1_1 PEMBANT_ TUKANG TUKANG_1\n1          0         23   515      1      0        0        1      0        1\n2          0          4   155      0      0        0        1      0        0\n3          5         25   971      0      1        0        4      0        0\n4         15         35   636      0      0        0        1      0        0\n5          0         16   265      1      0        0        7      0        0\n6          0          6  1085      0      0        0        5      0        0\n7          1          9   652      1      1        0        1      1        0\n8          0         11   357      0      0        0        6      0        0\n9          2         11   226      0      0        0        1      0        0\n10         4         15   542      0      0        0       10      0        0\n   TUKANG_12 TUKANG__13 TUKANG__14 TUKANG__15 TUKANG__16 TUKANG__17 PENATA\n1          0          1          0          1          7          1      0\n2          0          1          0          0          4          0      0\n3          0          0          0          0         10          0      0\n4          0          1          0          1         14          0      0\n5          0          0          0          0          2          0      1\n6          0          0          0          0          7          0      0\n7          0          1          0          1          8          1      0\n8          0          0          0          0          8          0      0\n9          0          0          0          0          1          0      1\n10         0          1          0          0          0          0      0\n   PENATA_ PENATA1_1 MEKANIK SENIMAN_ TABIB PARAJI_ PERANCA_ PENTER_ IMAM_M\n1        0         0      11        4     1       0        0       1      0\n2        0         0       1        0     0       0        0       0      0\n3        0         0      10       12     0       0        0       0      0\n4        0         1       8       28     0       0        0       0      0\n5        0         0       4        2     1       0        0       1      0\n6        0         0       7        3     0       0        0       0      0\n7        0         1       8        4     1       0        0       0      0\n8        0         2       9        9     0       0        1       0      0\n9        0         0       0        6     0       0        0       0      0\n10       0         1      15        7     0       0        2       0      0\n   PENDETA PASTOR WARTAWAN USTADZ JURU_M PROMOT ANGGOTA_ ANGGOTA1 ANGGOTA1_1\n1        2      0        7      6      0      0        0        0          0\n2        4      1        1      1      0      0        1        0          0\n3        5      0       16      1      0      0        0        0          0\n4       33      1       27      5      0      0        0        0          0\n5       20      8        4      0      0      0        0        0          0\n6       10      0        8      0      0      0        1        0          0\n7        8      0        6      1      0      0        0        0          0\n8       30     23        9      0      0      0        1        0          0\n9       14      0        5      0      0      0        0        0          0\n10      14      0        9      1      0      0        0        0          0\n   PRESIDEN WAKIL_PRES ANGGOTA1_2 ANGGOTA1_3 DUTA_B GUBERNUR WAKIL_GUBE BUPATI\n1         0          0          0          0      0        0          0      0\n2         0          0          0          0      0        0          0      0\n3         0          0          0          0      0        0          0      0\n4         0          0          0          0      0        0          0      0\n5         0          0          0          0      0        0          0      0\n6         0          0          0          0      0        0          0      0\n7         0          0          0          0      0        0          0      0\n8         0          0          0          0      0        0          0      0\n9         0          0          0          0      0        0          0      0\n10        0          0          0          0      0        0          0      0\n   WAKIL_BUPA WALIKOTA WAKIL_WALI ANGGOTA1_4 ANGGOTA1_5 DOSEN GURU PILOT\n1           0        0          0          0          0     3   72     1\n2           0        0          0          0          0     2   40     0\n3           0        0          0          0          0    23  272     2\n4           0        0          0          0          0    36  378     3\n5           0        0          0          0          0    11   69     0\n6           0        0          0          0          0     3  126     0\n7           0        0          0          0          0     5   71     0\n8           0        0          0          0          0    14   97     0\n9           0        0          0          0          0     6   23     0\n10          0        0          0          0          0    28  106     0\n   PENGACARA_ NOTARIS ARSITEK AKUNTA_ KONSUL_ DOKTER BIDAN PERAWAT APOTEK_\n1           4       0       1       1       1     16     3       7       0\n2           1       0       0       0       0     32     1       0       0\n3           8       3       2       0       2     35     9      25       2\n4          22       5       3       0      11     68    18      44       3\n5           5       4       2       0       4     63     1       3       0\n6           5       0       0       0       0     27     3      12       1\n7           4       0       0       0       0     32     3      20       2\n8           4       5       4       2       6     63     3       7       1\n9           3       0       1       0       2     48    10      26       2\n10         12       5       2       1       3     60    10      16       3\n   PSIKIATER PENYIA_ PENYIA1 PELAUT PENELITI SOPIR PIALAN PARANORMAL PEDAGA_\n1          0       0       0      0        0    65      0          0     379\n2          0       0       0      0        1     3      0          0     126\n3          1       0       0      6        0    94      0          0     321\n4          0       0       0     16        0   123      0          0     562\n5          0       0       0      0        1    61      0          0     412\n6          0       0       0      2        0    76      0          0     202\n7          0       0       0      4        0    79      0          1     225\n8          0       0       0      2        0    63      0          0     271\n9          1       0       0      4        0    44      0          0     212\n10         0       0       0      2        0   101      0          0     331\n   PERANG_ KEPALA_ BIARAW_ WIRASWAST_ LAINNYA_12 LUAS_DESA KODE_DES_3\n1        0       0       0       1370         94     25476 3173031006\n2        0       0       1        611         57     25477 3173031007\n3        0       0       0       1723         82     25396 3171031003\n4        0       0       0       3099        122     25399 3171031006\n5        0       0      22       1128         41     25389 3171021001\n6        0       0       3       2321         89     25390 3171021002\n7        0       0       0       2677        158     25393 3171021005\n8        0       0       2       1018         37     25385 3171011003\n9        0       0       1        871         15     25402 3171041001\n10       0       0       0       1749         94     25407 3171041006\n           DESA_KEL_1 KODE_12                       geometry\n1           KEAGUNGAN  317303 MULTIPOLYGON (((-3626874 69...\n2              GLODOK  317303 MULTIPOLYGON (((-3627130 69...\n3       HARAPAN MULIA  317103 MULTIPOLYGON (((-3621251 68...\n4        CEMPAKA BARU  317103 MULTIPOLYGON (((-3620608 69...\n5          PASAR BARU  317102 MULTIPOLYGON (((-3624097 69...\n6        KARANG ANYAR  317102 MULTIPOLYGON (((-3624785 69...\n7  MANGGA DUA SELATAN  317102 MULTIPOLYGON (((-3624752 69...\n8        PETOJO UTARA  317101 MULTIPOLYGON (((-3626121 69...\n9               SENEN  317104 MULTIPOLYGON (((-3623189 69...\n10             BUNGUR  317104 MULTIPOLYGON (((-3622451 69...\n\nunique(geoJAR$KAB_KOTA) # Now we are only left with 5 unique vairbales in KAB_KOTA\n\n[1] \"JAKARTA BARAT\"   \"JAKARTA PUSAT\"   \"JAKARTA UTARA\"   \"JAKARTA TIMUR\"  \n[5] \"JAKARTA SELATAN\"\n\nqtm(geoJAR)\n\n\n\n\n\n\n2.1.1.5 Retaining the first 9 fields of geoJAR\nAs per the assignment requirements, we only need to retain the first 9 fields in the geoJAR table.\n\n# filters out other fields by accepting only the first 9 fields\ngeoJAR <- geoJAR[, 0:9]\n\n\n\n2.1.1.6 Translating column names\nFor ease of comprehension, we translate the column names into english.\n\ngeoJAR <- geoJAR %>% \n  dplyr::select(2,7) |> \n  dplyr::rename(\n    subdistrict=KECAMATAN, \n    village_code=KODE_DESA\n    )\n\n\n\n\n2.1.2 Aspatial Data\nFor the purpose of this assignment, data from Riwayat File Vaksinasi DKI Jakarta will be used. Daily vaccination data are provides. We are only required to download either the first day of the month or last day of the month of the study period.\nAs per the assignment criteria, we downloaded vaccination data of the first day of the month from july 2021 to june 2022. For ease of reading the data into one dataframe, we rename the excel sheets to be in the following format “month year”. For instance, “Data Vaksinasi Bebasis Keluarhan 1 Juli 2021).xlsx” will be renamed to “July 2021”.\nNow we are ready to read the excel data!\n\ntmap_mode(\"view\")\nqtm(geoJAR) #two polygons have different name\n\n\n\n\n\n\n\n2.1.2.1 Reading the Aspatial Data\nIn the code chunk below, we read all the excel files in our data folder to a dataframe with the date as the title.\n\n# Load the readxl library\nlibrary(readxl)\n\n# Set the working directory to the folder containing the Excel files\nsetwd(\"data/aspatial/\") \n\n# Get a list of all Excel files in the directory\nfiles <- list.files(pattern = \".xlsx\")\n\n# Loop through the files and read each one into a data frame\nfor (file in files) {\n  assign(gsub(\".xlsx\", \"\", file), read_excel(file))\n}\n\n\n\n2.1.2.2 Creating a function to modify dataframes\nAt the end of the day, we want to create a large dataframe of all the vaccination data, categorised by the date. As part of data wrangling of our dataframes, we want to achieve the following\n\ncreate a date column for each dataframe\nselecting columns relating to spatial information: (`KODE KELURAHAN`, `WILAYAH KOTA`, KECAMATAN, KELURAHAN)\nselecting columns relating to vaccination information: (`BELUM VAKSIN`, `TOTAL VAKSIN\\r\\nDIBERIKAN`)\ntranslating columns to english\ncreating a new column for total population and vaccination rate\n\nWe do so by creating a function that can be used to wrangle all of the date dataframes we have in our environment.\n\nmutate_df<-function(data){\n df_name <- deparse(substitute(data))\n modified<-data |> \n   select( `KODE KELURAHAN`, \n          `WILAYAH KOTA`, \n          KECAMATAN, \n          KELURAHAN, \n          `BELUM VAKSIN`, \n          SASARAN) |> \n   rename(village_code=`KODE KELURAHAN`, \n          city_region =`WILAYAH KOTA`, \n          subdistrict=`KECAMATAN`, \n          district=`KELURAHAN`, \n          target_vaccination= `SASARAN`, \n          not_vaccinated=`BELUM VAKSIN`) |> \n  mutate(total_population=target_vaccination) |> \n  mutate(vaccination_rate=(target_vaccination-not_vaccinated)/total_population)\n return(modified)\n}\n\nWe can now run all the dataframes through this function.\n\nlist_month<- list(`July 2021`, `August 2021`, `September 2021`, `October 2021`, `November 2021`, `December 2021`, `January 2022`, `February 2022`, `March 2022`, `April 2022`, `May 2022`, `June 2022`)\n\ndate <- c(\"2021-7-1\", \"2021-8-1\", \"2021-9-1\", \"2021-10-1\", \"2021-11-1\", \"2021-12-1\", \"2022-1-1\", \"2022-2-1\", \"2022-3-1\", \"2022-4-1\", \"2022-5-1\", \"2022-6-1\")\n\nlists<-list()\n\nfor (i in c(1:12)){\n  lists[[i]]<-mutate_df(list_month[[i]]) |> \n    mutate(date=as.Date(date[i]),\n           .before=1)\n    \n    \n}\n\n\n\n2.1.2.3 Joining vaccination data\n\ndf<-Reduce(rbind, lists)\nglimpse(df)\n\nRows: 3,216\nColumns: 9\n$ date               <date> 2021-07-01, 2021-07-01, 2021-07-01, 2021-07-01, 20…\n$ village_code       <chr> NA, \"3172051003\", \"3173041007\", \"3175041005\", \"3175…\n$ city_region        <chr> NA, \"JAKARTA UTARA\", \"JAKARTA BARAT\", \"JAKARTA TIMU…\n$ subdistrict        <chr> NA, \"PADEMANGAN\", \"TAMBORA\", \"KRAMAT JATI\", \"JATINE…\n$ district           <chr> \"TOTAL\", \"ANCOL\", \"ANGKE\", \"BALE KAMBANG\", \"BALI ME…\n$ not_vaccinated     <dbl> 5041111, 13272, 16477, 18849, 5743, 15407, 12503, 1…\n$ target_vaccination <dbl> 7739060, 20393, 25785, 25158, 8683, 22768, 18930, 2…\n$ total_population   <dbl> 7739060, 20393, 25785, 25158, 8683, 22768, 18930, 2…\n$ vaccination_rate   <dbl> 0.3486146, 0.3491884, 0.3609851, 0.2507751, 0.33859…\n\n\n\n\n2.1.2.4 Preparing data for joining with geospatial data\nIn performing a join with the data, we can join via the village code, as both dataframes have a village code assigned to each row. Upon checking the data, we see that the unique variables in city_region field is different from the geospatial data.\n\nunique(df$city_region)\n\n[1] NA                   \"JAKARTA UTARA\"      \"JAKARTA BARAT\"     \n[4] \"JAKARTA TIMUR\"      \"JAKARTA SELATAN\"    \"JAKARTA PUSAT\"     \n[7] \"KAB.ADM.KEP.SERIBU\"\n\n\nWe see that there is an extra variable “KAB.ADM.KEP.SERIBU” that is present in aspatial data and not the geospatial data. We remove it using a filter function.\n\ndf <- df|> \n   filter(city_region != \"KAB.ADM.KEP.SERIBU\") \nunique(df$city_region) \n\n[1] \"JAKARTA UTARA\"   \"JAKARTA BARAT\"   \"JAKARTA TIMUR\"   \"JAKARTA SELATAN\"\n[5] \"JAKARTA PUSAT\"  \n\n\nNext, we need to add in the village code. Earlier, we found 2 polygons with missing values\n\nsetdiff(geoJAR$village_code, df$village_code)\n\n[1] \"3188888801\" \"3188888802\"\n\n\nWe see that in the aspatial data, no information was collected for these 2 sub districts. We add in these two subdistricts so that both dataframes will match when it is joint.\n\naspatial_data <- rbind(df, c(\"2021-07-01\", 3188888801,NA),\n                       c(\"2021-08-01\", 3188888801,NA),\n                       c(\"2021-09-01\", 3188888801,NA),\n                       c(\"2021-10-01\", 3188888801,NA),\n                       c(\"2021-11-01\", 3188888801,NA),\n                       c(\"2021-12-01\", 3188888801,NA),\n                       c(\"2022-01-01\", 3188888801,NA),\n                       c(\"2022-02-01\", 3188888801,NA),\n                       c(\"2022-03-01\", 3188888801,NA),\n                       c(\"2022-04-01\", 3188888801,NA),\n                       c(\"2022-05-01\", 3188888801,NA),\n                       c(\"2022-06-01\", 3188888801,NA),\n                       c(\"2021-07-01\", 3188888802,NA),\n                       c(\"2021-08-01\", 3188888802,NA),\n                       c(\"2021-09-01\", 3188888802,NA),\n                       c(\"2021-10-01\", 3188888802,NA),\n                       c(\"2021-11-01\", 3188888802,NA),\n                       c(\"2021-12-01\", 3188888802,NA),\n                       c(\"2022-01-01\", 3188888802,NA),\n                       c(\"2022-02-01\", 3188888802,NA),\n                       c(\"2022-03-01\", 3188888802,NA),\n                       c(\"2022-04-01\", 3188888802,NA),\n                       c(\"2022-05-01\", 3188888802,NA),\n                       c(\"2022-06-01\", 3188888802,NA))\n\n\n\n2.1.2.5 Ensuring that the subdistrict names match between Geospatial and Aspatial\nNext we do a quick check if subdistricts in Geospatial and Aspatial dataframes match with each other.\n\nsetdiff(geoJAR$subdistrict, df$subdistrict)\n\n[1] \"KALIDERES\"  \"PAL MERAH\"  \"SETIABUDI\"  \"PULOGADUNG\" \"KRAMATJATI\"\n[6] NA          \n\n\nWe see that there are a few differences in names. We rename the geospatial dataframe, in the column for subdistricts in the code chunk below.\n\nn1 <- which(geoJAR$subdistrict == \"KRAMATJATI\") \n\nn2 <- which(geoJAR$subdistrict == \"PAL MERAH\") \n\nn3 <- which(geoJAR$subdistrict == \"PULOGADUNG\") \n\nn4 <- which(geoJAR$subdistrict == \"SETIABUDI\") \n\nn5 <- which(geoJAR$subdistrict == \"KALIDERES\") \n\nfor (i in n1) {   geoJAR$subdistrict[i] <- \"KRAMAT JATI\" } \n\nfor (i in n2) {   geoJAR$subdistrict[i] <- \"PALMERAH\" } \n\nfor (i in n3) {   geoJAR$subdistrict[i] <- \"PULO GADUNG\" } \n\nfor (i in n4) {   geoJAR$subdistrict[i] <- \"SETIA BUDI\" } \n\nfor (i in n5) {   geoJAR$subdistrict[i] <- \"KALI DERES\"}\n\nsetdiff(geoJAR$subdistrict, df$subdistrict)\n\n[1] NA\n\n\n\n\n2.1.2.6 Selecting columns\nTo keep our data organised, we retain only specific columns of our data.\n\ndf |> \n  select(date, village_code, subdistrict, vaccination_rate)\n\n# A tibble: 3,132 × 4\n   date       village_code subdistrict      vaccination_rate\n   <date>     <chr>        <chr>                       <dbl>\n 1 2021-07-01 3172051003   PADEMANGAN                  0.349\n 2 2021-07-01 3173041007   TAMBORA                     0.361\n 3 2021-07-01 3175041005   KRAMAT JATI                 0.251\n 4 2021-07-01 3175031003   JATINEGARA                  0.339\n 5 2021-07-01 3175101006   CIPAYUNG                    0.323\n 6 2021-07-01 3174031002   MAMPANG PRAPATAN            0.340\n 7 2021-07-01 3175051002   PASAR REBO                  0.444\n 8 2021-07-01 3175041004   KRAMAT JATI                 0.267\n 9 2021-07-01 3171071002   TANAH ABANG                 0.395\n10 2021-07-01 3175031002   JATINEGARA                  0.276\n# … with 3,122 more rows\n\n\nNow we are ready to join the data together!\n\n\n\n2.1.3 Combine data\nTo ensure that the end result is a sf dataframe, we use left_join and our sf object is placed at the left. We use st_as_sf to ensure that the output is a sf object.\n\ncombined_df <- left_join(df, geoJAR, by = c(\"village_code\", \"subdistrict\")) |> \n  st_as_sf()\n\nclass(combined_df)\n\n[1] \"sf\"         \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n\nFor ease of plotting, we also change the class of date to a factor format.\n\ncombined_df<- combined_df |>\n  mutate(date=as.factor(date))\n\nNow we are ready for Choropleth mapping!"
  },
  {
    "objectID": "Geospatial/Take-home_Ex02/Take-home_Ex02.html#creating-a-tmap-function",
    "href": "Geospatial/Take-home_Ex02/Take-home_Ex02.html#creating-a-tmap-function",
    "title": "Take-home Exercise 2",
    "section": "3.1 Creating a tmap function",
    "text": "3.1 Creating a tmap function\nGiven that we have 12 maps to create–corresponding to the 12 months required to plot, we create a tmap plotting function called plot that holds the tmap function that we want to plot.\nThe function has 2 inputs, the dataframe that holds the vaccination data for all the months, as well as the chosen date, which would be used for filtering for vaccination rates. The output would be a tmap graph.\n\nplot<-function(dataframe, chosen_date){\n  tmap_mode(\"plot\")\ntm_shape(filter(combined_df, date %in% chosen_date)) +\n  tm_fill(\"vaccination_rate\",\n          n= 6,\n          style = \"equal\",\n          palette=\"Blues\")+\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title= chosen_date, \n            main.title.position=\"center\",\n            main.title.size=1.2,\n            legend.height=0.45,\n            legend.width = 0.35,\n            frame=TRUE)+\n    tm_scale_bar()+\n    tm_grid(alpha=0.2)\n}\nplot(combined_df, \"2021-07-01\")"
  },
  {
    "objectID": "Geospatial/Take-home_Ex02/Take-home_Ex02.html#plotting-maps",
    "href": "Geospatial/Take-home_Ex02/Take-home_Ex02.html#plotting-maps",
    "title": "Take-home Exercise 2",
    "section": "3.2 Plotting maps",
    "text": "3.2 Plotting maps\nUsing the function we have created, we run each month’s vaccination data into the function to create 12 maps.\n\ntmap_mode(\"plot\")\ntmap_arrange(plot(combined_df, \"2021-07-01\"),\n             plot(combined_df, \"2021-08-01\"),\n             plot(combined_df, \"2021-09-01\"),\n             plot(combined_df, \"2021-10-01\"),\n             plot(combined_df, \"2021-11-01\"),\n             plot(combined_df, \"2021-12-01\"))\n\n\n\n\n\ntmap_mode(\"plot\")\ntmap_arrange(plot(combined_df, \"2022-01-01\"),\n             plot(combined_df, \"2022-02-01\"),\n             plot(combined_df, \"2022-03-01\"),\n             plot(combined_df, \"2022-04-01\"),\n             plot(combined_df, \"2022-05-01\"),\n             plot(combined_df, \"2022-06-01\"))"
  },
  {
    "objectID": "Geospatial/Take-home_Ex02/Take-home_Ex02.html#shiny-app-for-interactive-chloropleth-map",
    "href": "Geospatial/Take-home_Ex02/Take-home_Ex02.html#shiny-app-for-interactive-chloropleth-map",
    "title": "Take-home Exercise 2",
    "section": "3.3 Shiny App for Interactive Chloropleth map",
    "text": "3.3 Shiny App for Interactive Chloropleth map\nWe also included a shiny app where the user can input the date desired, and the output would be the tmap graph corresponding to the date selected. However, because of some technical difficulties, i am unable to display the web app.\n\nlibrary(shiny)\nlibrary(tmap)\n\ndate <- unique(combined_df$date)\n\n# Define the UI\nui <- fluidPage(\n  selectInput(\n    \"date\",\n    label=\"pick a month\",\n    choices=date,\n    selected=\"2021-07-01\",\n    multiple=FALSE\n  ),\n  # Create a tmap output element\n  tmapOutput(\"my_map\")\n)\n\n# Define the server\nserver <- function(input, output) {\n  # Render the tmap in the output element\n  output$my_map <- renderTmap({\n    df<- combined_df |> \n      filter(date %in% input$date)\n    # Create the tmap\n    tm_shape(df) +\n  tm_fill(\"vaccination_rate\",\n          style=\"quantile\",\n          palette=\"Blues\")\n  })\n}\n\n# Run the app\nshinyApp(ui, server)"
  },
  {
    "objectID": "Geospatial/Take-home_Ex02/Take-home_Ex02.html#spatial-patterns-revealed-by-the-choropleth-maps-not-more-than-200-words",
    "href": "Geospatial/Take-home_Ex02/Take-home_Ex02.html#spatial-patterns-revealed-by-the-choropleth-maps-not-more-than-200-words",
    "title": "Take-home Exercise 2",
    "section": "3.4 Spatial Patterns revealed by the choropleth maps (not more than 200 words)",
    "text": "3.4 Spatial Patterns revealed by the choropleth maps (not more than 200 words)\nFrom the choropleth map, we can see that vaccination rates has been increasing across DKI jarkarta over time. This can be seen from the change in bins, where the lower limit has been increasing over time. Concurrently we see an intensification of blue within the tmaps over time.\nFocusing on the south region of the map, we see that it initially started off having a much lower vaccination rate compared to the rest of the area, but in the later months, it caught up to have a vaccination rate similar to the rest of the area."
  },
  {
    "objectID": "Geospatial/Take-home_Ex02/Take-home_Ex02.html#create-a-tmap-function",
    "href": "Geospatial/Take-home_Ex02/Take-home_Ex02.html#create-a-tmap-function",
    "title": "Take-home Exercise 2",
    "section": "4.1 Create a tmap function",
    "text": "4.1 Create a tmap function\nTo graph out the Gi maps of the monthly vaccination rate, we create a tmap function which is able to take Gi values from the previous lisa_LMI list and plot it into a graph.\nNotice that we are required to only display the significant Gi* values where its p-value < 0.5. In the previous step to generate gistar values, we used simulations. Therefore, we will be using the “p_sim” column in filtering p-value for significance.\n\ntmap_function<-  function(x){\n plot <- \n    tm_shape(x) +\n    tm_polygons() +\n    tm_shape(x %>% filter(p_sim <0.05)) +\n    tm_fill(\"gi_star\",\n            style=\"equal\",\n            n=5) +\n    tm_borders(alpha = 0.5) +\n    tm_layout(main.title = paste(\"Significant Local Gi\", \"(\",x$date[1],\")\"),\n              main.title.size = 0.8)\n  return(plot)\n}\n\ntmap_function(lisa_LMI[[1]]) #Testing the tmap function out\n\n\n\n\nThe graph in the test looks alright! We will now go ahead with running all the dataframes within “lisa_LMI” through the tmap function!"
  },
  {
    "objectID": "Geospatial/Take-home_Ex02/Take-home_Ex02.html#creating-tmaps",
    "href": "Geospatial/Take-home_Ex02/Take-home_Ex02.html#creating-tmaps",
    "title": "Take-home Exercise 2",
    "section": "4.2 Creating tmaps",
    "text": "4.2 Creating tmaps\n\ntmap_mode(\"plot\")\ntmap_arrange(tmap_function(lisa_LMI[[1]]),\n             tmap_function(lisa_LMI[[2]]),\n             tmap_function(lisa_LMI[[3]]),\n             tmap_function(lisa_LMI[[4]]),\n             tmap_function(lisa_LMI[[5]]),\n             tmap_function(lisa_LMI[[6]]))\n\n\n\n\n\ntmap_arrange(tmap_function(lisa_LMI[[7]]),\n             tmap_function(lisa_LMI[[8]]),\n             tmap_function(lisa_LMI[[9]]),\n             tmap_function(lisa_LMI[[10]]),\n             tmap_function(lisa_LMI[[11]]),\n             tmap_function(lisa_LMI[[12]]))"
  },
  {
    "objectID": "Geospatial/Take-home_Ex02/Take-home_Ex02.html#statistical-conclusions-not-more-than-250-words",
    "href": "Geospatial/Take-home_Ex02/Take-home_Ex02.html#statistical-conclusions-not-more-than-250-words",
    "title": "Take-home Exercise 2",
    "section": "4.3 Statistical conclusions (not more than 250 words)",
    "text": "4.3 Statistical conclusions (not more than 250 words)\nFrom this website, we understand that a Gi* value that has a significant and positive z-score, it is a hot spot, representing clustering of high values, and the converse is true for cold spots (significant negative z-score represent clustering of low values).\nOur tmap graph isolates observations that are significant. Importantly, areas which are colored “red” are significant cold spots, while areas colored “green” are significant hot spots.\nHot-spot analysis: We notice that the spatial distribution of the hotspots eventually spread to the north regions of the area, as corroborated from our earlier Tmap findings.\nCold-spot analysis: We notice that the cold spots within the area are mostly randomly distributed, besides a small area within the middle of the map. This could be a sign that there is a persistent and unsolved issue of a lack of healthcare resources in that area. We notice that the lower bound of the bandwidths used is also getting increasingly negative, which could signify greater clustering of cold spots over time."
  },
  {
    "objectID": "Geospatial/Take-home_Ex02/Take-home_Ex02.html#creating-a-time-series-cube",
    "href": "Geospatial/Take-home_Ex02/Take-home_Ex02.html#creating-a-time-series-cube",
    "title": "Take-home Exercise 2",
    "section": "5.1 Creating a time series cube",
    "text": "5.1 Creating a time series cube\nIn creating a time series cube, we need to ensure that there is a location identifier and a time identifier. Notice that in our combined dataframe (combined_df), our date column is of class “factor”. We mutate this to class “date” for the creation of a time series cube.\nFor the location column, we choose “village_code” given that it is the lowest level of identification for location, to prevent duplicates in disrupting our analysis.\n\nmutate(combined_df, date=lubridate::as_date(date))\n\nSimple feature collection with 3132 features and 9 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -3644275 ymin: 663887.8 xmax: -3606237 ymax: 701380.1\nProjected CRS: DGN95 / Indonesia TM-3 zone 54.1\n# A tibble: 3,132 × 10\n   date       village_…¹ city_…² subdi…³ distr…⁴ not_v…⁵ targe…⁶ total…⁷ vacci…⁸\n * <date>     <chr>      <chr>   <chr>   <chr>     <dbl>   <dbl>   <dbl>   <dbl>\n 1 2021-07-01 3172051003 JAKART… PADEMA… ANCOL     13272   20393   20393   0.349\n 2 2021-07-01 3173041007 JAKART… TAMBORA ANGKE     16477   25785   25785   0.361\n 3 2021-07-01 3175041005 JAKART… KRAMAT… BALE K…   18849   25158   25158   0.251\n 4 2021-07-01 3175031003 JAKART… JATINE… BALI M…    5743    8683    8683   0.339\n 5 2021-07-01 3175101006 JAKART… CIPAYU… BAMBU …   15407   22768   22768   0.323\n 6 2021-07-01 3174031002 JAKART… MAMPAN… BANGKA    12503   18930   18930   0.340\n 7 2021-07-01 3175051002 JAKART… PASAR … BARU      11268   20267   20267   0.444\n 8 2021-07-01 3175041004 JAKART… KRAMAT… BATU A…   30358   41389   41389   0.267\n 9 2021-07-01 3171071002 JAKART… TANAH … BENDUN…   11502   19008   19008   0.395\n10 2021-07-01 3175031002 JAKART… JATINE… BIDARA…   23395   32331   32331   0.276\n# … with 3,122 more rows, 1 more variable: geometry <MULTIPOLYGON [m]>, and\n#   abbreviated variable names ¹​village_code, ²​city_region, ³​subdistrict,\n#   ⁴​district, ⁵​not_vaccinated, ⁶​target_vaccination, ⁷​total_population,\n#   ⁸​vaccination_rate\n\nVac_st<- as_spacetime(combined_df,\n                                .loc_col = \"village_code\",\n                                .time_col = \"date\")\n\nSubsequently, we compute spatial contiguity weights using the space time cube, for Gi computation.\n\nVac_nb<-Vac_st |> \n  activate(\"geometry\") |> \n  mutate(\n    nb=include_self(st_contiguity(geometry)),\n    wt=st_inverse_distance(nb, \n                           geometry,\n                           scale=1,\n                           alpha=1),\n    .before=1\n  ) |> \n  set_nbs(\"nb\") |> \n  set_wts(\"wt\")"
  },
  {
    "objectID": "Geospatial/Take-home_Ex02/Take-home_Ex02.html#computing-gi",
    "href": "Geospatial/Take-home_Ex02/Take-home_Ex02.html#computing-gi",
    "title": "Take-home Exercise 2",
    "section": "5.2 Computing Gi*",
    "text": "5.2 Computing Gi*\nOnce again, we compute the Gi* value by using spatial contiguity weights generated earlier.\n\ngi_stars<-Vac_nb |> \n  group_by(date) |> \n  mutate(gi_star = local_gstar_perm(\n    vaccination_rate, nb, wt, nsim=99)) |> \n  tidyr::unnest(gi_star)\ngi_stars\n\n# A tibble: 3,132 × 19\n# Groups:   date [12]\n   date    villa…¹ city_…² subdi…³ distr…⁴ not_v…⁵ targe…⁶ total…⁷ vacci…⁸ nb   \n   <fct>   <chr>   <chr>   <chr>   <chr>     <dbl>   <dbl>   <dbl>   <dbl> <lis>\n 1 2021-0… 317205… JAKART… PADEMA… ANCOL     13272   20393   20393   0.349 <int>\n 2 2021-0… 317304… JAKART… TAMBORA ANGKE     16477   25785   25785   0.361 <int>\n 3 2021-0… 317504… JAKART… KRAMAT… BALE K…   18849   25158   25158   0.251 <int>\n 4 2021-0… 317503… JAKART… JATINE… BALI M…    5743    8683    8683   0.339 <int>\n 5 2021-0… 317510… JAKART… CIPAYU… BAMBU …   15407   22768   22768   0.323 <int>\n 6 2021-0… 317403… JAKART… MAMPAN… BANGKA    12503   18930   18930   0.340 <int>\n 7 2021-0… 317505… JAKART… PASAR … BARU      11268   20267   20267   0.444 <int>\n 8 2021-0… 317504… JAKART… KRAMAT… BATU A…   30358   41389   41389   0.267 <int>\n 9 2021-0… 317107… JAKART… TANAH … BENDUN…   11502   19008   19008   0.395 <int>\n10 2021-0… 317503… JAKART… JATINE… BIDARA…   23395   32331   32331   0.276 <int>\n# … with 3,122 more rows, 9 more variables: wt <list>, gi_star <dbl>,\n#   e_gi <dbl>, var_gi <dbl>, p_value <dbl>, p_sim <dbl>, p_folded_sim <dbl>,\n#   skewness <dbl>, kurtosis <dbl>, and abbreviated variable names\n#   ¹​village_code, ²​city_region, ³​subdistrict, ⁴​district, ⁵​not_vaccinated,\n#   ⁶​target_vaccination, ⁷​total_population, ⁸​vaccination_rate"
  },
  {
    "objectID": "Geospatial/Take-home_Ex02/Take-home_Ex02.html#mann-kendall-test",
    "href": "Geospatial/Take-home_Ex02/Take-home_Ex02.html#mann-kendall-test",
    "title": "Take-home Exercise 2",
    "section": "5.3 Mann Kendall Test",
    "text": "5.3 Mann Kendall Test\nThe Mann-Kendall test is a statistical test used in spatial analysis to detect trends and changes in spatial datasets. The Mann-Kendall test works by calculating the Kendall rank correlation coefficient between pairs of observations in the dataset. If the coefficient is positive, it indicates an increasing trend, while a negative coefficient indicates a decreasing trend. The significance of the trend is then determined using a statistical test.\nAs per the assignment requirements, we choose 3 unique village codes to focus our analysis on, as listed below:\n\nKoja (3172031001)\nTambora(3173041007)\nJohar Baru (3171081003)\n\nTo visualise the distribution of the Gi* value, we first create plots.\n\ncbg <- gi_stars %>% \n  ungroup() %>% \n  filter(village_code %in% c(3172031001, 3173041007, 3171081003)) |> \n  select(village_code, date, gi_star)\n\n\ncbg_1<-filter(cbg, village_code==3172031001)\n\nggplot(data=cbg_1, aes(x=lubridate::as_date(date), y=gi_star))+\n  geom_line()+\n  theme_light()\n\n\n\n\n\ncbg_2<-filter(cbg, village_code==3173041007)\n\nggplot(data=cbg_2, aes(x=lubridate::as_date(date), y=gi_star))+\n  geom_line()+\n  theme_light()\n\n\n\n\n\ncbg_3<-filter(cbg, village_code==3171081003)\n\nggplot(data=cbg_3, aes(x=lubridate::as_date(date), y=gi_star))+\n  geom_line()+\n  theme_light()\n\n\n\n\nWe can then run a Mann Kendall test for each of these categories defined by locations chosen. Importantly, the output of the MannKendall() test includes:\n\ntau: The Kendall’s tau statistic, which measures the strength and direction of the trend in the data. The tau value ranges from -1 to 1, with negative values indicating a decreasing trend, positive values indicating an increasing trend, and values close to zero indicating no trend.\nsl: The two-sided p-value for the test, which measures the statistical significance of the trend. The p-value ranges from 0 to 1, with p-values less than the significance level indicating that the trend is statistically significant.\n\n\ncbg_1 |> \n  summarise(mk = list(\n    unclass(\n      Kendall::MannKendall(gi_star)))) |> \n  tidyr::unnest_wider(mk)\n\n# A tibble: 1 × 5\n    tau    sl     S     D  varS\n  <dbl> <dbl> <dbl> <dbl> <dbl>\n1 0.121 0.631     8  66.0  213.\n\n\nFrom the results of Koja, we see a slight positive tau value and an insignificant sl value. This means that there is a slight positive association in gi* values and date. However, this result is insignificant. We repeat this for the other 2 locations.\n\ncbg_2 |> \n  summarise(mk = list(\n    unclass(\n      Kendall::MannKendall(gi_star)))) |> \n  tidyr::unnest_wider(mk)\n\n# A tibble: 1 × 5\n     tau    sl     S     D  varS\n   <dbl> <dbl> <dbl> <dbl> <dbl>\n1 0.0606 0.837     4  66.0  213.\n\n\nWe see a similar result for Tambora.\n\ncbg_3 |> \n  summarise(mk = list(\n    unclass(\n      Kendall::MannKendall(gi_star)))) |> \n  tidyr::unnest_wider(mk)\n\n# A tibble: 1 × 5\n     tau     sl     S     D  varS\n   <dbl>  <dbl> <dbl> <dbl> <dbl>\n1 -0.545 0.0164   -36  66.0  213.\n\n\nFor our last location (Johar Baru), the tau and sl values are vastly different. We see that there is a negative association between gi* and date, and that this association is positive.\n\n5.3.1 Statistical Conclusions\nFor Koja and Tambora, there was a positive association between Gi* and date. This means that as months passes, Gi* will become more positive. This signifies greater clustering of hotspots. In this context, this means that there is greater clustering of high vaccination rates in Koja and Tambora, which is desirable from a policy-maker’s perspective. However, this conclusion is not statistically significant.\nOn the other hand, for Johar Baru, there was a negative association between Gi* and date. As month passes, there is a greater clustering of low vaccination rates in Johar Baru, which is undesirable from a policy-makers perspective. This result is statistically significant, and implies that further action needs to be taken."
  },
  {
    "objectID": "Geospatial/Take-home_Ex02/Take-home_Ex02.html#performing-emerging-hotspot-analysis",
    "href": "Geospatial/Take-home_Ex02/Take-home_Ex02.html#performing-emerging-hotspot-analysis",
    "title": "Take-home Exercise 2",
    "section": "5.4 Performing Emerging Hotspot analysis",
    "text": "5.4 Performing Emerging Hotspot analysis\nIn performing emerging hotspot analysis, we run the gi_star values from all the village codes through the MannKendall function (ehsa). We can narrow the ehsa list to only significant gi_star observations by using a filter function (ehsa_significant).\n\nehsa <- gi_stars %>%\n  group_by(village_code) %>%\n  summarise(mk = list(\n    unclass(\n      Kendall::MannKendall(gi_star)))) %>%\n  tidyr::unnest_wider(mk)\n\nehsa_significant<-ehsa |> \n  filter(sl<=0.05)\n\n\n5.4.1 Arrange to show significant emerging hot/cold spots\n\nemerging <- ehsa %>% \n  arrange(sl, abs(tau)) %>% \n  slice(1:5)"
  },
  {
    "objectID": "Geospatial/Take-home_Ex02/Take-home_Ex02.html#performing-emerging-hotspot-analysis-1",
    "href": "Geospatial/Take-home_Ex02/Take-home_Ex02.html#performing-emerging-hotspot-analysis-1",
    "title": "Take-home Exercise 2",
    "section": "5.5 Performing Emerging Hotspot analysis",
    "text": "5.5 Performing Emerging Hotspot analysis\nWe will perform ENSA analysis by using emerging_hotspot_analysis() of sfdep package. It takes a space time object and the quoted name if the variable of interest. The k argument is used to specify the number of time lags which is set to 1 by default. nsim map numvers of simulation to be performed\n\nset.seed(1234)\n         ehsa <- emerging_hotspot_analysis(\n  x = Vac_st, \n  .var = \"vaccination_rate\", \n  k = 1, \n  nsim = 99\n)"
  },
  {
    "objectID": "Geospatial/Take-home_Ex02/Take-home_Ex02.html#visualising-the-distribution-of-ehsa-classes",
    "href": "Geospatial/Take-home_Ex02/Take-home_Ex02.html#visualising-the-distribution-of-ehsa-classes",
    "title": "Take-home Exercise 2",
    "section": "5.6 Visualising the distribution of EHSA classes",
    "text": "5.6 Visualising the distribution of EHSA classes\nIn the code chunk below, ggplot2 functions are used to reveal the distribution of EHSA classes as a bar chart.\n\nggplot(data = ehsa,\n       aes(x = classification)) +\n  geom_bar()"
  },
  {
    "objectID": "Geospatial/Take-home_Ex02/Take-home_Ex02.html#visualising-ehsa",
    "href": "Geospatial/Take-home_Ex02/Take-home_Ex02.html#visualising-ehsa",
    "title": "Take-home Exercise 2",
    "section": "5.7 Visualising EHSA",
    "text": "5.7 Visualising EHSA\nTo visualise the geographic distribution of the EHSA classes, we need to join the ehsa table with our classification column with geoJAR, with the geometry field. We conduct a left join of the two.\n\nvaccination_ehsa <- geoJAR |>\n  left_join(ehsa,\n            by= c(\"village_code\"=\"location\"))\n\nWe are now ready to plot all the significant classifications.\n\nehsa_sig <- vaccination_ehsa  %>%\n  filter(p_value < 0.05)\ntmap_mode(\"plot\")\ntm_shape(vaccination_ehsa) +\n  tm_polygons() +\n  tm_borders(alpha = 0.5) +\ntm_shape(ehsa_sig) +\n  tm_fill(\"classification\") + \n  tm_borders(alpha = 0.4)\n\n\n\n\n\n5.7.1 Statistical Conclusions\n\nSource: pro.arcgis.com\n\n\nPattern Name\nDefinition\n\n\n\n\nOscillating coldspot\nA statistically significant cold spot for the final time-step interval that has a history of also being a statistically significant hot spot during a prior time step. Less than 90 percent of the time-step intervals have been statistically significant cold spots.\n\n\nOscillating hotspot\nA statistically significant hot spot for the final time-step interval that has a history of also being a statistically significant cold spot during a prior time step. Less than 90 percent of the time-step intervals have been statistically significant hot spots.\n\n\nSporadic cold spot\nA statistically significant cold spot for the final time-step interval with a history of also being an on-again and off-again cold spot. Less than 90 percent of the time-step intervals have been statistically significant cold spots and none of the time-step intervals have been statistically significant hot spots.\n\n\n\nWith this understanding, we can infer that\n\nPresence of large numbers of oscillating hotspots and cold spots\n\nThe presence of oscillating spots for vaccination rates in emerging hotspot analysis could suggest that there are fluctuations in vaccine uptake within certain areas or populations over time. This may indicate that certain groups of people are more likely to get vaccinated during certain times or in response to certain events or circumstances, such as outbreaks of infectious diseases.\n\nGreater number of oscillating hot spots than oscillating cold spots\n\nThis could imply that there more states are moving from the clustering of low to high vaccination rates as compared to the reverse. This makes sense if we consider the ongoing inoculation program by the government.\n\nPresence of Sporadic cold spots\n\nThe presence of sporadic cold spots for vaccination rates in emerging hotspot analysis could indicate that certain areas or populations are not receiving the same level of vaccination coverage as others. These cold spots may represent areas where people have limited access to vaccines, where vaccine hesitancy is more prevalent, or where there is a lack of public health messaging about the importance of vaccination."
  },
  {
    "objectID": "Geospatial/Introduction.html",
    "href": "Geospatial/Introduction.html",
    "title": "Introduction - Geospatial Analytics",
    "section": "",
    "text": "This section seeks to showcase my coursework for Geospatial Analytics taught by Professor Kam Tin Seong at SMU. This course provides students with an introduction to the concepts, principles and methods of geospatial analytics and their practical applications of geospatial analytics in real world operations. Emphasis will be placed on\n\nperforming geospatial data science tasks such as importing, tidying, manipulating, transforming, projecting and processing geospatial data programmatically,\nvisualising, analysing and describing geographical patterns and process using appropriate geovisualisation and thematic mapping techniques,\nConducting geospatial analysis by using appropriate spatial statistics and machine learning methods and\nbuilding web-based geospatial analytics applications."
  },
  {
    "objectID": "Geospatial/Introduction.html#assignments-showcased",
    "href": "Geospatial/Introduction.html#assignments-showcased",
    "title": "Introduction - Geospatial Analytics",
    "section": "2 Assignments showcased",
    "text": "2 Assignments showcased\nIn this website, I will showcase my attempts on the take-home assignments in the course. The problem statements answered are as follows:\n\nHow can we analyse geographical distributions of point data? Analysis using spatial point pattern and data on waterpoints in Osun state, Nigeria.\nHow can we uncover the spatio-temporal trends of vaccination (i.e. clustering of data points) using exploratory spatial data analysis? Anaylsis using Covid-19 vaccination in DKI Jarkarta.\nHow can we create a predictive model to predict property prices using geographically weighted methods. Analysis using Geographically weighted regression on HDB resale prices in Singapore.\n\nFeel free to explore these projects by toggling to the respective tabs!"
  },
  {
    "objectID": "Text-analytics/Intro-to-text-analytics/Intro-to-text-analytics.html#unnest-token-function",
    "href": "Text-analytics/Intro-to-text-analytics/Intro-to-text-analytics.html#unnest-token-function",
    "title": "About Text Analytics",
    "section": "2.1 Unnest token function",
    "text": "2.1 Unnest token function\nMost often, we are faced with paragraphs of words. But in analysing text, the form we want to create is in a token tidy text form. The textbook provided an example text written by Emily Dickinson:\n\nlibrary(dplyr)\n\ntext <- c(\"Because I could not stop for Death -\",\n          \"He kindly stopped for me -\",\n          \"The Carriage held but just Ourselves -\",\n          \"and Immortality\")\ntext_df<-tibble(line=1:4, text=text)\n\nThis is in a tibble form, which is not yet compatible with tidy text analysis. We need to convert this so that it has one-token-per-document-per-row.\n\nlibrary(tidytext)\ntext_df |> \n  unnest_tokens(word, text)\n\n# A tibble: 20 × 2\n    line word       \n   <int> <chr>      \n 1     1 because    \n 2     1 i          \n 3     1 could      \n 4     1 not        \n 5     1 stop       \n 6     1 for        \n 7     1 death      \n 8     2 he         \n 9     2 kindly     \n10     2 stopped    \n11     2 for        \n12     2 me         \n13     3 the        \n14     3 carriage   \n15     3 held       \n16     3 but        \n17     3 just       \n18     3 ourselves  \n19     4 and        \n20     4 immortality\n\n\nThe unnest_token() arguments are column names: i.e. output column name: word, input column that the text comes from: text."
  },
  {
    "objectID": "Text-analytics/Intro-to-text-analytics/Intro-to-text-analytics.html#example-tidying-the-works-of-jane-austen",
    "href": "Text-analytics/Intro-to-text-analytics/Intro-to-text-analytics.html#example-tidying-the-works-of-jane-austen",
    "title": "About Text Analytics",
    "section": "2.2 Example: tidying the works of Jane Austen",
    "text": "2.2 Example: tidying the works of Jane Austen\n\n2.2.1 Data wrangling\nThe text of Jane Austen’s 6 completed, published novels from the janeaustenr package (Silge 2016) was used.\n\nlibrary(janeaustenr)\nlibrary(dplyr)\nlibrary(stringr)\nlibrary(tidyverse)\n\nausten_books()\n\n# A tibble: 73,422 × 2\n   text                    book               \n * <chr>                   <fct>              \n 1 \"SENSE AND SENSIBILITY\" Sense & Sensibility\n 2 \"\"                      Sense & Sensibility\n 3 \"by Jane Austen\"        Sense & Sensibility\n 4 \"\"                      Sense & Sensibility\n 5 \"(1811)\"                Sense & Sensibility\n 6 \"\"                      Sense & Sensibility\n 7 \"\"                      Sense & Sensibility\n 8 \"\"                      Sense & Sensibility\n 9 \"\"                      Sense & Sensibility\n10 \"CHAPTER 1\"             Sense & Sensibility\n# … with 73,412 more rows\n\n\nUsing mutate(), we annotate a linenumber quantity to keep track of lines in the original format, and a chapter.\n\noriginal_books <- austen_books() |> \n  group_by(book) |> \n  mutate(linenumber = row_number(),\n         chapter = cumsum(str_detect(text, \n                                     regex(\"^chapter [\\\\divxlc]\",\n                                           ignore_case = TRUE)))) %>%\n  ungroup()\n\noriginal_books\n\n# A tibble: 73,422 × 4\n   text                    book                linenumber chapter\n   <chr>                   <fct>                    <int>   <int>\n 1 \"SENSE AND SENSIBILITY\" Sense & Sensibility          1       0\n 2 \"\"                      Sense & Sensibility          2       0\n 3 \"by Jane Austen\"        Sense & Sensibility          3       0\n 4 \"\"                      Sense & Sensibility          4       0\n 5 \"(1811)\"                Sense & Sensibility          5       0\n 6 \"\"                      Sense & Sensibility          6       0\n 7 \"\"                      Sense & Sensibility          7       0\n 8 \"\"                      Sense & Sensibility          8       0\n 9 \"\"                      Sense & Sensibility          9       0\n10 \"CHAPTER 1\"             Sense & Sensibility         10       1\n# … with 73,412 more rows\n\n\nTo create a tidy dataset, we restructure it in the one-token-per-row format, using the unnest_tokens() function\n\nlibrary(tidytext)\ntidy_books<-original_books |> \n  unnest_tokens(word, text)\n\ntidy_books\n\n# A tibble: 725,055 × 4\n   book                linenumber chapter word       \n   <fct>                    <int>   <int> <chr>      \n 1 Sense & Sensibility          1       0 sense      \n 2 Sense & Sensibility          1       0 and        \n 3 Sense & Sensibility          1       0 sensibility\n 4 Sense & Sensibility          3       0 by         \n 5 Sense & Sensibility          3       0 jane       \n 6 Sense & Sensibility          3       0 austen     \n 7 Sense & Sensibility          5       0 1811       \n 8 Sense & Sensibility         10       1 chapter    \n 9 Sense & Sensibility         10       1 1          \n10 Sense & Sensibility         13       1 the        \n# … with 725,045 more rows\n\n\nOften in analysis, we want to remove stop words, which are words not useful in analysis. We can do so using anti_join()\n\ntidy_books<-tidy_books |> \n  anti_join(stop_words)\n\ntidy_books\n\n# A tibble: 217,609 × 4\n   book                linenumber chapter word       \n   <fct>                    <int>   <int> <chr>      \n 1 Sense & Sensibility          1       0 sense      \n 2 Sense & Sensibility          1       0 sensibility\n 3 Sense & Sensibility          3       0 jane       \n 4 Sense & Sensibility          3       0 austen     \n 5 Sense & Sensibility          5       0 1811       \n 6 Sense & Sensibility         10       1 chapter    \n 7 Sense & Sensibility         10       1 1          \n 8 Sense & Sensibility         13       1 family     \n 9 Sense & Sensibility         13       1 dashwood   \n10 Sense & Sensibility         13       1 settled    \n# … with 217,599 more rows\n\n\nWe see that after this code chunk, the number of rows reduced significantly (725k to 217k rows).\n\n\n2.2.2 Data analysis\n\nWe can use dplyr’s count() to find the most common words in all the books as a whole\n\n\ntidy_books |> \n  count(word, sort=TRUE)\n\n# A tibble: 13,914 × 2\n   word       n\n   <chr>  <int>\n 1 miss    1855\n 2 time    1337\n 3 fanny    862\n 4 dear     822\n 5 lady     817\n 6 sir      806\n 7 day      797\n 8 emma     787\n 9 sister   727\n10 house    699\n# … with 13,904 more rows\n\n\nSome basic visualisations can be done with ggplot\n\nlibrary(ggplot2)\n\ntidy_books |> \n  count(word, sort=TRUE) |> \n  filter(n>600) |> \n  mutate(word=reorder(word, n)) |> \n  ggplot(aes(n, word))+\n  geom_col()+\n  labs(y=NULL)"
  },
  {
    "objectID": "Statistical-learning/Introduction.html",
    "href": "Statistical-learning/Introduction.html",
    "title": "Introduction - Statistical Learning",
    "section": "",
    "text": "This section seeks to showcase my coursework for Statistical Learning with R taught by Professor Kwong Koon Shing at SMU. This course aims at introducing the concepts of statistical methodologies for searching analytical solutions to problems related in business with the practical use of big data. Topics include R-programming, Statistical Modelling, Linear Regression, Resampling Methods, Linear Model Selection and Regularization, Tree-based methods and Unsupervised Learning."
  },
  {
    "objectID": "Statistical-learning/Introduction.html#assignments-showcased",
    "href": "Statistical-learning/Introduction.html#assignments-showcased",
    "title": "Introduction - Statistical Learning",
    "section": "2 Assignments showcased",
    "text": "2 Assignments showcased\nIn this website, I will showcase my group project work under this course. As part of the project, students were instructed to create and evaluate 2 regression models for resale HDB prices in Singapore.\n\nA Explanatory Model\nA Predictive Model\n\nThe specific methodologies used to construct these regressions are showcased in the following 2 tabs in this section."
  },
  {
    "objectID": "Text-analytics/Intro-to-text-analytics/Intro-to-text-analytics.html#data-visualisation",
    "href": "Text-analytics/Intro-to-text-analytics/Intro-to-text-analytics.html#data-visualisation",
    "title": "Introduction",
    "section": "3.1 Data visualisation",
    "text": "3.1 Data visualisation\nWith our data prepared, we can now visualise trends in the changes of sentiments over time. We may be interested in how speech sentiments have varied in the last 30 years\n\nggplot(filter(sentiments, year>1990), aes(x=as.numeric(year), y=sentiment))+\n  geom_point(aes(color=president))+\n  geom_smooth(method=\"auto\")\n\n\n\n\nWe can further segment the sentiment data based on the president.\n\nggplot(filter(sentiments, year>1990), aes(x = president, y = sentiment, color = president)) + \n  geom_boxplot()\n\n\n\n\nWe can further investigate if there is a significant difference in sentiments from Republican and Democratic presidents. To do this, we can create 2 dataframes, capturing sentiments from Republican and Democratic presidents respectively. From this website, we can download data on the president, year and party of each president.\n\npartydata<- read.csv(\"US presidents listed - ALL NAMES.csv\")\npartydata<- partydata |> \n  rename(year=Years..after.inauguration.) |> \n  select(year, Party)\n\nsentiments.party<-sentiments |> \n  inner_join(partydata, by=\"year\")\n\ndemocrat<-sentiments.party |> \n  filter(Party==\"Democrat\")\n\nrepublican<-sentiments.party |> \n  filter(Party==\"Republican\")\n\n#conduct t-test\nt.test(democrat$sentiment, republican$sentiment)\n\n\n    Welch Two Sample t-test\n\ndata:  democrat$sentiment and republican$sentiment\nt = -1.8834, df = 160.74, p-value = 0.06146\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -60.095494   1.424815\nsample estimates:\nmean of x mean of y \n 142.6310  171.9663 \n\n\nFrom our t-test result, we see that while sample mean of democrats (142.6310) and republican (171.9663) may be different, there is insufficient evidence at the 0.05 level of confidence to conclude that there is a statistical significant difference in the sentiments from both groups. At the same time, given that the 95% confidence interval includes zero, it further supports the argument that there is no significant statistical difference.\nHowever, we can still visualise the difference in terms of sample means and variance using a box plot below.\n\nggplot(filter(sentiments.party, Party==\"Democrat\"|Party==\"Republican\"), aes(x = Party, y = sentiment, color = Party)) + geom_boxplot() + geom_point()"
  },
  {
    "objectID": "text.html",
    "href": "text.html",
    "title": "R Notebook",
    "section": "",
    "text": "This project works on information provided on this website, to learn more about sentiment analysis in text analysis in R.\nThe aim of the project is to build a sentiment analysis model which will categorise words based on their sentiments, whether they are positive, negative and the magnitude of it. Sentiment analysis is a process of extracting opinions that have different polarities, ie positive, negative or neutral."
  },
  {
    "objectID": "text.html#what-is-text-mining",
    "href": "text.html#what-is-text-mining",
    "title": "R Notebook",
    "section": "What is text mining",
    "text": "What is text mining\nThe process of analysing collections of textual materials in order to capture key concepts and themes to uncover hidden relationships and trends without requiring that you know precise words or terms that authors have used to express those concepts"
  },
  {
    "objectID": "text.html#packages-used",
    "href": "text.html#packages-used",
    "title": "R Notebook",
    "section": "Packages used",
    "text": "Packages used\n\npacman::p_load(janeaustenr, stringr, tidytext)\n\ntidytext::sentiments\n\n# A tibble: 6,786 × 2\n   word        sentiment\n   <chr>       <chr>    \n 1 2-faces     negative \n 2 abnormal    negative \n 3 abolish     negative \n 4 abominable  negative \n 5 abominably  negative \n 6 abominate   negative \n 7 abomination negative \n 8 abort       negative \n 9 aborted     negative \n10 aborts      negative \n# … with 6,776 more rows\n\n\n\nTidytext: There are a variety of methods and dictionaries that exist for evaluating opinion or emotion in text. The tidytext package provides access to several sentiment lexicons.\n\n\nAFINN from Finn Årup Nielsen,\nbing from Bing Liu and collaborators, and\nnrc from Saif Mohammad and Peter Turney.\n\nwhich are all based on unigrams i.e single words. The nrc lexicon categorizes words in a binary fashion (\"yes\"/\"no\") into categories of positive, negative, anger, anticipation, disgust, fear, joy, sadness, surprise, and trust. The bing lexicon categorizes words in a binary fashion into positive and negative categories. The AFINN lexicon assigns words with a score that runs between -5 and 5, with negative scores indicating negative sentiment and positive scores indicating positive sentiment.\n\nlibrary(tidytext)\nget_sentiments(\"afinn\")\n\n# A tibble: 2,477 × 2\n   word       value\n   <chr>      <dbl>\n 1 abandon       -2\n 2 abandoned     -2\n 3 abandons      -2\n 4 abducted      -2\n 5 abduction     -2\n 6 abductions    -2\n 7 abhor         -3\n 8 abhorred      -3\n 9 abhorrent     -3\n10 abhors        -3\n# … with 2,467 more rows\n\nget_sentiments(\"bing\")\n\n# A tibble: 6,786 × 2\n   word        sentiment\n   <chr>       <chr>    \n 1 2-faces     negative \n 2 abnormal    negative \n 3 abolish     negative \n 4 abominable  negative \n 5 abominably  negative \n 6 abominate   negative \n 7 abomination negative \n 8 abort       negative \n 9 aborted     negative \n10 aborts      negative \n# … with 6,776 more rows\n\nget_sentiments(\"nrc\")\n\n# A tibble: 13,872 × 2\n   word        sentiment\n   <chr>       <chr>    \n 1 abacus      trust    \n 2 abandon     fear     \n 3 abandon     negative \n 4 abandon     sadness  \n 5 abandoned   anger    \n 6 abandoned   fear     \n 7 abandoned   negative \n 8 abandoned   sadness  \n 9 abandonment anger    \n10 abandonment fear     \n# … with 13,862 more rows\n\n\nMore info on sentiment analysis can be found on this website."
  },
  {
    "objectID": "text.html#performing-sentiment-analysis-using-inner-join",
    "href": "text.html#performing-sentiment-analysis-using-inner-join",
    "title": "R Notebook",
    "section": "Performing Sentiment Analysis using Inner Join",
    "text": "Performing Sentiment Analysis using Inner Join\n\nlibrary(janeaustenr)\nlibrary(stringr)\nlibrary(tidytext)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\n\ntidy_data <- austen_books() %>%\n group_by(book) %>%\n mutate(linenumber = row_number(),\n   chapter = cumsum(str_detect(text, regex(\"^chapter [\\\\divxlc]\", \n                          ignore_case = TRUE)))) %>%\nungroup() %>%\nunnest_tokens(word, text)"
  },
  {
    "objectID": "Text-analytics/Intro-to-text-analytics/Intro-to-text-analytics.html#importance-of-sentiment-analysis",
    "href": "Text-analytics/Intro-to-text-analytics/Intro-to-text-analytics.html#importance-of-sentiment-analysis",
    "title": "About Text Analytics",
    "section": "3.1 Importance of sentiment analysis",
    "text": "3.1 Importance of sentiment analysis\nIn this next section, we hope to learn more about sentiment analysis in R. Sentiment analysis is the computational task of automatically determining what feelings a writer is expressing in text. Approaches that analyst can take to conduct sentiment analysis includes\n\nCreate/find a list of words associated with strongly positive or negative sentiment\nCount the number of positive and negative words in the text\nAnalyse the mix of positive and negative words.\n\nIn the tutorial, we will analyse how the sentiment of the State of address, speech given by the President of the United States\n\n#load in the libraries we'll need\nlibrary(tidyverse)\nlibrary(tidytext)\nlibrary(glue)\nlibrary(stringr)\nlibrary(glue)\n\nfiles<-list.files(path=\"data\")\nfiles\n\n  [1] \"Adams_1797.txt\"      \"Adams_1798.txt\"      \"Adams_1799.txt\"     \n  [4] \"Adams_1800.txt\"      \"Adams_1825.txt\"      \"Adams_1826.txt\"     \n  [7] \"Adams_1827.txt\"      \"Adams_1828.txt\"      \"Arthur_1881.txt\"    \n [10] \"Arthur_1882.txt\"     \"Arthur_1883.txt\"     \"Arthur_1884.txt\"    \n [13] \"Buchanan_1857.txt\"   \"Buchanan_1858.txt\"   \"Buchanan_1859.txt\"  \n [16] \"Buchanan_1860.txt\"   \"Buren_1837.txt\"      \"Buren_1838.txt\"     \n [19] \"Buren_1839.txt\"      \"Buren_1840.txt\"      \"Bush_1989.txt\"      \n [22] \"Bush_1990.txt\"       \"Bush_1991.txt\"       \"Bush_1992.txt\"      \n [25] \"Bush_2001.txt\"       \"Bush_2002.txt\"       \"Bush_2003.txt\"      \n [28] \"Bush_2004.txt\"       \"Bush_2005.txt\"       \"Bush_2006.txt\"      \n [31] \"Bush_2007.txt\"       \"Bush_2008.txt\"       \"Carter_1978.txt\"    \n [34] \"Carter_1979.txt\"     \"Carter_1980.txt\"     \"Carter_1981.txt\"    \n [37] \"Cleveland_1885.txt\"  \"Cleveland_1886.txt\"  \"Cleveland_1887.txt\" \n [40] \"Cleveland_1888.txt\"  \"Cleveland_1893.txt\"  \"Cleveland_1894.txt\" \n [43] \"Cleveland_1895.txt\"  \"Cleveland_1896.txt\"  \"Clinton_1993.txt\"   \n [46] \"Clinton_1994.txt\"    \"Clinton_1995.txt\"    \"Clinton_1996.txt\"   \n [49] \"Clinton_1997.txt\"    \"Clinton_1998.txt\"    \"Clinton_1999.txt\"   \n [52] \"Clinton_2000.txt\"    \"Coolidge_1923.txt\"   \"Coolidge_1924.txt\"  \n [55] \"Coolidge_1925.txt\"   \"Coolidge_1926.txt\"   \"Coolidge_1927.txt\"  \n [58] \"Coolidge_1928.txt\"   \"Eisenhower_1954.txt\" \"Eisenhower_1955.txt\"\n [61] \"Eisenhower_1956.txt\" \"Eisenhower_1957.txt\" \"Eisenhower_1958.txt\"\n [64] \"Eisenhower_1959.txt\" \"Eisenhower_1960.txt\" \"Eisenhower_1961.txt\"\n [67] \"Fillmore_1850.txt\"   \"Fillmore_1851.txt\"   \"Fillmore_1852.txt\"  \n [70] \"Ford_1975.txt\"       \"Ford_1976.txt\"       \"Ford_1977.txt\"      \n [73] \"Grant_1869.txt\"      \"Grant_1870.txt\"      \"Grant_1871.txt\"     \n [76] \"Grant_1872.txt\"      \"Grant_1873.txt\"      \"Grant_1874.txt\"     \n [79] \"Grant_1875.txt\"      \"Grant_1876.txt\"      \"Harding_1921.txt\"   \n [82] \"Harding_1922.txt\"    \"Harrison_1889.txt\"   \"Harrison_1890.txt\"  \n [85] \"Harrison_1891.txt\"   \"Harrison_1892.txt\"   \"Hayes_1877.txt\"     \n [88] \"Hayes_1878.txt\"      \"Hayes_1879.txt\"      \"Hayes_1880.txt\"     \n [91] \"Hoover_1929.txt\"     \"Hoover_1930.txt\"     \"Hoover_1931.txt\"    \n [94] \"Hoover_1932.txt\"     \"Jackson_1829.txt\"    \"Jackson_1830.txt\"   \n [97] \"Jackson_1831.txt\"    \"Jackson_1832.txt\"    \"Jackson_1833.txt\"   \n[100] \"Jackson_1834.txt\"    \"Jackson_1835.txt\"    \"Jackson_1836.txt\"   \n[103] \"Jefferson_1801.txt\"  \"Jefferson_1802.txt\"  \"Jefferson_1803.txt\" \n[106] \"Jefferson_1804.txt\"  \"Jefferson_1805.txt\"  \"Jefferson_1806.txt\" \n[109] \"Jefferson_1807.txt\"  \"Jefferson_1808.txt\"  \"Johnson_1865.txt\"   \n[112] \"Johnson_1866.txt\"    \"Johnson_1867.txt\"    \"Johnson_1868.txt\"   \n[115] \"Johnson_1964.txt\"    \"Johnson_1965.txt\"    \"Johnson_1966.txt\"   \n[118] \"Johnson_1967.txt\"    \"Johnson_1968.txt\"    \"Johnson_1969.txt\"   \n[121] \"Kennedy_1962.txt\"    \"Kennedy_1963.txt\"    \"Lincoln_1861.txt\"   \n[124] \"Lincoln_1862.txt\"    \"Lincoln_1863.txt\"    \"Lincoln_1864.txt\"   \n[127] \"Madison_1809.txt\"    \"Madison_1810.txt\"    \"Madison_1811.txt\"   \n[130] \"Madison_1812.txt\"    \"Madison_1813.txt\"    \"Madison_1814.txt\"   \n[133] \"Madison_1815.txt\"    \"Madison_1816.txt\"    \"McKinley_1897.txt\"  \n[136] \"McKinley_1898.txt\"   \"McKinley_1899.txt\"   \"McKinley_1900.txt\"  \n[139] \"Monroe_1817.txt\"     \"Monroe_1818.txt\"     \"Monroe_1819.txt\"    \n[142] \"Monroe_1820.txt\"     \"Monroe_1821.txt\"     \"Monroe_1822.txt\"    \n[145] \"Monroe_1823.txt\"     \"Monroe_1824.txt\"     \"Nixon_1970.txt\"     \n[148] \"Nixon_1971.txt\"      \"Nixon_1972.txt\"      \"Nixon_1973.txt\"     \n[151] \"Nixon_1974.txt\"      \"Obama_2009.txt\"      \"Obama_2010.txt\"     \n[154] \"Obama_2011.txt\"      \"Obama_2012.txt\"      \"Obama_2013.txt\"     \n[157] \"Obama_2014.txt\"      \"Obama_2015.txt\"      \"Obama_2016.txt\"     \n[160] \"Pierce_1853.txt\"     \"Pierce_1854.txt\"     \"Pierce_1855.txt\"    \n[163] \"Pierce_1856.txt\"     \"Polk_1845.txt\"       \"Polk_1846.txt\"      \n[166] \"Polk_1847.txt\"       \"Polk_1848.txt\"       \"Reagan_1982.txt\"    \n[169] \"Reagan_1983.txt\"     \"Reagan_1984.txt\"     \"Reagan_1985.txt\"    \n[172] \"Reagan_1986.txt\"     \"Reagan_1987.txt\"     \"Reagan_1988.txt\"    \n[175] \"Roosevelt_1901.txt\"  \"Roosevelt_1902.txt\"  \"Roosevelt_1903.txt\" \n[178] \"Roosevelt_1904.txt\"  \"Roosevelt_1905.txt\"  \"Roosevelt_1906.txt\" \n[181] \"Roosevelt_1907.txt\"  \"Roosevelt_1908.txt\"  \"Roosevelt_1934.txt\" \n[184] \"Roosevelt_1935.txt\"  \"Roosevelt_1936.txt\"  \"Roosevelt_1937.txt\" \n[187] \"Roosevelt_1938.txt\"  \"Roosevelt_1939.txt\"  \"Roosevelt_1940.txt\" \n[190] \"Roosevelt_1941.txt\"  \"Roosevelt_1942.txt\"  \"Roosevelt_1943.txt\" \n[193] \"Roosevelt_1944.txt\"  \"Roosevelt_1945.txt\"  \"Taft_1909.txt\"      \n[196] \"Taft_1910.txt\"       \"Taft_1911.txt\"       \"Taft_1912.txt\"      \n[199] \"Taylor_1849.txt\"     \"Truman_1946.txt\"     \"Truman_1947.txt\"    \n[202] \"Truman_1948.txt\"     \"Truman_1949.txt\"     \"Truman_1950.txt\"    \n[205] \"Truman_1951.txt\"     \"Truman_1952.txt\"     \"Truman_1953.txt\"    \n[208] \"Trump_2017.txt\"      \"Trump_2018.txt\"      \"Tyler_1841.txt\"     \n[211] \"Tyler_1842.txt\"      \"Tyler_1843.txt\"      \"Tyler_1844.txt\"     \n[214] \"Washington_1791.txt\" \"Washington_1792.txt\" \"Washington_1793.txt\"\n[217] \"Washington_1794.txt\" \"Washington_1795.txt\" \"Washington_1796.txt\"\n[220] \"Wilson_1913.txt\"     \"Wilson_1914.txt\"     \"Wilson_1915.txt\"    \n[223] \"Wilson_1916.txt\"     \"Wilson_1917.txt\"     \"Wilson_1918.txt\"    \n[226] \"Wilson_1919.txt\"     \"Wilson_1920.txt\""
  },
  {
    "objectID": "Text-analytics/Intro-to-text-analytics/Intro-to-text-analytics.html#data-wrangling-1",
    "href": "Text-analytics/Intro-to-text-analytics/Intro-to-text-analytics.html#data-wrangling-1",
    "title": "About Text Analytics",
    "section": "3.2 Data wrangling",
    "text": "3.2 Data wrangling\nOur end goal is to perform the following tasks for all the text files we have 1. Read all the files into R 2. Tokenise all the text files 3. Categorise tokens into sentiment groups (positive or negative) 4. Analyse trends in sentiments\nFor the first 3 steps, we explore how the code would look like if we perform the functions only for 1 file first. In the following code chunk, we import 1 selected file into into R and tokenise the text.\n\n# stick together the path to the file & 1st file name\nfileName <- glue::glue(\"data/\", files[1], sep = \"\") \n\n# get rid of any sneaky trailing spaces \nfileName <- trimws(fileName)\n\n# read in the new file\nfileText <- glue::glue(read_file(fileName)) \n\n# remove any dollar signs (they're special characters in R)\nfileText <- gsub(\"\\\\$\", \"\", fileText)\n\n# tokenize\ntokens <- data_frame(text = fileText) |>  unnest_tokens(word, text)\n\nWith this list of tokens, we can compare them against a list of words with either positive or negative sentiment.\nThe tidytext package has several lists of sentiment lexicons including 1. Afinn: containing negative and positive words on a scale from -5 to 5 2. Bing: containing words coded as negative or positive sentiment 3. NRC: containing words representing the following wider range of sentiments: anger, anticipation, disgust, fear, joy… etc.\nWe will be using Bing as part of this tutorial\n\n# getting the sentiment from the first text:\ntokens |> \n  inner_join(get_sentiments(\"bing\")) |> \n  count(sentiment) |> \n  pivot_wider(names_from = sentiment, values_from=n) |> \n  mutate(sentiment=positive-negative)\n\n# A tibble: 1 × 3\n  negative positive sentiment\n     <int>    <int>     <int>\n1       59       89        30\n\n\nThis output means that for the first text “Adams_1797”, there are 47 negative, and 102 positive polarity words. This means that there are 55 more positive than negative words.\nWe can create a function to generate the number of words with each sentiment to run through all the txt files that we have.\n\nGetSentiment<- function(file){\n  #tokenise\n  fileName <- glue(\"data/\", file, sep = \"\")\n  fileName <- trimws(fileName)\n  fileText <- glue(read_file(fileName))\n  fileText <- gsub(\"\\\\$\", \"\", fileText)\n  tokens <- data_frame(text = fileText) |>  unnest_tokens(word, text)\n  \n  #get sentiment\n  sentiment<- tokens |> \n    inner_join(get_sentiments(\"bing\")) |>\n    count(sentiment) |> \n    pivot_wider(names_from = sentiment, values_from=n) |> \n    mutate(sentiment=positive-negative) |>\n    mutate(file=file) |> #add name of file\n    mutate(year = as.numeric(str_match(file, \"\\\\d{4}\"))) |> \n    mutate(president = str_match(file, \"(.*?)_\")[2]) #add president\n  \n  return(sentiment)\n}\n\nGetSentiment(files[1])\n\n# A tibble: 1 × 6\n  negative positive sentiment file            year president\n     <int>    <int>     <int> <chr>          <dbl> <chr>    \n1       59       89        30 Adams_1797.txt  1797 Adams    \n\n\nWe can create a loop that runs this function through all our txt files:\n\nsentiments<- data_frame() # initialisig an empty data frame first\n\n#creating a loop to populate the empty data frame\nfor(i in files){\n    sentiments <- rbind(sentiments, GetSentiment(i))\n}\n\nsummary(sentiments)\n\n    negative        positive        sentiment         file          \n Min.   : 20.0   Min.   :  57.0   Min.   : -7.0   Length:227        \n 1st Qu.:101.0   1st Qu.: 180.0   1st Qu.: 76.5   Class :character  \n Median :138.0   Median : 281.0   Median :128.0   Mode  :character  \n Mean   :177.3   Mean   : 320.7   Mean   :143.3                     \n 3rd Qu.:209.5   3rd Qu.: 384.5   3rd Qu.:183.5                     \n Max.   :820.0   Max.   :1458.0   Max.   :763.0                     \n      year       president        \n Min.   :1791   Length:227        \n 1st Qu.:1848   Class :character  \n Median :1904   Mode  :character  \n Mean   :1904                     \n 3rd Qu.:1962                     \n Max.   :2018                     \n\n\nBecause there are some surnames that are commone, we manually edit them below:\n\n# disambiguate Bush Sr. and George W. Bush \n# correct president in applicable rows\nbushSr <- sentiments %>% \n  filter(president == \"Bush\") %>% # get rows where the president is named \"Bush\"...\n  filter(year < 2000) %>% # ...and the year is before 200\n  mutate(president = \"Bush Sr.\") # and change \"Bush\" to \"Bush Sr.\"\n\n# remove incorrect rows\nsentiments <- anti_join(sentiments, sentiments[sentiments$president == \"Bush\" & sentiments$year < 2000, ])\n\n# add corrected rows to data_frame \nsentiments <- full_join(sentiments, bushSr)\n\nsummary(sentiments)\n\n    negative        positive        sentiment         file          \n Min.   : 20.0   Min.   :  57.0   Min.   : -7.0   Length:227        \n 1st Qu.:101.0   1st Qu.: 180.0   1st Qu.: 76.5   Class :character  \n Median :138.0   Median : 281.0   Median :128.0   Mode  :character  \n Mean   :177.3   Mean   : 320.7   Mean   :143.3                     \n 3rd Qu.:209.5   3rd Qu.: 384.5   3rd Qu.:183.5                     \n Max.   :820.0   Max.   :1458.0   Max.   :763.0                     \n      year       president        \n Min.   :1791   Length:227        \n 1st Qu.:1848   Class :character  \n Median :1904   Mode  :character  \n Mean   :1904                     \n 3rd Qu.:1962                     \n Max.   :2018"
  },
  {
    "objectID": "Text-analytics/Intro-to-text-analytics/Intro-to-text-analytics.html#data-visualisation-and-analysis",
    "href": "Text-analytics/Intro-to-text-analytics/Intro-to-text-analytics.html#data-visualisation-and-analysis",
    "title": "About Text Analytics",
    "section": "3.3 Data visualisation and analysis",
    "text": "3.3 Data visualisation and analysis\nWith our data prepared, we can now visualise trends in the changes of sentiments over time. We may be interested in how speech sentiments have varied in the last 30 years\n\nggplot(filter(sentiments, year>1990), aes(x=as.numeric(year), y=sentiment))+\n  geom_point(aes(color=president))+\n  geom_smooth(method=\"auto\")\n\n\n\n\nWe can further segment the sentiment data based on the president.\n\nggplot(filter(sentiments, year>1990), aes(x = president, y = sentiment, color = president)) + \n  geom_boxplot()\n\n\n\n\nWe can further investigate if there is a significant difference in sentiments from Republican and Democratic presidents. To do this, we can create 2 dataframes, capturing sentiments from Republican and Democratic presidents respectively. From this website, we can download data on the president, year and party of each president.\n\npartydata<- read.csv(\"US presidents listed - ALL NAMES.csv\")\npartydata<- partydata |> \n  rename(year=Years..after.inauguration.) |> \n  select(year, Party)\n\nsentiments.party<-sentiments |> \n  inner_join(partydata, by=\"year\")\n\ndemocrat<-sentiments.party |> \n  filter(Party==\"Democrat\")\n\nrepublican<-sentiments.party |> \n  filter(Party==\"Republican\")\n\n#conduct t-test\nt.test(democrat$sentiment, republican$sentiment)\n\n\n    Welch Two Sample t-test\n\ndata:  democrat$sentiment and republican$sentiment\nt = -1.8834, df = 160.74, p-value = 0.06146\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -60.095494   1.424815\nsample estimates:\nmean of x mean of y \n 142.6310  171.9663 \n\n\nFrom our t-test result, we see that while sample mean of democrats (142.6310) and republican (171.9663) may be different, there is insufficient evidence at the 0.05 level of confidence to conclude that there is a statistical significant difference in the sentiments from both groups. At the same time, given that the 95% confidence interval includes zero, it further supports the argument that there is no significant statistical difference.\nHowever, we can still visualise the difference in terms of sample means and variance using a box plot below.\n\nggplot(filter(sentiments.party, Party==\"Democrat\"|Party==\"Republican\"), aes(x = Party, y = sentiment, color = Party)) + geom_boxplot() + geom_point()"
  }
]